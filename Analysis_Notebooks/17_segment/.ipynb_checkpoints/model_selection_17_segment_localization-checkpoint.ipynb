{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add to sys path so codebase modules can be found\n",
    "import sys\n",
    "import os\n",
    "BASE_DIR = os.getcwd().split('e-emagin-pet')[0] + 'e-emagin-pet/'\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import json\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import seaborn as sns\n",
    "from codebase import data_utils\n",
    "from codebase import model_selection_utils as ms\n",
    "from codebase import evaluation_utils as eu\n",
    "from codebase.ModelEvalWrapper import ModelEvalWrapper\n",
    "# from codebase import MLP\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, LeaveOneGroupOut\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from hyperopt import hp\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model selection for Random Forest for localization problem on 17 segment dataset\n",
    "\n",
    "Test set evaluation of these models can be achieved by running the saved models from this notebook in the evaluation.py script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training and test sets and split each set into features and labels\n",
    "\n",
    "As is common notation when using sklearn, we use the prefix X to indicate the feature matrix, and the prefix y to indicate corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_utils.load_dataset(data_dir='/q/PET-MBF/data',\n",
    "                               dataset='17_segment',\n",
    "                               problem='localization',\n",
    "                               split='train',\n",
    "                               val_col='nn_val_split')\n",
    "X, y, val_split = data['X'], data['y'], data['val_split']\n",
    "X_train, y_train = X[val_split == 0], y[val_split == 0]\n",
    "X_val, y_val = X[val_split == 1], y[val_split == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rest_basal_anterior',\n",
       " 'rest_basal_anteroseptal',\n",
       " 'rest_basal_inferoseptal',\n",
       " 'rest_basal_inferior',\n",
       " 'rest_basal_inferolateral',\n",
       " 'rest_basal_anterolateral',\n",
       " 'rest_mid_anterior',\n",
       " 'rest_mid_anteroseptal',\n",
       " 'rest_mid_inferoseptal',\n",
       " 'rest_mid_inferior',\n",
       " 'rest_mid_inferolateral',\n",
       " 'rest_mid_anterolateral',\n",
       " 'rest_apical_anterior',\n",
       " 'rest_apical_septal',\n",
       " 'rest_apical_inferior',\n",
       " 'rest_apical_lateral',\n",
       " 'rest_apex',\n",
       " 'stress_basal_anterior',\n",
       " 'stress_basal_anteroseptal',\n",
       " 'stress_basal_inferoseptal',\n",
       " 'stress_basal_inferior',\n",
       " 'stress_basal_inferolateral',\n",
       " 'stress_basal_anterolateral',\n",
       " 'stress_mid_anterior',\n",
       " 'stress_mid_anteroseptal',\n",
       " 'stress_mid_inferoseptal',\n",
       " 'stress_mid_inferior',\n",
       " 'stress_mid_inferolateral',\n",
       " 'stress_mid_anterolateral',\n",
       " 'stress_apical_anterior',\n",
       " 'stress_apical_septal',\n",
       " 'stress_apical_inferior',\n",
       " 'stress_apical_lateral',\n",
       " 'stress_apex',\n",
       " 'reserve_basal_anterior',\n",
       " 'reserve_basal_anteroseptal',\n",
       " 'reserve_basal_inferoseptal',\n",
       " 'reserve_basal_inferior',\n",
       " 'reserve_basal_inferolateral',\n",
       " 'reserve_basal_anterolateral',\n",
       " 'reserve_mid_anterior',\n",
       " 'reserve_mid_anteroseptal',\n",
       " 'reserve_mid_inferoseptal',\n",
       " 'reserve_mid_inferior',\n",
       " 'reserve_mid_inferolateral',\n",
       " 'reserve_mid_anterolateral',\n",
       " 'reserve_apical_anterior',\n",
       " 'reserve_apical_septal',\n",
       " 'reserve_apical_inferior',\n",
       " 'reserve_apical_lateral',\n",
       " 'reserve_apex',\n",
       " 'difference_basal_anterior',\n",
       " 'difference_basal_anteroseptal',\n",
       " 'difference_basal_inferoseptal',\n",
       " 'difference_basal_inferior',\n",
       " 'difference_basal_inferolateral',\n",
       " 'difference_basal_anterolateral',\n",
       " 'difference_mid_anterior',\n",
       " 'difference_mid_anteroseptal',\n",
       " 'difference_mid_inferoseptal',\n",
       " 'difference_mid_inferior',\n",
       " 'difference_mid_inferolateral',\n",
       " 'difference_mid_anterolateral',\n",
       " 'difference_apical_anterior',\n",
       " 'difference_apical_septal',\n",
       " 'difference_apical_inferior',\n",
       " 'difference_apical_lateral',\n",
       " 'difference_apex']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: dummy classifier\n",
    "\n",
    "The dummy classifier makes predictions based only on the class sizes in the training set. A dummy classifier is helpful as a benchmark to ensure that your model performs better than chance--especially in cases of highly imbalanced datasets, where the dummy classifier's prediction strategy can yeild high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "scar_lad:\n",
      "Accuracy: 0.6697530864197531\n",
      "Precision: 0.14492753623188406\n",
      "Recall: 0.1724137931034483\n",
      "F1: 0.15748031496062995\n",
      "Specificity: 0.7781954887218046\n",
      "\n",
      "scar_rca:\n",
      "Accuracy: 0.7098765432098766\n",
      "Precision: 0.2033898305084746\n",
      "Recall: 0.2033898305084746\n",
      "F1: 0.20338983050847462\n",
      "Specificity: 0.8226415094339623\n",
      "\n",
      "scar_lcx:\n",
      "Accuracy: 0.7530864197530864\n",
      "Precision: 0.10416666666666667\n",
      "Recall: 0.11904761904761904\n",
      "F1: 0.11111111111111112\n",
      "Specificity: 0.8475177304964538\n",
      "\n",
      "ischemia_lad:\n",
      "Accuracy: 0.6512345679012346\n",
      "Precision: 0.2361111111111111\n",
      "Recall: 0.22666666666666666\n",
      "F1: 0.23129251700680273\n",
      "Specificity: 0.7791164658634538\n",
      "\n",
      "ischemia_rca:\n",
      "Accuracy: 0.7160493827160493\n",
      "Precision: 0.16326530612244897\n",
      "Recall: 0.13559322033898305\n",
      "F1: 0.14814814814814814\n",
      "Specificity: 0.8452830188679246\n",
      "\n",
      "ischemia_lcx:\n",
      "Accuracy: 0.6820987654320988\n",
      "Precision: 0.19402985074626866\n",
      "Recall: 0.20967741935483872\n",
      "F1: 0.2015503875968992\n",
      "Specificity: 0.7938931297709924\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEWCAYAAACZnQc8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAa/UlEQVR4nO3debxVZd338c+Xg4CokDhwI6CgkYaW3eY8G5ojoqY9UBZ2m2RpDlnOj5qFT/ddZtZz+xhpSjkQkgNZjuR4m3OiIgk4MSqIyqCJnrN/zx97QVs8nLPZ7GGdi++b13qdta+11l6/rfg7l7/rWtdWRGBmZmno1OgAzMysepzUzcwS4qRuZpYQJ3Uzs4Q4qZuZJcRJ3cwsIU7qZmYJcVK3pEi6SNJ1jY7DrFGc1M3MEuKkbh2OpM6NjsEsr5zUrSYknSVpjqQlkl6UNERSk6RzJb2UtT8lqX92/uWSZklanLXvVfJeF0maIOk6SYuB41Yjjj0lPSLpnez9j5PURdIzkr6bndMk6X8kXVDtfw5m9eYej1WdpK2Bk4GdImKupAFAE/A9YARwCDAN+CzwXnbZE8DFwCLgVOAmSQMi4v3s+DDgGODrQNcy49gcuAMYBUwAegD9I+IDSccCD0m6Fzgqi2/0mnxuszxwUrdaaKGYeAdLWhARrwJI+iZwZkS8mJ03efkFEVE6uHmppPOBrUvO+VtE3Jrt/7PMOL4K3BsRN2avF2YbEfG8pB8DtwC9gZ0joqX8j2iWTy6/WNVFxAzgNOAiYL6kcZI2A/oDL7V2jaQzJE2VtEjSO0BPYOOSU2ZVEMoq75cZCwwA/hIR0yt4f7PccVK3moiIGyJiT2ALIID/pJiYt1r53Kx+fhbwZWDDiPgExTKMSt+ygjBavV+JK4DbgQMl7VnB+5vljpO6VZ2krSV9QVJX4H2K5ZIW4CrgR5IGqeizkjYCNgCagQVA52zAskcVQrke2F/SlyV1lrSRpM9lMX4N+DzFQddTgLGS1q/CPc0aykndaqEr8BPgTeB1YFPgXODnwHjgbmAxcDWwLnAXxQHNacBrFH8RVFJu+YiImElxUPYM4C3gGWD7bAD1F8DXI2JpRNwAPAlctqb3NGs0+ZuPzMzS4Z66mVmVSfqtpPmSni9p6yXpHknTs58blhw7R9KM7JmOA0vaPy/puezYLyVp5XutzEndOhxJd0ha2sp2bqNjM8tcCxy0UtvZwKSIGARMyl4jaTAwHNg2u+YKSU3ZNf+P4nMWg7Jt5ff8GM9Ttw4nIg5udAxmbYmIB7OH7koNA/bN9scC91Oc9TUMGBcRy4BXJM0Adpb0KtAjIv4GIOl3wBEUx59WKbdJvdcGg1zst49Zsuy99k+ytc6HH8xptyzR7nu8+XLZOafLJlt9i2IPerkxETGmnct6R8Q8gIiYJ2nTrL0v8GjJebOztg+z/ZXb25TbpG5mlldZAm8viZertV9I0UZ7m5zUzcwACjVfJeINSX2yXnofYH7WPpvi08/L9QPmZu39WmlvkwdKzcwAWprL3yozERiZ7Y8EbitpHy6pq6SBFAdEH89KNUsk7ZrNevl6yTWr5J66mRkQUajae0m6keKg6MaSZgMXUnwgb7yk44GZFFcdJSKmSBoPvEDxyeqTShaX+zbFmTTrUhwgbXOQFHL88JEHSq01Hii11lRjoPSD2c+VP1Da7zNrfL9acU/dzAygij31RnJSNzODegyU1oWTupkZuKduZpaSqHxWS644qZuZARTcUzczS4fLL2ZmCfFAqZlZQtxTNzNLiAdKzcwS4oFSM7N0/Gu5lY7NSd3MDFxTNzNLissvZmYJcU/dzCwhLR82OoKqcFI3MwOXX8zMkuLyi5lZQtxTNzNLiJO6mVk6wgOlZmYJcU3dzCwhLr+YmSXEPXUzs4S4p25mlhD31M3MEtLsL8kwM0uHe+pmZglxTd3MLCHuqZuZJcQ9dTOzhLinbmaWEM9+MTNLSESjI6gKJ3UzM3BN3cwsKYkk9U6NDsDMLBeiUP7WDkmnS5oi6XlJN0rqJqmXpHskTc9+blhy/jmSZkh6UdKBa/IxnNTNzABaWsrf2iCpL3AKsGNEbAc0AcOBs4FJETEImJS9RtLg7Pi2wEHAFZKaKv0YTupmZlAsv5S7ta8zsK6kzkB3YC4wDBibHR8LHJHtDwPGRcSyiHgFmAHsXOnHcFI3M4PVSuqSRkl6smQbtfxtImIO8DNgJjAPWBQRdwO9I2Jeds48YNPskr7ArJJIZmdtFfFAqZkZrNbDRxExBhjT2rGsVj4MGAi8A9wk6dg23k6t3aLsYFbipG5mBkShavPU9wdeiYgFAJJuBnYH3pDUJyLmSeoDzM/Onw30L7m+H8VyTUVcfjEzg2rW1GcCu0rqLknAEGAqMBEYmZ0zErgt258IDJfUVdJAYBDweKUfwz11MzNod1ZLuSLiMUkTgKeBZuDvFEs16wPjJR1PMfEfk50/RdJ44IXs/JMiouJgnNTNzKCqDx9FxIXAhSs1L6PYa2/t/NHA6Grc20ndzAySeaLUST2HOnXqxF8fvIV5895gxDGjOPf80zj40CEUCsGbCxZy0oln8frr89t/I0vW9GmPsnTpUlpaCjQ3N7Prboc0OqSOL5EFvTxQmkMnfmck0158acXrX11+FXvtNpR99jicu+68jx+cfXIDo7O82P+AY9hxpy86oVdLdR8+ahgn9ZzZbLN/44AD9+X3Y8evaFuyZOmK/e7rrUsk0qMwy5VClL/lWN3LL5K+ERHX1Pu+HcUl/3keF/3v/2L99df7SPt5F5zO8BFHsnjxEg4/9GsNis7yIiK44y83EhH85jfXcdXV1zc6pI6vSrNfGq0RPfUfrupA6aO3yz5cVM+YcuGLB+3HggULmfzMlI8dG33xZXzm03tz0/iJnDCqrYfTbG2wz75HsPMuB3HY0GP59rePY889d2l0SB1eFAplb3lWk6Qu6dlVbM8BvVd1XUSMiYgdI2LHruv0rEVoubbLrjtw8CFDeOb5+7jq2l+w1967cuVvfvaRcyaM/xNDh63RypyWgHnz3gBgwYKF3HrbHey00+caG1AKEim/1Kqn3hv4OjC0lW1hje7Z4f3ookvZbpu9+Nx2+/HN407joQcf5cQTvs+WW22x4pyDDxnC9GkvNzBKa7Tu3dddUZ7r3n1dDth/H6ZMebHBUSWgiuupN1Ktauq3A+tHxDMrH5B0f43umawLf/gDPjloIIVCgVmz5nLGqRc0OiRroN69N2HCTVcD0NS5iXHjbuXuu+9vbFApyHkPvFzK60yKXhsMymdg1lBLlr3X6BAshz78YE5rKx2ulncvGF52zlnv4nFrfL9a8cNHZmaQ+7JKuZzUzcwgmfKLk7qZGeR+qmK5nNTNzMA9dTOzpDipm5klJJFlApzUzcyo6neUNpSTupkZuPxiZpYUz34xM0uIe+pmZglxUjczS0e0uPxiZpYO99TNzNLhKY1mZilxUjczS0gaJXUndTMzgGhOI6s7qZuZgXvqZmYp8UCpmVlK3FM3M0uHe+pmZilxT93MLB3R3OgIqsNJ3cwMiER66p0aHYCZWS4UVmNrh6RPSJog6R+SpkraTVIvSfdImp793LDk/HMkzZD0oqQD1+RjOKmbmVHsqZe7leFy4M6I2AbYHpgKnA1MiohBwKTsNZIGA8OBbYGDgCskNVX6OZzUzcyoXlKX1APYG7gaICI+iIh3gGHA2Oy0scAR2f4wYFxELIuIV4AZwM6Vfg4ndTMzIFpU9iZplKQnS7ZRJW+1JbAAuEbS3yVdJWk9oHdEzAPIfm6and8XmFVy/eysrSJtDpRK6tXW8Yh4q9Ibm5nlyeoMlEbEGGDMKg53BnYAvhsRj0m6nKzUsgpq7RblR/Pxm7flqezNBWwOvJ3tfwKYCQys9MZmZnkShdZya0VmA7Mj4rHs9QSKSf0NSX0iYp6kPsD8kvP7l1zfD5hb6c3bLL9ExMCI2BK4CxgaERtHxEbAYcDNld7UzCxvqlVTj4jXgVmSts6ahgAvABOBkVnbSOC2bH8iMFxSV0kDgUHA45V+jnLnqe8UESeWBH2HpB9VelMzs7yJqFpPHeC7wPWSugAvA9+g2IkeL+l4ipWOY4r3jSmSxlNM/M3ASRHRUumNy03qb0o6H7iOYjnmWGBhpTc1M8ubaj58FBHPADu2cmjIKs4fDYyuxr3Lnf0yAtgEuCXbNsnazMySUGhR2VueldVTz2a5nCpp/YhYWuOYzMzqrooDpQ1VVk9d0u6SXqBY80HS9pKuqGlkZmZ1FAWVveVZueWXy4ADyeroETGZ4hNTZmZJiCh/y7OyV2mMiFnSR35DVTw6a2aWN3nvgZer3KQ+S9LuQGRTdE6huECNmVkSqjylsWHKTeonUlx1rC/Fp5/uBr5Tq6DMzOqtJeezWspVblLfOiK+WtogaQ/gf6ofkplZ/aXSUy93oPRXZbaZmXVIqcx+aW+Vxt2A3YFNJH2v5FAPoOJF3M3M8ibvs1rK1V75pQuwfnbeBiXti4GjaxWUmVm95b0HXq42k3pEPAA8IOnaiHitTjGZmdVdSyGN7wwq91NcJekTy19I2lDSXbUJycys/ta2h482zr5jD4CIeFvSpm2cb2bWoRTWstkvBUmbL38haQvW4OuWzMzyJkJlb3lWbk/9POBhSQ9kr/cGRrVxvplZh5L3skq5yl16905JOwC7UvyO0tMj4s2aRmZmVkeplF/am6e+TUT8I0vo8K8vQ91c0uYR8XStAju012dq9dbWgU1cOLnRIViiUpn90l5P/QzgBODSVo4F8IWqR2Rm1gCJVF/anad+QvZzv/qEY2bWGGtL+eWoto5HxM3VDcfMrDHyPqulXO2VX4ZmPzeluAbMX7PX+wH3A07qZpaEQqMDqJL2yi/fAJB0OzA4IuZlr/sA/1378MzM6iNYO3rqyw1YntAzbwCfqkE8ZmYN0byWlF+Wuz9b6+VGioPEw4H7ahaVmVmdrVU99Yg4WdKRFJ8kBRgTEbfULiwzs/paK2rqK3kaWBIR90rqLmmDiFhSq8DMzOoplZ56WY9QSToBmAD8OmvqC9xao5jMzOqusBpbnpXbUz8J2Bl4DCAipnvpXTNLSUsiPfVyk/qyiPhAKn5oSZ1J56laMzMS+Ta7spP6A5LOBdaVdADwHeBPtQvLzKy+Con01MtdluwsYAHwHPAt4C/A+bUKysys3mI1tjxrt6cuqRPwbERsB/ym9iGZmdVf3gdAy9VuUo+IgqTJ2frpM+sRlJlZvRWURvml3Jp6H2CKpMeBd5c3RsThNYnKzKzOWhodQJWUm9R/WNMozMwarNqzXyQ1AU8CcyLiMEm9gD8AA4BXgS9HxNvZuecAx1P83XJKRNxV6X3bW0+9G3Ai8EmKg6RXR0RzpTczM8urGsx+ORWYCvTIXp8NTIqIn0g6O3t9lqTBFNfT2hbYDLhX0qcioqL/eWhv9stYYEeKCf1gWv9aOzOzDq+as18k9QMOBa4qaR5GMaeS/TyipH1cRCyLiFeAGRQf9qxIe+WXwRHxmSzIq4HHK72RmVmerU75RdIoYFRJ05iIGFPy+hfAmcAGJW29ly9hHhHzSp7K7ws8WnLe7KytIu0l9Q+X70REsxIZHTYzW9nqTGnMEviY1o5JOgyYHxFPSdq3jLdrLbFWPB2+vaS+vaTFJTdeN3stICKix6ovNTPrOFqq12fdAzhc0iFAN6CHpOuANyT1yXrpfYD52fmzgf4l1/cD5lZ68zZr6hHRFBE9sm2DiOhcsu+EbmbJqNYqjRFxTkT0i4gBFAdA/xoRxwITgZHZaSOB27L9icBwSV0lDQQGsQal7tVZT93MLFl1eKL0J8B4SccDM4FjACJiiqTxwAtAM3BSpTNfwEndzAyAWnxFaUTcD9yf7S8EhqzivNHA6Grc00ndzIy1aO0XM7O1wdq2TICZWdLWti/JMDNLmssvZmYJcVI3M0tI3r/RqFxO6mZmuKZuZpYUz34xM0tIIZECjJO6mRkeKDUzS0oa/XQndTMzwD11M7OkNCuNvrqTupkZLr+YmSXF5Rczs4R4SqOZWULSSOlO6mZmgMsvZmZJaUmkr+6kbmaGe+pmZkkJ99TNzNLhnrpVRa8+G/Gty06h5yYbEoUC991wD3df82e+dMYIdjhgJ6IQLF64iDFn/Ip35r9N0zqd+Y9LTmTgZ7ciCsHvf3g1/3h0SqM/htVQ165duOOucXTp2oXOnZu47dY7+T+jL+easb/kk4MGAtCzZw8WLVrMXrsPbXC0HZenNFpVtLQUuOHHY3nt+Zfptl43Lr79Zzz/8GT+/Otb+eOlNwLwxeMO4YhTv8y15/2a/UbsD8C5B55Oj4168v2x53Ph0DOJSOMvpH3csmUfMPTQY3n33ffo3Lkzd93zB+65+wG+MfKUFef8+JJzWLx4SQOj7PhS+S+oU6MDWNstmv82rz3/MgDvv/s+c2fMplfvjXh/6T9XnNO1ezfIknbfQf2Z8sizACxeuIj3Fr/LwM9uVf/Ara7effc9ANZZpzPrrNP5Y7/EjzzqUCbcdHsjQktGM1H2lmc166lL2gYYBvSl+EtwLjAxIqbW6p4d3cb9NmGLbQcy45lpABz9g6+w51H78s8l73HJ8AsAmPnCq3z+gJ15dOLDbLTZxgzYbit6bbYxL0+e0cjQrcY6derEAw/fxpZbbsFVY67jqScnrzi2+x47sWD+m7z80quNCzABqQyU1qSnLuksYBwg4HHgiWz/Rklnt3HdKElPSnpy+tJXahFabnXt3o1TrjyT6y/+7Ype+oSf3sBpu43ikVsf5ICRBwPwwPhJvDVvIRf/6ad89YL/YMbT/6DQnMoXcdmqFAoF9tp9KIO33oMddtyeTw/+1IpjRx8zlAk3/amB0aWhsBpbntWqp348sG1EfFjaKOnnwBTgJ61dFBFjgDEAX9viqDR+bZahqXMTp1z5Ax659UGevPOxjx1/5LaH+P4153HzZX+g0FLg+h9ds+LYBTdfwuuvzqtnuNZAixYt4eGHHmX//fdm6gvTaGpqYujhB7LPnsMaHVqH55562wrAZq209yH/v+jq7pv/dRJzZ8zhzqv+1dvqPaDPiv0dDtiJuS/NAaBLty50XbcrANvtuT0tzS3MnT67vgFbXW20cS969twAgG7durLvfnswbdpLACv25859vZEhJsE99badBkySNB2YlbVtDnwSOLlG9+yQPrXjNuz5pX2ZOfVVfvyXSwG46afXs8//GkKfLftSKBRYOGcB15z7awB6bNyTM393AYUI3n59IVee/stGhm918G+9N+HKMT+lU1MTnTp14pab/8xdd94HwJeOPow/uvRSFS2JzCBTrabCSeoE7ExxoFTAbOCJiCirALw2lV+sfBMXTm7/JFvrLFr6ktb0Pb6yxZFl55wbXrtlje9XKzWb/RIRBeDRWr2/mVk1pVJT98NHZmbkv1ZeLid1MzPSWSbAT5SamVEsv5T7py2S+ku6T9JUSVMknZq195J0j6Tp2c8NS645R9IMSS9KOnBNPoeTupkZxdkv5W7taAbOiIhPA7sCJ0kaDJwNTIqIQcCk7DXZseHAtsBBwBWSmir9HE7qZmYUyy/lbm2JiHkR8XS2vwSYSnEW4DBgbHbaWOCIbH8YMC4ilkXEK8AMijMHK+KkbmbG6j18VLqkSbaNau09JQ0A/h14DOgdEfOgmPiBTbPT+vKv53mgOP27b6WfwwOlZmas3pTG0iVNVkXS+sAfgdMiYrG0yqntrR2oeNTWSd3MjOrOfpG0DsWEfn1E3Jw1vyGpT0TMk9QHmJ+1zwb6l1zej+KqthVx+cXMDIiIsre2qNglvxqYGhE/Lzk0ERiZ7Y8EbitpHy6pq6SBwCCKq9tWxD11MzOgpXo99T2ArwHPSXomazuX4uq04yUdD8wEjgGIiCmSxgMvUJw5c1K5y6m0xkndzIzqlV8i4mFar5MDDFnFNaOB0dW4v5O6mRkk8z2/TupmZqSzTICTupkZXqXRzCwpqXxJhpO6mRkuv5iZJcVJ3cwsIZ79YmaWEPfUzcwS4tkvZmYJaYk0vqXUSd3MDNfUzcyS4pq6mVlCXFM3M0tIweUXM7N0uKduZpYQz34xM0uIyy9mZglx+cXMLCHuqZuZJcQ9dTOzhLRES6NDqAondTMzvEyAmVlSvEyAmVlC3FM3M0uIZ7+YmSXEs1/MzBLiZQLMzBLimrqZWUJcUzczS4h76mZmCfE8dTOzhLinbmaWEM9+MTNLiAdKzcwSkkr5pVOjAzAzy4NYjT/tkXSQpBclzZB0dh3CX8E9dTMzqtdTl9QE/DdwADAbeELSxIh4oSo3aIeTupkZVa2p7wzMiIiXASSNA4YBa3dS//1rN6vRMeSFpFERMabRcVi++O9FdTV/MKfsnCNpFDCqpGlMyb+LvsCskmOzgV3WPMLyuKbeMYxq/xRbC/nvRYNExJiI2LFkK/3l2tovh7qNwjqpm5lV12ygf8nrfsDcet3cSd3MrLqeAAZJGiipCzAcmFivm+e2pm4f4bqptcZ/L3IoIpolnQzcBTQBv42IKfW6v1KZcG9mZi6/mJklxUndzCwhTuo518jHjS2fJP1W0nxJzzc6FssfJ/UcK3nc+GBgMDBC0uDGRmU5cC1wUKODsHxyUs+3FY8bR8QHwPLHjW0tFhEPAm81Og7LJyf1fGvtceO+DYrFzDoAJ/V8a+jjxmbW8Tip51tDHzc2s47HST3fGvq4sZl1PE7qORYRzcDyx42nAuPr+bix5ZOkG4G/AVtLmi3p+EbHZPnhZQLMzBLinrqZWUKc1M3MEuKkbmaWECd1M7OEOKmbmSXESd0aQtKRkkLSNu2cd5qk7mtwn+Mk/d9KrzfraJzUrVFGAA9TfKCqLacBFSd1s7WNk7rVnaT1gT2A48mSuqQmST+T9JykZyV9V9IpwGbAfZLuy85bWvI+R0u6NtsfKukxSX+XdK+k3vX+XGZ54C+etkY4ArgzIqZJekvSDsAuwEDg37Mv7u0VEW9J+h6wX0S82c57PgzsGhEh6ZvAmcAZtfwQZnnkpG6NMAL4RbY/Lnu9JXBltjQCEbG664X3A/4gqQ/QBXilOqGadSxO6lZXkjYCvgBsJymAJorLCT9FecsKl57TrWT/V8DPI2KipH2Bi6oRr1lH45q61dvRwO8iYouIGBAR/Sn2qp8GTpTUGUBSr+z8JcAGJde/IenTkjoBR5a09wTmZPsja/oJzHLMSd3qbQRwy0ptf6Q4IDoTeFbSZOAr2bExwB3LB0qBs4Hbgb8C80re4yLgJkkPAe3V382S5VUazcwS4p66mVlCnNTNzBLipG5mlhAndTOzhDipm5klxEndzCwhTupmZgn5/8x6C5cFyCPRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for col in y_val.columns:\n",
    "    print('\\n' + col +':')\n",
    "    dummy = DummyClassifier(strategy='stratified')\n",
    "    dummy.fit(X_train, y_train[col])\n",
    "    dummy.score(X_val, y_val[col])\n",
    "    y_pred = dummy.predict(X_val)\n",
    "    ev.confusion_matrix(y_val[col], y_pred, print_scores=True, title=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM not compared in localization task because of poor generalization to multilabel problems\n",
    "\n",
    "## SVM\n",
    "Resources on SVM hyperparameter tuning:\n",
    "https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf, https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf, https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html, https://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "The first two of the above sources suggest at least starting with the RBF kernel (the other two do not comment on which kernels to try). Resources on SVM hyperparameter tuning with polynomial and sigmoid kernels are relatively uncommon. Sklearn has not even published a guide on this topic, despite making suggestions for model selection with linear and rbf kernels. Thus, I have decided to include only linear and rbf kernels in my gridsearch. \n",
    "\n",
    "Because SVMs are not scale invariant, we normalize our data to be zero mean standard deviation 1. We test a range of hyperparameter values using grid search with 10 fold cross validation. The code for the grid search can be found in codebase/hpc_scripts/ms_on_hpc.py\n",
    "\n",
    "We start with values that cover a large area of hyperparameter space and then repeat our grid search to focus around the range of hyperparameter values that lead to the highest validation AUC validation scores. \n",
    "\n",
    "#### Hyperparameter significance from sklearn documentation:\n",
    "\n",
    "**kernel:** A kernel can project input data into higher dimensional space where it can potentially be easier to identify a decision boundry that can better separate the classes than when each instance is represented in the original input space.\n",
    "- The linear kernel does not alter the feature space. SVM with a linear kernel will create a linear decision boundry.\n",
    "- The radial basis function (rbf) kernel is a similarity measure that decreases as function of the distance between two vectors (in our case, two feature vectors, each containing measurements extracted from a PET scan). This function has the shape of a bell shaped curve. When SVM makes a new prediciton on some feature vector with the RBF kernel, the closest training instances will be more influential in the prediciton. Influence of training instances on the predicted class diminishes the further away those instances are. The gamma hyperparameter determines how quickly this influence decays depending on distance--i.e. the width of the \"bell\" in the bell shaped curve.\n",
    "    - More resources on RBF kernels: https://www.youtube.com/watch?v=Qc5IyLW_hns, http://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf \n",
    " \n",
    "**C:** This parameter trades off correct classification of training examples against maximization of the decision functions margin. A high C prioritizes classifying all training examples correctly over defining a large margin, while a low C will prioritize a larger margin over the correct classification of all samples. This hyperparameter applies to both linear and rbf kernels.\n",
    "- We start with 50 values with logarithmic spacing between 0.00001 and 1000 -- i.e. np.geomspace(0.00001, 1000, 50)\n",
    "\n",
    "**gamma:** For SVM with RBF Kernel, gamma defines how much influence a single training example has on the decision boundry. The larger gamma is, the closer other examples must be to be affected.\n",
    "- We start with 50 values with logarithmic spacing between 0.00001 and 1000 -- i.e. np.geomspace(0.00001, 1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Random Forest\n",
    "\n",
    "We do not normalize features to mean zero, standard deviation one for random forest. Because this classifier will split based on thresholds values, its decisions will not change based on feature scale. We use 10 fold cross validation in our grid search. The code for the grid search can be found in codebase/hpc_scripts/ms_on_hpc.py\n",
    "\n",
    "#### Hyperparameter significance\n",
    "          \n",
    "**n_estimators:** The number of base learners. Generally, I leave this lower while identifying other hyperparameter values, and then increase as a last step. Increasing n_estimators should increase performance, but should not increase overfitting. See article: https://arxiv.org/pdf/1705.05654.pdf\n",
    "- start with 200\n",
    "\n",
    "\n",
    "**max_depth:** The maximum depth of each base learner. The deeper the base learners, the more prone to overfitting the model will be. Shallower trees act to regularize the model. \n",
    "- start with [2, 5, 10, 15, 20, 30, 50, 75, 100, 200]\n",
    "\n",
    "**min_samples_split:** The minimum number of samples in an interior node for it to be eligible to be split. Lower values will lead to more complex base learners, and potentially over fitting, while larger values act to regularize. \n",
    "- start with [2, 5, 10, 15, 20, 30, 40, 60, 80, 100, 150, 200]\n",
    "\n",
    "**min_samples_leaf:** The minimum number of samples allowed in a leaf node. Smaller values will lead to more complex base learners, larger values will act to increase regularization.\n",
    "- start with [1, 5, 10, 20, 40, 80, 150]\n",
    "\n",
    "**max_features:** The fraction of total features which will be considered for each split in a given tree. Using only a fraction of all available features ensures variation accross the base learners.\n",
    "- start with [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 575\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hp_spec = {'n_estimators': [200],\n",
    "          'max_depth': [2, 5, 10, 15, 20, 30, 50, 75, 100, 200],\n",
    "          'min_samples_split': [2, 5, 10, 15, 20, 30, 40, 60, 80, 100, 150, 200],\n",
    "          'min_samples_leaf': [1, 5, 10, 20, 40, 80, 150],\n",
    "          'max_features': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\n",
    "\n",
    "ms.ms_on_hpc(model='rf',\n",
    "             data_dir='/q/PET-MBF/data',\n",
    "             out_dir='/q/PET-MBF/output',\n",
    "             hp_spec=hp_spec,\n",
    "             dataset='17_segment',\n",
    "             problem='localization',\n",
    "             ms_round=1,\n",
    "             mem='20GB',\n",
    "             time='72:00:00',\n",
    "             cpus=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n",
      "               591     batch jupyter- dberman_  R       7:43      1 ohi-hpc2-keon01 \n",
      "Usage example: change_mofed_version.sh 4.5-1.0.1\n",
      "2020-12-17 16:30:33.537269: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "metrics:  ['difference' 'reserve' 'rest' 'stress']\n",
      "regions:  ['apex' 'apical_anterior' 'apical_inferior' 'apical_lateral'\n",
      " 'apical_septal' 'basal_anterior' 'basal_anterolateral'\n",
      " 'basal_anteroseptal' 'basal_inferior' 'basal_inferolateral'\n",
      " 'basal_inferoseptal' 'mid_anterior' 'mid_anterolateral'\n",
      " 'mid_anteroseptal' 'mid_inferior' 'mid_inferolateral' 'mid_inferoseptal']\n",
      "label cols:  Index(['scar_lad', 'scar_rca', 'scar_lcx', 'ischemia_lad', 'ischemia_rca',\n",
      "       'ischemia_lcx'],\n",
      "      dtype='object')\n",
      "Fitting 10 folds for each of 8400 candidates, totalling 84000 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 410 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:   47.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1210 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1760 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2410 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3160 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4010 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4960 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6010 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7160 tasks      | elapsed: 21.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8410 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=-1)]: Done 9760 tasks      | elapsed: 31.2min\n",
      "[Parallel(n_jobs=-1)]: Done 11210 tasks      | elapsed: 36.7min\n",
      "[Parallel(n_jobs=-1)]: Done 12760 tasks      | elapsed: 45.7min\n",
      "[Parallel(n_jobs=-1)]: Done 14410 tasks      | elapsed: 58.5min\n",
      "[Parallel(n_jobs=-1)]: Done 16160 tasks      | elapsed: 76.3min\n",
      "[Parallel(n_jobs=-1)]: Done 18010 tasks      | elapsed: 86.6min\n",
      "[Parallel(n_jobs=-1)]: Done 19960 tasks      | elapsed: 97.7min\n",
      "[Parallel(n_jobs=-1)]: Done 22010 tasks      | elapsed: 117.2min\n",
      "[Parallel(n_jobs=-1)]: Done 24160 tasks      | elapsed: 147.2min\n",
      "[Parallel(n_jobs=-1)]: Done 26410 tasks      | elapsed: 167.2min\n",
      "[Parallel(n_jobs=-1)]: Done 28760 tasks      | elapsed: 184.1min\n",
      "[Parallel(n_jobs=-1)]: Done 31210 tasks      | elapsed: 215.7min\n",
      "[Parallel(n_jobs=-1)]: Done 33760 tasks      | elapsed: 259.9min\n",
      "[Parallel(n_jobs=-1)]: Done 36410 tasks      | elapsed: 275.3min\n",
      "[Parallel(n_jobs=-1)]: Done 39160 tasks      | elapsed: 311.3min\n",
      "[Parallel(n_jobs=-1)]: Done 42010 tasks      | elapsed: 366.8min\n",
      "[Parallel(n_jobs=-1)]: Done 44960 tasks      | elapsed: 386.7min\n",
      "[Parallel(n_jobs=-1)]: Done 48010 tasks      | elapsed: 433.4min\n",
      "[Parallel(n_jobs=-1)]: Done 51160 tasks      | elapsed: 490.1min\n",
      "[Parallel(n_jobs=-1)]: Done 54410 tasks      | elapsed: 522.1min\n",
      "[Parallel(n_jobs=-1)]: Done 57760 tasks      | elapsed: 591.5min\n",
      "[Parallel(n_jobs=-1)]: Done 61210 tasks      | elapsed: 630.3min\n",
      "[Parallel(n_jobs=-1)]: Done 64760 tasks      | elapsed: 685.6min\n",
      "[Parallel(n_jobs=-1)]: Done 68410 tasks      | elapsed: 750.6min\n",
      "[Parallel(n_jobs=-1)]: Done 72160 tasks      | elapsed: 793.9min\n",
      "[Parallel(n_jobs=-1)]: Done 76010 tasks      | elapsed: 875.6min\n",
      "[Parallel(n_jobs=-1)]: Done 79960 tasks      | elapsed: 914.1min\n",
      "[Parallel(n_jobs=-1)]: Done 84000 out of 84000 | elapsed: 1003.4min finished\n",
      "Job completed.\n"
     ]
    }
   ],
   "source": [
    "!squeue\n",
    "!cat /q/PET-MBF/output/17_segment/localization/rf/logs/dberman_mitre-rf_17s_r1-job575.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 param settings mean_test_accuracy:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6217</th>\n",
       "      <td>0.584327</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>0.748417</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.499018</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.595599</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>0.899348</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7057</th>\n",
       "      <td>0.584327</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>0.748417</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.499018</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.595599</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>0.899348</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7897</th>\n",
       "      <td>0.584327</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>0.748417</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.499018</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.595599</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>0.899348</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>200</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>0.583942</td>\n",
       "      <td>0.015567</td>\n",
       "      <td>0.748241</td>\n",
       "      <td>0.030494</td>\n",
       "      <td>0.498656</td>\n",
       "      <td>0.025558</td>\n",
       "      <td>0.595266</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.899160</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8317</th>\n",
       "      <td>0.583155</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.738721</td>\n",
       "      <td>0.027506</td>\n",
       "      <td>0.500974</td>\n",
       "      <td>0.023598</td>\n",
       "      <td>0.594308</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.896225</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7477</th>\n",
       "      <td>0.583155</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.738721</td>\n",
       "      <td>0.027506</td>\n",
       "      <td>0.500974</td>\n",
       "      <td>0.023598</td>\n",
       "      <td>0.594308</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.896225</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6637</th>\n",
       "      <td>0.583155</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.738721</td>\n",
       "      <td>0.027506</td>\n",
       "      <td>0.500974</td>\n",
       "      <td>0.023598</td>\n",
       "      <td>0.594308</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.896225</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4537</th>\n",
       "      <td>0.582790</td>\n",
       "      <td>0.016772</td>\n",
       "      <td>0.747468</td>\n",
       "      <td>0.030195</td>\n",
       "      <td>0.496092</td>\n",
       "      <td>0.026743</td>\n",
       "      <td>0.593155</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.896823</td>\n",
       "      <td>0.008707</td>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5797</th>\n",
       "      <td>0.582770</td>\n",
       "      <td>0.014869</td>\n",
       "      <td>0.738477</td>\n",
       "      <td>0.027570</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.024178</td>\n",
       "      <td>0.593753</td>\n",
       "      <td>0.022539</td>\n",
       "      <td>0.896320</td>\n",
       "      <td>0.009522</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4117</th>\n",
       "      <td>0.582758</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>0.740117</td>\n",
       "      <td>0.028996</td>\n",
       "      <td>0.492837</td>\n",
       "      <td>0.023575</td>\n",
       "      <td>0.589060</td>\n",
       "      <td>0.023127</td>\n",
       "      <td>0.889479</td>\n",
       "      <td>0.010435</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "6217            0.584327           0.015436             0.748417   \n",
       "7057            0.584327           0.015436             0.748417   \n",
       "7897            0.584327           0.015436             0.748417   \n",
       "5377            0.583942           0.015567             0.748241   \n",
       "8317            0.583155           0.014356             0.738721   \n",
       "7477            0.583155           0.014356             0.738721   \n",
       "6637            0.583155           0.014356             0.738721   \n",
       "4537            0.582790           0.016772             0.747468   \n",
       "5797            0.582770           0.014869             0.738477   \n",
       "4117            0.582758           0.014526             0.740117   \n",
       "\n",
       "      std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "6217            0.030309          0.499018         0.025049      0.595599   \n",
       "7057            0.030309          0.499018         0.025049      0.595599   \n",
       "7897            0.030309          0.499018         0.025049      0.595599   \n",
       "5377            0.030494          0.498656         0.025558      0.595266   \n",
       "8317            0.027506          0.500974         0.023598      0.594308   \n",
       "7477            0.027506          0.500974         0.023598      0.594308   \n",
       "6637            0.027506          0.500974         0.023598      0.594308   \n",
       "4537            0.030195          0.496092         0.026743      0.593155   \n",
       "5797            0.027570          0.500300         0.024178      0.593753   \n",
       "4117            0.028996          0.492837         0.023575      0.589060   \n",
       "\n",
       "      std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "6217     0.024382       0.899348      0.007906                   75   \n",
       "7057     0.024382       0.899348      0.007906                  100   \n",
       "7897     0.024382       0.899348      0.007906                  200   \n",
       "5377     0.024845       0.899160      0.007951                   50   \n",
       "8317     0.022095       0.896225      0.009505                  200   \n",
       "7477     0.022095       0.896225      0.009505                  100   \n",
       "6637     0.022095       0.896225      0.009505                   75   \n",
       "4537     0.025791       0.896823      0.008707                   30   \n",
       "5797     0.022539       0.896320      0.009522                   50   \n",
       "4117     0.023127       0.889479      0.010435                   20   \n",
       "\n",
       "      param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "6217                     0.5                           1   \n",
       "7057                     0.5                           1   \n",
       "7897                     0.5                           1   \n",
       "5377                     0.5                           1   \n",
       "8317                     1.0                           1   \n",
       "7477                     1.0                           1   \n",
       "6637                     1.0                           1   \n",
       "4537                     0.5                           1   \n",
       "5797                     1.0                           1   \n",
       "4117                     1.0                           1   \n",
       "\n",
       "      param_rf__min_samples_split  param_rf__n_estimators  \n",
       "6217                            5                     200  \n",
       "7057                            5                     200  \n",
       "7897                            5                     200  \n",
       "5377                            5                     200  \n",
       "8317                            5                     200  \n",
       "7477                            5                     200  \n",
       "6637                            5                     200  \n",
       "4537                            5                     200  \n",
       "5797                            5                     200  \n",
       "4117                            5                     200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 5 param settings mean_test_precision:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7590</th>\n",
       "      <td>0.565916</td>\n",
       "      <td>0.007878</td>\n",
       "      <td>0.763944</td>\n",
       "      <td>0.032556</td>\n",
       "      <td>0.385222</td>\n",
       "      <td>0.022767</td>\n",
       "      <td>0.502928</td>\n",
       "      <td>0.021104</td>\n",
       "      <td>0.887760</td>\n",
       "      <td>0.011206</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>0.565916</td>\n",
       "      <td>0.007878</td>\n",
       "      <td>0.763944</td>\n",
       "      <td>0.032556</td>\n",
       "      <td>0.385222</td>\n",
       "      <td>0.022767</td>\n",
       "      <td>0.502928</td>\n",
       "      <td>0.021104</td>\n",
       "      <td>0.887760</td>\n",
       "      <td>0.011206</td>\n",
       "      <td>75</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3390</th>\n",
       "      <td>0.565916</td>\n",
       "      <td>0.007878</td>\n",
       "      <td>0.763944</td>\n",
       "      <td>0.032556</td>\n",
       "      <td>0.385222</td>\n",
       "      <td>0.022767</td>\n",
       "      <td>0.502928</td>\n",
       "      <td>0.021104</td>\n",
       "      <td>0.887233</td>\n",
       "      <td>0.011323</td>\n",
       "      <td>20</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5070</th>\n",
       "      <td>0.565916</td>\n",
       "      <td>0.007878</td>\n",
       "      <td>0.763944</td>\n",
       "      <td>0.032556</td>\n",
       "      <td>0.385222</td>\n",
       "      <td>0.022767</td>\n",
       "      <td>0.502928</td>\n",
       "      <td>0.021104</td>\n",
       "      <td>0.887760</td>\n",
       "      <td>0.011206</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4230</th>\n",
       "      <td>0.565916</td>\n",
       "      <td>0.007878</td>\n",
       "      <td>0.763944</td>\n",
       "      <td>0.032556</td>\n",
       "      <td>0.385222</td>\n",
       "      <td>0.022767</td>\n",
       "      <td>0.502928</td>\n",
       "      <td>0.021104</td>\n",
       "      <td>0.887743</td>\n",
       "      <td>0.011220</td>\n",
       "      <td>30</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6750</th>\n",
       "      <td>0.565916</td>\n",
       "      <td>0.007878</td>\n",
       "      <td>0.763944</td>\n",
       "      <td>0.032556</td>\n",
       "      <td>0.385222</td>\n",
       "      <td>0.022767</td>\n",
       "      <td>0.502928</td>\n",
       "      <td>0.021104</td>\n",
       "      <td>0.887760</td>\n",
       "      <td>0.011206</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>0.565533</td>\n",
       "      <td>0.008617</td>\n",
       "      <td>0.763760</td>\n",
       "      <td>0.032471</td>\n",
       "      <td>0.385238</td>\n",
       "      <td>0.023023</td>\n",
       "      <td>0.502822</td>\n",
       "      <td>0.021322</td>\n",
       "      <td>0.885255</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>15</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>0.571274</td>\n",
       "      <td>0.012619</td>\n",
       "      <td>0.763644</td>\n",
       "      <td>0.033226</td>\n",
       "      <td>0.434394</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>0.549081</td>\n",
       "      <td>0.019666</td>\n",
       "      <td>0.885849</td>\n",
       "      <td>0.011574</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>0.577055</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>0.763350</td>\n",
       "      <td>0.028373</td>\n",
       "      <td>0.436626</td>\n",
       "      <td>0.024136</td>\n",
       "      <td>0.550947</td>\n",
       "      <td>0.024039</td>\n",
       "      <td>0.887066</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.009825</td>\n",
       "      <td>0.762845</td>\n",
       "      <td>0.033879</td>\n",
       "      <td>0.380296</td>\n",
       "      <td>0.022003</td>\n",
       "      <td>0.498074</td>\n",
       "      <td>0.021439</td>\n",
       "      <td>0.880644</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "7590            0.565916           0.007878             0.763944   \n",
       "5910            0.565916           0.007878             0.763944   \n",
       "3390            0.565916           0.007878             0.763944   \n",
       "5070            0.565916           0.007878             0.763944   \n",
       "4230            0.565916           0.007878             0.763944   \n",
       "6750            0.565916           0.007878             0.763944   \n",
       "2550            0.565533           0.008617             0.763760   \n",
       "1682            0.571274           0.012619             0.763644   \n",
       "1680            0.577055           0.014154             0.763350   \n",
       "1710            0.563613           0.009825             0.762845   \n",
       "\n",
       "      std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "7590            0.032556          0.385222         0.022767      0.502928   \n",
       "5910            0.032556          0.385222         0.022767      0.502928   \n",
       "3390            0.032556          0.385222         0.022767      0.502928   \n",
       "5070            0.032556          0.385222         0.022767      0.502928   \n",
       "4230            0.032556          0.385222         0.022767      0.502928   \n",
       "6750            0.032556          0.385222         0.022767      0.502928   \n",
       "2550            0.032471          0.385238         0.023023      0.502822   \n",
       "1682            0.033226          0.434394         0.021942      0.549081   \n",
       "1680            0.028373          0.436626         0.024136      0.550947   \n",
       "1710            0.033879          0.380296         0.022003      0.498074   \n",
       "\n",
       "      std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "7590     0.021104       0.887760      0.011206                  200   \n",
       "5910     0.021104       0.887760      0.011206                   75   \n",
       "3390     0.021104       0.887233      0.011323                   20   \n",
       "5070     0.021104       0.887760      0.011206                   50   \n",
       "4230     0.021104       0.887743      0.011220                   30   \n",
       "6750     0.021104       0.887760      0.011206                  100   \n",
       "2550     0.021322       0.885255      0.011903                   15   \n",
       "1682     0.019666       0.885849      0.011574                   10   \n",
       "1680     0.024039       0.887066      0.011340                   10   \n",
       "1710     0.021439       0.880644      0.012821                   10   \n",
       "\n",
       "      param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "7590                     0.1                          10   \n",
       "5910                     0.1                          10   \n",
       "3390                     0.1                          10   \n",
       "5070                     0.1                          10   \n",
       "4230                     0.1                          10   \n",
       "6750                     0.1                          10   \n",
       "2550                     0.1                          10   \n",
       "1682                     0.1                           1   \n",
       "1680                     0.1                           1   \n",
       "1710                     0.1                          10   \n",
       "\n",
       "      param_rf__min_samples_split  param_rf__n_estimators  \n",
       "7590                           40                     200  \n",
       "5910                           40                     200  \n",
       "3390                           40                     200  \n",
       "5070                           40                     200  \n",
       "4230                           40                     200  \n",
       "6750                           40                     200  \n",
       "2550                           40                     200  \n",
       "1682                           10                     200  \n",
       "1680                            2                     200  \n",
       "1710                           40                     200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 5 param settings mean_test_recall:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6469</th>\n",
       "      <td>0.581616</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.741587</td>\n",
       "      <td>0.031155</td>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.023354</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.024699</td>\n",
       "      <td>0.896678</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>75</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8149</th>\n",
       "      <td>0.581616</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.741587</td>\n",
       "      <td>0.031155</td>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.023354</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.024699</td>\n",
       "      <td>0.896678</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7309</th>\n",
       "      <td>0.581616</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.741587</td>\n",
       "      <td>0.031155</td>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.023354</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.024699</td>\n",
       "      <td>0.896678</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5629</th>\n",
       "      <td>0.581616</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.741579</td>\n",
       "      <td>0.031158</td>\n",
       "      <td>0.501805</td>\n",
       "      <td>0.023343</td>\n",
       "      <td>0.595638</td>\n",
       "      <td>0.024696</td>\n",
       "      <td>0.896650</td>\n",
       "      <td>0.009187</td>\n",
       "      <td>50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6637</th>\n",
       "      <td>0.583155</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.738721</td>\n",
       "      <td>0.027506</td>\n",
       "      <td>0.500974</td>\n",
       "      <td>0.023598</td>\n",
       "      <td>0.594308</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.896225</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8317</th>\n",
       "      <td>0.583155</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.738721</td>\n",
       "      <td>0.027506</td>\n",
       "      <td>0.500974</td>\n",
       "      <td>0.023598</td>\n",
       "      <td>0.594308</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.896225</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7477</th>\n",
       "      <td>0.583155</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.738721</td>\n",
       "      <td>0.027506</td>\n",
       "      <td>0.500974</td>\n",
       "      <td>0.023598</td>\n",
       "      <td>0.594308</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.896225</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5797</th>\n",
       "      <td>0.582770</td>\n",
       "      <td>0.014869</td>\n",
       "      <td>0.738477</td>\n",
       "      <td>0.027570</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.024178</td>\n",
       "      <td>0.593753</td>\n",
       "      <td>0.022539</td>\n",
       "      <td>0.896320</td>\n",
       "      <td>0.009522</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6217</th>\n",
       "      <td>0.584327</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>0.748417</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.499018</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.595599</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>0.899348</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7057</th>\n",
       "      <td>0.584327</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>0.748417</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.499018</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.595599</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>0.899348</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "6469            0.581616           0.017659             0.741587   \n",
       "8149            0.581616           0.017659             0.741587   \n",
       "7309            0.581616           0.017659             0.741587   \n",
       "5629            0.581616           0.017659             0.741579   \n",
       "6637            0.583155           0.014356             0.738721   \n",
       "8317            0.583155           0.014356             0.738721   \n",
       "7477            0.583155           0.014356             0.738721   \n",
       "5797            0.582770           0.014869             0.738477   \n",
       "6217            0.584327           0.015436             0.748417   \n",
       "7057            0.584327           0.015436             0.748417   \n",
       "\n",
       "      std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "6469            0.031155          0.501875         0.023354      0.595700   \n",
       "8149            0.031155          0.501875         0.023354      0.595700   \n",
       "7309            0.031155          0.501875         0.023354      0.595700   \n",
       "5629            0.031158          0.501805         0.023343      0.595638   \n",
       "6637            0.027506          0.500974         0.023598      0.594308   \n",
       "8317            0.027506          0.500974         0.023598      0.594308   \n",
       "7477            0.027506          0.500974         0.023598      0.594308   \n",
       "5797            0.027570          0.500300         0.024178      0.593753   \n",
       "6217            0.030309          0.499018         0.025049      0.595599   \n",
       "7057            0.030309          0.499018         0.025049      0.595599   \n",
       "\n",
       "      std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "6469     0.024699       0.896678      0.009308                   75   \n",
       "8149     0.024699       0.896678      0.009308                  200   \n",
       "7309     0.024699       0.896678      0.009308                  100   \n",
       "5629     0.024696       0.896650      0.009187                   50   \n",
       "6637     0.022095       0.896225      0.009505                   75   \n",
       "8317     0.022095       0.896225      0.009505                  200   \n",
       "7477     0.022095       0.896225      0.009505                  100   \n",
       "5797     0.022539       0.896320      0.009522                   50   \n",
       "6217     0.024382       0.899348      0.007906                   75   \n",
       "7057     0.024382       0.899348      0.007906                  100   \n",
       "\n",
       "      param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "6469                     0.8                           1   \n",
       "8149                     0.8                           1   \n",
       "7309                     0.8                           1   \n",
       "5629                     0.8                           1   \n",
       "6637                     1.0                           1   \n",
       "8317                     1.0                           1   \n",
       "7477                     1.0                           1   \n",
       "5797                     1.0                           1   \n",
       "6217                     0.5                           1   \n",
       "7057                     0.5                           1   \n",
       "\n",
       "      param_rf__min_samples_split  param_rf__n_estimators  \n",
       "6469                            5                     200  \n",
       "8149                            5                     200  \n",
       "7309                            5                     200  \n",
       "5629                            5                     200  \n",
       "6637                            5                     200  \n",
       "8317                            5                     200  \n",
       "7477                            5                     200  \n",
       "5797                            5                     200  \n",
       "6217                            5                     200  \n",
       "7057                            5                     200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 5 param settings mean_test_f1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6469</th>\n",
       "      <td>0.581616</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.741587</td>\n",
       "      <td>0.031155</td>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.023354</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.024699</td>\n",
       "      <td>0.896678</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>75</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8149</th>\n",
       "      <td>0.581616</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.741587</td>\n",
       "      <td>0.031155</td>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.023354</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.024699</td>\n",
       "      <td>0.896678</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7309</th>\n",
       "      <td>0.581616</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.741587</td>\n",
       "      <td>0.031155</td>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.023354</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.024699</td>\n",
       "      <td>0.896678</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5629</th>\n",
       "      <td>0.581616</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.741579</td>\n",
       "      <td>0.031158</td>\n",
       "      <td>0.501805</td>\n",
       "      <td>0.023343</td>\n",
       "      <td>0.595638</td>\n",
       "      <td>0.024696</td>\n",
       "      <td>0.896650</td>\n",
       "      <td>0.009187</td>\n",
       "      <td>50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6217</th>\n",
       "      <td>0.584327</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>0.748417</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.499018</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.595599</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>0.899348</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7897</th>\n",
       "      <td>0.584327</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>0.748417</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.499018</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.595599</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>0.899348</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>200</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7057</th>\n",
       "      <td>0.584327</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>0.748417</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.499018</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.595599</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>0.899348</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>0.583942</td>\n",
       "      <td>0.015567</td>\n",
       "      <td>0.748241</td>\n",
       "      <td>0.030494</td>\n",
       "      <td>0.498656</td>\n",
       "      <td>0.025558</td>\n",
       "      <td>0.595266</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.899160</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7477</th>\n",
       "      <td>0.583155</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.738721</td>\n",
       "      <td>0.027506</td>\n",
       "      <td>0.500974</td>\n",
       "      <td>0.023598</td>\n",
       "      <td>0.594308</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.896225</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8317</th>\n",
       "      <td>0.583155</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.738721</td>\n",
       "      <td>0.027506</td>\n",
       "      <td>0.500974</td>\n",
       "      <td>0.023598</td>\n",
       "      <td>0.594308</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.896225</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "6469            0.581616           0.017659             0.741587   \n",
       "8149            0.581616           0.017659             0.741587   \n",
       "7309            0.581616           0.017659             0.741587   \n",
       "5629            0.581616           0.017659             0.741579   \n",
       "6217            0.584327           0.015436             0.748417   \n",
       "7897            0.584327           0.015436             0.748417   \n",
       "7057            0.584327           0.015436             0.748417   \n",
       "5377            0.583942           0.015567             0.748241   \n",
       "7477            0.583155           0.014356             0.738721   \n",
       "8317            0.583155           0.014356             0.738721   \n",
       "\n",
       "      std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "6469            0.031155          0.501875         0.023354      0.595700   \n",
       "8149            0.031155          0.501875         0.023354      0.595700   \n",
       "7309            0.031155          0.501875         0.023354      0.595700   \n",
       "5629            0.031158          0.501805         0.023343      0.595638   \n",
       "6217            0.030309          0.499018         0.025049      0.595599   \n",
       "7897            0.030309          0.499018         0.025049      0.595599   \n",
       "7057            0.030309          0.499018         0.025049      0.595599   \n",
       "5377            0.030494          0.498656         0.025558      0.595266   \n",
       "7477            0.027506          0.500974         0.023598      0.594308   \n",
       "8317            0.027506          0.500974         0.023598      0.594308   \n",
       "\n",
       "      std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "6469     0.024699       0.896678      0.009308                   75   \n",
       "8149     0.024699       0.896678      0.009308                  200   \n",
       "7309     0.024699       0.896678      0.009308                  100   \n",
       "5629     0.024696       0.896650      0.009187                   50   \n",
       "6217     0.024382       0.899348      0.007906                   75   \n",
       "7897     0.024382       0.899348      0.007906                  200   \n",
       "7057     0.024382       0.899348      0.007906                  100   \n",
       "5377     0.024845       0.899160      0.007951                   50   \n",
       "7477     0.022095       0.896225      0.009505                  100   \n",
       "8317     0.022095       0.896225      0.009505                  200   \n",
       "\n",
       "      param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "6469                     0.8                           1   \n",
       "8149                     0.8                           1   \n",
       "7309                     0.8                           1   \n",
       "5629                     0.8                           1   \n",
       "6217                     0.5                           1   \n",
       "7897                     0.5                           1   \n",
       "7057                     0.5                           1   \n",
       "5377                     0.5                           1   \n",
       "7477                     1.0                           1   \n",
       "8317                     1.0                           1   \n",
       "\n",
       "      param_rf__min_samples_split  param_rf__n_estimators  \n",
       "6469                            5                     200  \n",
       "8149                            5                     200  \n",
       "7309                            5                     200  \n",
       "5629                            5                     200  \n",
       "6217                            5                     200  \n",
       "7897                            5                     200  \n",
       "7057                            5                     200  \n",
       "5377                            5                     200  \n",
       "7477                            5                     200  \n",
       "8317                            5                     200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 5 param settings mean_test_auc:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6217</th>\n",
       "      <td>0.584327</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>0.748417</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.499018</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.595599</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>0.899348</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7057</th>\n",
       "      <td>0.584327</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>0.748417</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.499018</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.595599</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>0.899348</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7897</th>\n",
       "      <td>0.584327</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>0.748417</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.499018</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.595599</td>\n",
       "      <td>0.024382</td>\n",
       "      <td>0.899348</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>200</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5125</th>\n",
       "      <td>0.578955</td>\n",
       "      <td>0.012985</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.020216</td>\n",
       "      <td>0.481503</td>\n",
       "      <td>0.022197</td>\n",
       "      <td>0.582721</td>\n",
       "      <td>0.018483</td>\n",
       "      <td>0.899230</td>\n",
       "      <td>0.008787</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>0.583942</td>\n",
       "      <td>0.015567</td>\n",
       "      <td>0.748241</td>\n",
       "      <td>0.030494</td>\n",
       "      <td>0.498656</td>\n",
       "      <td>0.025558</td>\n",
       "      <td>0.595266</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.899160</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6805</th>\n",
       "      <td>0.579340</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>0.749050</td>\n",
       "      <td>0.020111</td>\n",
       "      <td>0.481503</td>\n",
       "      <td>0.022197</td>\n",
       "      <td>0.582833</td>\n",
       "      <td>0.018396</td>\n",
       "      <td>0.899148</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5965</th>\n",
       "      <td>0.579340</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>0.749050</td>\n",
       "      <td>0.020111</td>\n",
       "      <td>0.481503</td>\n",
       "      <td>0.022197</td>\n",
       "      <td>0.582833</td>\n",
       "      <td>0.018396</td>\n",
       "      <td>0.899148</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7645</th>\n",
       "      <td>0.579340</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>0.749050</td>\n",
       "      <td>0.020111</td>\n",
       "      <td>0.481503</td>\n",
       "      <td>0.022197</td>\n",
       "      <td>0.582833</td>\n",
       "      <td>0.018396</td>\n",
       "      <td>0.899148</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>200</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5124</th>\n",
       "      <td>0.579707</td>\n",
       "      <td>0.019952</td>\n",
       "      <td>0.749207</td>\n",
       "      <td>0.029931</td>\n",
       "      <td>0.476553</td>\n",
       "      <td>0.025225</td>\n",
       "      <td>0.579342</td>\n",
       "      <td>0.024787</td>\n",
       "      <td>0.899096</td>\n",
       "      <td>0.008161</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5964</th>\n",
       "      <td>0.579707</td>\n",
       "      <td>0.019952</td>\n",
       "      <td>0.749207</td>\n",
       "      <td>0.029931</td>\n",
       "      <td>0.476553</td>\n",
       "      <td>0.025225</td>\n",
       "      <td>0.579342</td>\n",
       "      <td>0.024787</td>\n",
       "      <td>0.899035</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "6217            0.584327           0.015436             0.748417   \n",
       "7057            0.584327           0.015436             0.748417   \n",
       "7897            0.584327           0.015436             0.748417   \n",
       "5125            0.578955           0.012985             0.748718   \n",
       "5377            0.583942           0.015567             0.748241   \n",
       "6805            0.579340           0.013090             0.749050   \n",
       "5965            0.579340           0.013090             0.749050   \n",
       "7645            0.579340           0.013090             0.749050   \n",
       "5124            0.579707           0.019952             0.749207   \n",
       "5964            0.579707           0.019952             0.749207   \n",
       "\n",
       "      std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "6217            0.030309          0.499018         0.025049      0.595599   \n",
       "7057            0.030309          0.499018         0.025049      0.595599   \n",
       "7897            0.030309          0.499018         0.025049      0.595599   \n",
       "5125            0.020216          0.481503         0.022197      0.582721   \n",
       "5377            0.030494          0.498656         0.025558      0.595266   \n",
       "6805            0.020111          0.481503         0.022197      0.582833   \n",
       "5965            0.020111          0.481503         0.022197      0.582833   \n",
       "7645            0.020111          0.481503         0.022197      0.582833   \n",
       "5124            0.029931          0.476553         0.025225      0.579342   \n",
       "5964            0.029931          0.476553         0.025225      0.579342   \n",
       "\n",
       "      std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "6217     0.024382       0.899348      0.007906                   75   \n",
       "7057     0.024382       0.899348      0.007906                  100   \n",
       "7897     0.024382       0.899348      0.007906                  200   \n",
       "5125     0.018483       0.899230      0.008787                   50   \n",
       "5377     0.024845       0.899160      0.007951                   50   \n",
       "6805     0.018396       0.899148      0.008772                  100   \n",
       "5965     0.018396       0.899148      0.008772                   75   \n",
       "7645     0.018396       0.899148      0.008772                  200   \n",
       "5124     0.024787       0.899096      0.008161                   50   \n",
       "5964     0.024787       0.899035      0.008111                   75   \n",
       "\n",
       "      param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "6217                     0.5                           1   \n",
       "7057                     0.5                           1   \n",
       "7897                     0.5                           1   \n",
       "5125                     0.2                           1   \n",
       "5377                     0.5                           1   \n",
       "6805                     0.2                           1   \n",
       "5965                     0.2                           1   \n",
       "7645                     0.2                           1   \n",
       "5124                     0.2                           1   \n",
       "5964                     0.2                           1   \n",
       "\n",
       "      param_rf__min_samples_split  param_rf__n_estimators  \n",
       "6217                            5                     200  \n",
       "7057                            5                     200  \n",
       "7897                            5                     200  \n",
       "5125                            5                     200  \n",
       "5377                            5                     200  \n",
       "6805                            5                     200  \n",
       "5965                            5                     200  \n",
       "7645                            5                     200  \n",
       "5124                            2                     200  \n",
       "5964                            2                     200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_rf = ms.load_gs_results(\"/q/PET-MBF/output/17_segment/localization/rf/results/rf_17s_r1-job575.csv\")\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "ms.print_top_n_hyperparams(res_rf, metrics, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 575\r\n",
      "Cluster: deepops\r\n",
      "Use of uninitialized value $user in concatenation (.) or string at /usr/local/bin/seff line 154, <DATA> line 602.\r\n",
      "User/Group: /domain users\r\n",
      "State: COMPLETED (exit code 0)\r\n",
      "Nodes: 1\r\n",
      "Cores per node: 20\r\n",
      "CPU Utilized: 13-20:28:20\r\n",
      "CPU Efficiency: 99.39% of 13-22:31:40 core-walltime\r\n",
      "Job Wall-clock time: 16:43:35\r\n",
      "Memory Utilized: 2.22 GB\r\n",
      "Memory Efficiency: 11.08% of 20.00 GB\r\n"
     ]
    }
   ],
   "source": [
    "!seff 575"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review hyperparameter results that yeild highest AUC\n",
    "\n",
    "**max_depth:** We see values of this hyperparameter fluxuate quite a bit, with a minimum of 50 and a max of 200. Below, we investigate how deep individual trees in a RF might actually be on avg for each different hyperparameter settings that commonly appear in the highest performing area of hyperparameter soace. From the below investigation, we see that average tree depth of a RF in this hyperparameter space tends to max out around 50 with the least restrictive hyperparameters. Thus, we wouldn't expect a max_depth substantially over 50 to have a large affect on training.\n",
    "\n",
    "Thus, in the next round of hyperparameter tuning we focus on this range between 30 and 50, as the highest performing models in the previous round had max depth of 50 or above, but those depth restrictions were so lenient that they didnt result in shorter trees. Thus, the maximum value to use for this hyperparameter should be only slightly above 50. Because this range between 30 and 50 was not investigated at all, we will adjust the other hyperparameter ranges, but still keep them somewhat broad, so that their interactions with restrictions on depth between 30 and 50 can still be observed over a somewhat broad range of values. We see that max features seemed to be the main hyperparameter that controlled the depth of the trees, so we will keep max_features unchanged.\n",
    "\n",
    "- [2, 5, 10, 15, 20, 30, 50, 75, 100, 200] --> [24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54]\n",
    "\n",
    "**min_samples_split:** We see for the highest validation auc, min_samples_split tends to be 5, although occasionally 2. In the next round of gridsearch we will search a range of values centered on this range.\n",
    "- [2, 5, 10, 15, 20, 30, 40, 60, 80, 100, 150, 200] --> [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "\n",
    "**min_samples_leaf:** We see that for the best validation auc min_samples_leaf tends to be 1. In the next round of gridsearch we will search a range of values centered on 1.\n",
    "- [1, 5, 10, 20, 40, 80, 150] --> [1, 3, 5, 7, 9, 11, 13, 15, 17]\n",
    "\n",
    "**max_features:** We see that the values of max_features yeilding the highest AUC scores tend to be between 0.5 and and 0.2. In the next round of gridsearch, we will run a more fine grained search centered around this range.\n",
    "- [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] --> [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_features</th>\n",
       "      <th>avg_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>28.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>29.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>29.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>29.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>29.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>29.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>29.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>29.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>29.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>29.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>29.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>29.540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>29.540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>29.540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>29.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>29.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>29.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>29.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>29.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>29.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>29.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>29.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>29.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>200.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>75.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>75.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>50.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>75.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>75.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>75.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>200.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>75.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>75.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>75.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>50.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>32.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>75.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>32.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>32.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>200.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>32.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>75.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>32.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>32.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>32.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>32.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>75.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>50.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>75.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>75.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>75.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>200.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>33.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>33.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>50.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>33.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>33.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>33.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>33.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>33.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>33.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>33.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>33.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>75.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>33.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>75.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>33.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>50.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>41.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>75.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>41.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>41.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>41.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>41.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>41.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>75.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>200.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>75.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>75.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>43.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>50.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>44.260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>44.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>75.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>44.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>75.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>75.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>44.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>44.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>44.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>75.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>200.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>44.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>44.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>44.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>75.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>44.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>44.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>200.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>44.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>75.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>44.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>44.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>44.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>50.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>44.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>50.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>45.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>75.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>200.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>75.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>45.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>75.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>75.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>45.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>45.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>45.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>75.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>75.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>45.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>45.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>45.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>45.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>45.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>45.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>45.985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_depth  min_samples_split  min_samples_leaf  max_features  avg_depth\n",
       "28        30.0               15.0               5.0           0.2     28.830\n",
       "36        30.0               20.0               5.0           0.2     29.025\n",
       "12        30.0                5.0               5.0           0.2     29.220\n",
       "20        30.0               10.0               5.0           0.2     29.220\n",
       "4         30.0                2.0               5.0           0.2     29.220\n",
       "5         30.0                2.0               5.0           0.5     29.395\n",
       "21        30.0               10.0               5.0           0.5     29.395\n",
       "13        30.0                5.0               5.0           0.5     29.395\n",
       "37        30.0               20.0               5.0           0.5     29.465\n",
       "29        30.0               15.0               5.0           0.5     29.475\n",
       "39        30.0               20.0               5.0           0.9     29.495\n",
       "7         30.0                2.0               5.0           0.9     29.540\n",
       "23        30.0               10.0               5.0           0.9     29.540\n",
       "15        30.0                5.0               5.0           0.9     29.540\n",
       "30        30.0               15.0               5.0           0.7     29.590\n",
       "31        30.0               15.0               5.0           0.9     29.610\n",
       "22        30.0               10.0               5.0           0.7     29.610\n",
       "14        30.0                5.0               5.0           0.7     29.610\n",
       "6         30.0                2.0               5.0           0.7     29.610\n",
       "38        30.0               20.0               5.0           0.7     29.620\n",
       "32        30.0               20.0               1.0           0.2     29.985\n",
       "33        30.0               20.0               1.0           0.5     29.990\n",
       "8         30.0                5.0               1.0           0.2     29.995\n",
       "0         30.0                2.0               1.0           0.2     30.000\n",
       "35        30.0               20.0               1.0           0.9     30.000\n",
       "27        30.0               15.0               1.0           0.9     30.000\n",
       "34        30.0               20.0               1.0           0.7     30.000\n",
       "25        30.0               15.0               1.0           0.5     30.000\n",
       "26        30.0               15.0               1.0           0.7     30.000\n",
       "2         30.0                2.0               1.0           0.7     30.000\n",
       "3         30.0                2.0               1.0           0.9     30.000\n",
       "9         30.0                5.0               1.0           0.5     30.000\n",
       "10        30.0                5.0               1.0           0.7     30.000\n",
       "1         30.0                2.0               1.0           0.5     30.000\n",
       "16        30.0               10.0               1.0           0.2     30.000\n",
       "17        30.0               10.0               1.0           0.5     30.000\n",
       "18        30.0               10.0               1.0           0.7     30.000\n",
       "19        30.0               10.0               1.0           0.9     30.000\n",
       "24        30.0               15.0               1.0           0.2     30.000\n",
       "11        30.0                5.0               1.0           0.9     30.000\n",
       "68        50.0               15.0               5.0           0.2     31.015\n",
       "188      200.0               15.0               5.0           0.2     31.015\n",
       "148      100.0               15.0               5.0           0.2     31.015\n",
       "108       75.0               15.0               5.0           0.2     31.015\n",
       "116       75.0               20.0               5.0           0.2     31.155\n",
       "196      200.0               20.0               5.0           0.2     31.155\n",
       "76        50.0               20.0               5.0           0.2     31.155\n",
       "156      100.0               20.0               5.0           0.2     31.155\n",
       "124      100.0                2.0               5.0           0.2     31.705\n",
       "164      200.0                2.0               5.0           0.2     31.705\n",
       "60        50.0               10.0               5.0           0.2     31.705\n",
       "132      100.0                5.0               5.0           0.2     31.705\n",
       "140      100.0               10.0               5.0           0.2     31.705\n",
       "180      200.0               10.0               5.0           0.2     31.705\n",
       "172      200.0                5.0               5.0           0.2     31.705\n",
       "100       75.0               10.0               5.0           0.2     31.705\n",
       "92        75.0                5.0               5.0           0.2     31.705\n",
       "44        50.0                2.0               5.0           0.2     31.705\n",
       "84        75.0                2.0               5.0           0.2     31.705\n",
       "52        50.0                5.0               5.0           0.2     31.705\n",
       "109       75.0               15.0               5.0           0.5     32.435\n",
       "149      100.0               15.0               5.0           0.5     32.435\n",
       "189      200.0               15.0               5.0           0.5     32.435\n",
       "69        50.0               15.0               5.0           0.5     32.435\n",
       "157      100.0               20.0               5.0           0.5     32.600\n",
       "77        50.0               20.0               5.0           0.5     32.600\n",
       "117       75.0               20.0               5.0           0.5     32.600\n",
       "197      200.0               20.0               5.0           0.5     32.600\n",
       "101       75.0               10.0               5.0           0.5     32.660\n",
       "93        75.0                5.0               5.0           0.5     32.660\n",
       "85        75.0                2.0               5.0           0.5     32.660\n",
       "133      100.0                5.0               5.0           0.5     32.660\n",
       "141      100.0               10.0               5.0           0.5     32.660\n",
       "61        50.0               10.0               5.0           0.5     32.660\n",
       "53        50.0                5.0               5.0           0.5     32.660\n",
       "165      200.0                2.0               5.0           0.5     32.660\n",
       "173      200.0                5.0               5.0           0.5     32.660\n",
       "181      200.0               10.0               5.0           0.5     32.660\n",
       "45        50.0                2.0               5.0           0.5     32.660\n",
       "125      100.0                2.0               5.0           0.5     32.660\n",
       "71        50.0               15.0               5.0           0.9     32.680\n",
       "111       75.0               15.0               5.0           0.9     32.680\n",
       "151      100.0               15.0               5.0           0.9     32.680\n",
       "191      200.0               15.0               5.0           0.9     32.680\n",
       "119       75.0               20.0               5.0           0.9     32.780\n",
       "159      100.0               20.0               5.0           0.9     32.780\n",
       "199      200.0               20.0               5.0           0.9     32.780\n",
       "79        50.0               20.0               5.0           0.9     32.780\n",
       "126      100.0                2.0               5.0           0.7     32.800\n",
       "46        50.0                2.0               5.0           0.7     32.800\n",
       "134      100.0                5.0               5.0           0.7     32.800\n",
       "182      200.0               10.0               5.0           0.7     32.800\n",
       "94        75.0                5.0               5.0           0.7     32.800\n",
       "174      200.0                5.0               5.0           0.7     32.800\n",
       "142      100.0               10.0               5.0           0.7     32.800\n",
       "166      200.0                2.0               5.0           0.7     32.800\n",
       "54        50.0                5.0               5.0           0.7     32.800\n",
       "86        75.0                2.0               5.0           0.7     32.800\n",
       "62        50.0               10.0               5.0           0.7     32.800\n",
       "102       75.0               10.0               5.0           0.7     32.800\n",
       "158      100.0               20.0               5.0           0.7     32.920\n",
       "78        50.0               20.0               5.0           0.7     32.920\n",
       "118       75.0               20.0               5.0           0.7     32.920\n",
       "198      200.0               20.0               5.0           0.7     32.920\n",
       "150      100.0               15.0               5.0           0.7     32.965\n",
       "110       75.0               15.0               5.0           0.7     32.965\n",
       "190      200.0               15.0               5.0           0.7     32.965\n",
       "70        50.0               15.0               5.0           0.7     32.965\n",
       "55        50.0                5.0               5.0           0.9     33.115\n",
       "167      200.0                2.0               5.0           0.9     33.115\n",
       "63        50.0               10.0               5.0           0.9     33.115\n",
       "175      200.0                5.0               5.0           0.9     33.115\n",
       "47        50.0                2.0               5.0           0.9     33.115\n",
       "135      100.0                5.0               5.0           0.9     33.115\n",
       "183      200.0               10.0               5.0           0.9     33.115\n",
       "127      100.0                2.0               5.0           0.9     33.115\n",
       "87        75.0                2.0               5.0           0.9     33.115\n",
       "143      100.0               10.0               5.0           0.9     33.115\n",
       "103       75.0               10.0               5.0           0.9     33.115\n",
       "95        75.0                5.0               5.0           0.9     33.115\n",
       "56        50.0               10.0               1.0           0.2     41.660\n",
       "96        75.0               10.0               1.0           0.2     41.840\n",
       "136      100.0               10.0               1.0           0.2     41.840\n",
       "176      200.0               10.0               1.0           0.2     41.840\n",
       "64        50.0               15.0               1.0           0.2     41.975\n",
       "72        50.0               20.0               1.0           0.2     41.990\n",
       "104       75.0               15.0               1.0           0.2     42.045\n",
       "144      100.0               15.0               1.0           0.2     42.045\n",
       "184      200.0               15.0               1.0           0.2     42.045\n",
       "112       75.0               20.0               1.0           0.2     42.085\n",
       "192      200.0               20.0               1.0           0.2     42.085\n",
       "152      100.0               20.0               1.0           0.2     42.085\n",
       "48        50.0                5.0               1.0           0.2     42.580\n",
       "40        50.0                2.0               1.0           0.2     42.605\n",
       "128      100.0                5.0               1.0           0.2     42.785\n",
       "168      200.0                5.0               1.0           0.2     42.785\n",
       "88        75.0                5.0               1.0           0.2     42.785\n",
       "160      200.0                2.0               1.0           0.2     42.860\n",
       "120      100.0                2.0               1.0           0.2     42.860\n",
       "80        75.0                2.0               1.0           0.2     42.860\n",
       "73        50.0               20.0               1.0           0.5     43.995\n",
       "57        50.0               10.0               1.0           0.5     44.155\n",
       "75        50.0               20.0               1.0           0.9     44.260\n",
       "74        50.0               20.0               1.0           0.7     44.330\n",
       "65        50.0               15.0               1.0           0.5     44.355\n",
       "193      200.0               20.0               1.0           0.5     44.365\n",
       "153      100.0               20.0               1.0           0.5     44.365\n",
       "113       75.0               20.0               1.0           0.5     44.365\n",
       "66        50.0               15.0               1.0           0.7     44.425\n",
       "41        50.0                2.0               1.0           0.5     44.495\n",
       "137      100.0               10.0               1.0           0.5     44.530\n",
       "97        75.0               10.0               1.0           0.5     44.530\n",
       "177      200.0               10.0               1.0           0.5     44.530\n",
       "49        50.0                5.0               1.0           0.5     44.590\n",
       "115       75.0               20.0               1.0           0.9     44.640\n",
       "155      100.0               20.0               1.0           0.9     44.640\n",
       "195      200.0               20.0               1.0           0.9     44.640\n",
       "105       75.0               15.0               1.0           0.5     44.665\n",
       "185      200.0               15.0               1.0           0.5     44.665\n",
       "145      100.0               15.0               1.0           0.5     44.665\n",
       "50        50.0                5.0               1.0           0.7     44.850\n",
       "194      200.0               20.0               1.0           0.7     44.860\n",
       "114       75.0               20.0               1.0           0.7     44.860\n",
       "154      100.0               20.0               1.0           0.7     44.860\n",
       "186      200.0               15.0               1.0           0.7     44.885\n",
       "106       75.0               15.0               1.0           0.7     44.885\n",
       "146      100.0               15.0               1.0           0.7     44.885\n",
       "67        50.0               15.0               1.0           0.9     44.885\n",
       "59        50.0               10.0               1.0           0.9     44.895\n",
       "161      200.0                2.0               1.0           0.5     45.055\n",
       "81        75.0                2.0               1.0           0.5     45.055\n",
       "121      100.0                2.0               1.0           0.5     45.055\n",
       "58        50.0               10.0               1.0           0.7     45.160\n",
       "89        75.0                5.0               1.0           0.5     45.165\n",
       "129      100.0                5.0               1.0           0.5     45.165\n",
       "169      200.0                5.0               1.0           0.5     45.165\n",
       "51        50.0                5.0               1.0           0.9     45.165\n",
       "187      200.0               15.0               1.0           0.9     45.255\n",
       "107       75.0               15.0               1.0           0.9     45.255\n",
       "147      100.0               15.0               1.0           0.9     45.255\n",
       "42        50.0                2.0               1.0           0.7     45.355\n",
       "99        75.0               10.0               1.0           0.9     45.355\n",
       "179      200.0               10.0               1.0           0.9     45.355\n",
       "139      100.0               10.0               1.0           0.9     45.355\n",
       "43        50.0                2.0               1.0           0.9     45.415\n",
       "90        75.0                5.0               1.0           0.7     45.485\n",
       "170      200.0                5.0               1.0           0.7     45.485\n",
       "130      100.0                5.0               1.0           0.7     45.485\n",
       "91        75.0                5.0               1.0           0.9     45.670\n",
       "131      100.0                5.0               1.0           0.9     45.670\n",
       "171      200.0                5.0               1.0           0.9     45.670\n",
       "98        75.0               10.0               1.0           0.7     45.745\n",
       "138      100.0               10.0               1.0           0.7     45.745\n",
       "178      200.0               10.0               1.0           0.7     45.745\n",
       "122      100.0                2.0               1.0           0.7     45.930\n",
       "162      200.0                2.0               1.0           0.7     45.930\n",
       "82        75.0                2.0               1.0           0.7     45.930\n",
       "163      200.0                2.0               1.0           0.9     45.985\n",
       "83        75.0                2.0               1.0           0.9     45.985\n",
       "123      100.0                2.0               1.0           0.9     45.985"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_depths_list = [30, 50, 75, 100, 200]\n",
    "min_samples_splits_list = [2, 5, 10, 15, 20]\n",
    "min_samples_leafs_list = [1, 5]\n",
    "max_features_list = [0.2, 0.5, 0.7, 0.9]\n",
    "\n",
    "ms.avg_depths(X_train, y_train, max_depths_list, min_samples_splits_list, min_samples_leafs_list, max_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try grid search with hyperparameter values specified above, and see how performance changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 592\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hp_spec = {'n_estimators': [200],\n",
    "           'max_depth':  [24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54],\n",
    "           'min_samples_split': [2, 4, 6, 8, 10, 12, 14, 16], \n",
    "           'min_samples_leaf': [1, 3, 5, 7, 9, 11, 13, 15, 17],\n",
    "           'max_features': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\n",
    "\n",
    "ms.ms_on_hpc(model='rf',\n",
    "             data_dir='/q/PET-MBF/data',\n",
    "             out_dir='/q/PET-MBF/output',\n",
    "             hp_spec=hp_spec,\n",
    "             dataset='17_segment',\n",
    "             problem='localization',\n",
    "             ms_round=2,\n",
    "             mem='8GB',\n",
    "             time='48:00:00',\n",
    "             cpus=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \r\n",
      "               595     batch jupyter- dberman_  R      20:16      1 ohi-hpc2-keon01 \r\n"
     ]
    }
   ],
   "source": [
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check efficiency of job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 592\r\n",
      "Cluster: deepops\r\n",
      "Use of uninitialized value $user in concatenation (.) or string at /usr/local/bin/seff line 154, <DATA> line 602.\r\n",
      "User/Group: /domain users\r\n",
      "State: COMPLETED (exit code 0)\r\n",
      "Nodes: 1\r\n",
      "Cores per node: 20\r\n",
      "CPU Utilized: 22-08:13:43\r\n",
      "CPU Efficiency: 99.38% of 22-11:33:20 core-walltime\r\n",
      "Job Wall-clock time: 1-02:58:40\r\n",
      "Memory Utilized: 2.33 GB\r\n",
      "Memory Efficiency: 29.15% of 8.00 GB\r\n"
     ]
    }
   ],
   "source": [
    "!seff 592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 param settings mean_test_accuracy:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2882</th>\n",
       "      <td>0.584725</td>\n",
       "      <td>0.015421</td>\n",
       "      <td>0.755645</td>\n",
       "      <td>0.029374</td>\n",
       "      <td>0.464504</td>\n",
       "      <td>0.023223</td>\n",
       "      <td>0.570885</td>\n",
       "      <td>0.020691</td>\n",
       "      <td>0.898350</td>\n",
       "      <td>0.008396</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>0.584708</td>\n",
       "      <td>0.018985</td>\n",
       "      <td>0.747470</td>\n",
       "      <td>0.031340</td>\n",
       "      <td>0.495725</td>\n",
       "      <td>0.018110</td>\n",
       "      <td>0.592755</td>\n",
       "      <td>0.018734</td>\n",
       "      <td>0.897178</td>\n",
       "      <td>0.009404</td>\n",
       "      <td>39</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3170</th>\n",
       "      <td>0.584699</td>\n",
       "      <td>0.019189</td>\n",
       "      <td>0.747030</td>\n",
       "      <td>0.031903</td>\n",
       "      <td>0.494180</td>\n",
       "      <td>0.019874</td>\n",
       "      <td>0.591541</td>\n",
       "      <td>0.020461</td>\n",
       "      <td>0.896751</td>\n",
       "      <td>0.009301</td>\n",
       "      <td>36</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322</th>\n",
       "      <td>0.584342</td>\n",
       "      <td>0.015522</td>\n",
       "      <td>0.755913</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>0.464612</td>\n",
       "      <td>0.024454</td>\n",
       "      <td>0.571007</td>\n",
       "      <td>0.021164</td>\n",
       "      <td>0.898475</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>42</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5762</th>\n",
       "      <td>0.584342</td>\n",
       "      <td>0.015522</td>\n",
       "      <td>0.755614</td>\n",
       "      <td>0.028987</td>\n",
       "      <td>0.464959</td>\n",
       "      <td>0.024152</td>\n",
       "      <td>0.571204</td>\n",
       "      <td>0.021034</td>\n",
       "      <td>0.898647</td>\n",
       "      <td>0.008655</td>\n",
       "      <td>48</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7202</th>\n",
       "      <td>0.584342</td>\n",
       "      <td>0.015522</td>\n",
       "      <td>0.755614</td>\n",
       "      <td>0.028987</td>\n",
       "      <td>0.464959</td>\n",
       "      <td>0.024152</td>\n",
       "      <td>0.571204</td>\n",
       "      <td>0.021034</td>\n",
       "      <td>0.898554</td>\n",
       "      <td>0.008724</td>\n",
       "      <td>54</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6482</th>\n",
       "      <td>0.584342</td>\n",
       "      <td>0.015522</td>\n",
       "      <td>0.755614</td>\n",
       "      <td>0.028987</td>\n",
       "      <td>0.464959</td>\n",
       "      <td>0.024152</td>\n",
       "      <td>0.571204</td>\n",
       "      <td>0.021034</td>\n",
       "      <td>0.898543</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>51</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>0.584341</td>\n",
       "      <td>0.016550</td>\n",
       "      <td>0.751800</td>\n",
       "      <td>0.029883</td>\n",
       "      <td>0.487072</td>\n",
       "      <td>0.029014</td>\n",
       "      <td>0.588238</td>\n",
       "      <td>0.027114</td>\n",
       "      <td>0.898395</td>\n",
       "      <td>0.009032</td>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>0.584341</td>\n",
       "      <td>0.016550</td>\n",
       "      <td>0.751800</td>\n",
       "      <td>0.029883</td>\n",
       "      <td>0.487072</td>\n",
       "      <td>0.029014</td>\n",
       "      <td>0.588238</td>\n",
       "      <td>0.027114</td>\n",
       "      <td>0.898395</td>\n",
       "      <td>0.009032</td>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>0.584341</td>\n",
       "      <td>0.016550</td>\n",
       "      <td>0.751800</td>\n",
       "      <td>0.029883</td>\n",
       "      <td>0.487072</td>\n",
       "      <td>0.029014</td>\n",
       "      <td>0.588238</td>\n",
       "      <td>0.027114</td>\n",
       "      <td>0.898395</td>\n",
       "      <td>0.009032</td>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3896</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898902</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>39</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6058</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898960</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>48</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898973</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>54</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751479</td>\n",
       "      <td>0.030606</td>\n",
       "      <td>0.487392</td>\n",
       "      <td>0.029035</td>\n",
       "      <td>0.588403</td>\n",
       "      <td>0.027314</td>\n",
       "      <td>0.898762</td>\n",
       "      <td>0.008959</td>\n",
       "      <td>36</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3897</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898902</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>39</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3898</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898902</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>39</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3177</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751479</td>\n",
       "      <td>0.030606</td>\n",
       "      <td>0.487392</td>\n",
       "      <td>0.029035</td>\n",
       "      <td>0.588403</td>\n",
       "      <td>0.027314</td>\n",
       "      <td>0.898762</td>\n",
       "      <td>0.008959</td>\n",
       "      <td>36</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898960</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>48</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898960</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>48</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3178</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751479</td>\n",
       "      <td>0.030606</td>\n",
       "      <td>0.487392</td>\n",
       "      <td>0.029035</td>\n",
       "      <td>0.588403</td>\n",
       "      <td>0.027314</td>\n",
       "      <td>0.898762</td>\n",
       "      <td>0.008959</td>\n",
       "      <td>36</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7496</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898973</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>54</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4618</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898871</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>42</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6778</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898961</td>\n",
       "      <td>0.008760</td>\n",
       "      <td>51</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6776</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898961</td>\n",
       "      <td>0.008760</td>\n",
       "      <td>51</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4616</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898871</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>42</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4617</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898871</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>42</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898903</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5337</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898903</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751448</td>\n",
       "      <td>0.030520</td>\n",
       "      <td>0.487319</td>\n",
       "      <td>0.029109</td>\n",
       "      <td>0.588318</td>\n",
       "      <td>0.027224</td>\n",
       "      <td>0.898537</td>\n",
       "      <td>0.009096</td>\n",
       "      <td>33</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751448</td>\n",
       "      <td>0.030520</td>\n",
       "      <td>0.487319</td>\n",
       "      <td>0.029109</td>\n",
       "      <td>0.588318</td>\n",
       "      <td>0.027224</td>\n",
       "      <td>0.898537</td>\n",
       "      <td>0.009096</td>\n",
       "      <td>33</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751448</td>\n",
       "      <td>0.030520</td>\n",
       "      <td>0.487319</td>\n",
       "      <td>0.029109</td>\n",
       "      <td>0.588318</td>\n",
       "      <td>0.027224</td>\n",
       "      <td>0.898537</td>\n",
       "      <td>0.009096</td>\n",
       "      <td>33</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6777</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898961</td>\n",
       "      <td>0.008760</td>\n",
       "      <td>51</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898973</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>54</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5336</th>\n",
       "      <td>0.584339</td>\n",
       "      <td>0.016939</td>\n",
       "      <td>0.751520</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>0.487707</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.898903</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>0.584333</td>\n",
       "      <td>0.017347</td>\n",
       "      <td>0.751827</td>\n",
       "      <td>0.029407</td>\n",
       "      <td>0.484077</td>\n",
       "      <td>0.023042</td>\n",
       "      <td>0.585698</td>\n",
       "      <td>0.022748</td>\n",
       "      <td>0.898478</td>\n",
       "      <td>0.009130</td>\n",
       "      <td>33</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.584330</td>\n",
       "      <td>0.016986</td>\n",
       "      <td>0.751865</td>\n",
       "      <td>0.029465</td>\n",
       "      <td>0.486803</td>\n",
       "      <td>0.029970</td>\n",
       "      <td>0.588057</td>\n",
       "      <td>0.027711</td>\n",
       "      <td>0.897047</td>\n",
       "      <td>0.009438</td>\n",
       "      <td>24</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.584330</td>\n",
       "      <td>0.016986</td>\n",
       "      <td>0.751865</td>\n",
       "      <td>0.029465</td>\n",
       "      <td>0.486803</td>\n",
       "      <td>0.029970</td>\n",
       "      <td>0.588057</td>\n",
       "      <td>0.027711</td>\n",
       "      <td>0.897047</td>\n",
       "      <td>0.009438</td>\n",
       "      <td>24</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.584330</td>\n",
       "      <td>0.016986</td>\n",
       "      <td>0.751865</td>\n",
       "      <td>0.029465</td>\n",
       "      <td>0.486803</td>\n",
       "      <td>0.029970</td>\n",
       "      <td>0.588057</td>\n",
       "      <td>0.027711</td>\n",
       "      <td>0.897047</td>\n",
       "      <td>0.009438</td>\n",
       "      <td>24</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7490</th>\n",
       "      <td>0.584324</td>\n",
       "      <td>0.017817</td>\n",
       "      <td>0.746647</td>\n",
       "      <td>0.030665</td>\n",
       "      <td>0.496622</td>\n",
       "      <td>0.018824</td>\n",
       "      <td>0.593197</td>\n",
       "      <td>0.019037</td>\n",
       "      <td>0.897621</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>54</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4470</th>\n",
       "      <td>0.584322</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.746027</td>\n",
       "      <td>0.032031</td>\n",
       "      <td>0.485101</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.584184</td>\n",
       "      <td>0.021417</td>\n",
       "      <td>0.897425</td>\n",
       "      <td>0.009433</td>\n",
       "      <td>42</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>0.584322</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.746271</td>\n",
       "      <td>0.031973</td>\n",
       "      <td>0.485101</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.584278</td>\n",
       "      <td>0.021566</td>\n",
       "      <td>0.897001</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>39</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>0.583965</td>\n",
       "      <td>0.019428</td>\n",
       "      <td>0.741435</td>\n",
       "      <td>0.026449</td>\n",
       "      <td>0.496219</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.591649</td>\n",
       "      <td>0.027006</td>\n",
       "      <td>0.896136</td>\n",
       "      <td>0.009667</td>\n",
       "      <td>33</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4105</th>\n",
       "      <td>0.583963</td>\n",
       "      <td>0.020564</td>\n",
       "      <td>0.740533</td>\n",
       "      <td>0.027493</td>\n",
       "      <td>0.496945</td>\n",
       "      <td>0.030977</td>\n",
       "      <td>0.591907</td>\n",
       "      <td>0.027587</td>\n",
       "      <td>0.896707</td>\n",
       "      <td>0.009695</td>\n",
       "      <td>39</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7705</th>\n",
       "      <td>0.583954</td>\n",
       "      <td>0.017748</td>\n",
       "      <td>0.740245</td>\n",
       "      <td>0.026537</td>\n",
       "      <td>0.499615</td>\n",
       "      <td>0.029935</td>\n",
       "      <td>0.593731</td>\n",
       "      <td>0.026577</td>\n",
       "      <td>0.897643</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>54</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7427</th>\n",
       "      <td>0.583948</td>\n",
       "      <td>0.016948</td>\n",
       "      <td>0.751669</td>\n",
       "      <td>0.029270</td>\n",
       "      <td>0.483715</td>\n",
       "      <td>0.022766</td>\n",
       "      <td>0.585395</td>\n",
       "      <td>0.022495</td>\n",
       "      <td>0.898485</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>54</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5267</th>\n",
       "      <td>0.583948</td>\n",
       "      <td>0.016948</td>\n",
       "      <td>0.751669</td>\n",
       "      <td>0.029270</td>\n",
       "      <td>0.483715</td>\n",
       "      <td>0.022766</td>\n",
       "      <td>0.585395</td>\n",
       "      <td>0.022495</td>\n",
       "      <td>0.898515</td>\n",
       "      <td>0.009083</td>\n",
       "      <td>45</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547</th>\n",
       "      <td>0.583948</td>\n",
       "      <td>0.016948</td>\n",
       "      <td>0.751669</td>\n",
       "      <td>0.029270</td>\n",
       "      <td>0.483715</td>\n",
       "      <td>0.022766</td>\n",
       "      <td>0.585395</td>\n",
       "      <td>0.022495</td>\n",
       "      <td>0.898441</td>\n",
       "      <td>0.009075</td>\n",
       "      <td>42</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>0.583948</td>\n",
       "      <td>0.016948</td>\n",
       "      <td>0.751493</td>\n",
       "      <td>0.029125</td>\n",
       "      <td>0.483368</td>\n",
       "      <td>0.022547</td>\n",
       "      <td>0.585067</td>\n",
       "      <td>0.022259</td>\n",
       "      <td>0.898124</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>30</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3827</th>\n",
       "      <td>0.583948</td>\n",
       "      <td>0.016948</td>\n",
       "      <td>0.751493</td>\n",
       "      <td>0.029125</td>\n",
       "      <td>0.483368</td>\n",
       "      <td>0.022547</td>\n",
       "      <td>0.585067</td>\n",
       "      <td>0.022259</td>\n",
       "      <td>0.898516</td>\n",
       "      <td>0.009159</td>\n",
       "      <td>39</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5987</th>\n",
       "      <td>0.583948</td>\n",
       "      <td>0.016948</td>\n",
       "      <td>0.751669</td>\n",
       "      <td>0.029270</td>\n",
       "      <td>0.483715</td>\n",
       "      <td>0.022766</td>\n",
       "      <td>0.585395</td>\n",
       "      <td>0.022495</td>\n",
       "      <td>0.898482</td>\n",
       "      <td>0.009096</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "2882            0.584725           0.015421             0.755645   \n",
       "3890            0.584708           0.018985             0.747470   \n",
       "3170            0.584699           0.019189             0.747030   \n",
       "4322            0.584342           0.015522             0.755913   \n",
       "5762            0.584342           0.015522             0.755614   \n",
       "7202            0.584342           0.015522             0.755614   \n",
       "6482            0.584342           0.015522             0.755614   \n",
       "1738            0.584341           0.016550             0.751800   \n",
       "1737            0.584341           0.016550             0.751800   \n",
       "1736            0.584341           0.016550             0.751800   \n",
       "3896            0.584339           0.016939             0.751520   \n",
       "6058            0.584339           0.016939             0.751520   \n",
       "7498            0.584339           0.016939             0.751520   \n",
       "3176            0.584339           0.016939             0.751479   \n",
       "3897            0.584339           0.016939             0.751520   \n",
       "3898            0.584339           0.016939             0.751520   \n",
       "3177            0.584339           0.016939             0.751479   \n",
       "6057            0.584339           0.016939             0.751520   \n",
       "6056            0.584339           0.016939             0.751520   \n",
       "3178            0.584339           0.016939             0.751479   \n",
       "7496            0.584339           0.016939             0.751520   \n",
       "4618            0.584339           0.016939             0.751520   \n",
       "6778            0.584339           0.016939             0.751520   \n",
       "6776            0.584339           0.016939             0.751520   \n",
       "4616            0.584339           0.016939             0.751520   \n",
       "4617            0.584339           0.016939             0.751520   \n",
       "5338            0.584339           0.016939             0.751520   \n",
       "5337            0.584339           0.016939             0.751520   \n",
       "2456            0.584339           0.016939             0.751448   \n",
       "2457            0.584339           0.016939             0.751448   \n",
       "2458            0.584339           0.016939             0.751448   \n",
       "6777            0.584339           0.016939             0.751520   \n",
       "7497            0.584339           0.016939             0.751520   \n",
       "5336            0.584339           0.016939             0.751520   \n",
       "2387            0.584333           0.017347             0.751827   \n",
       "296             0.584330           0.016986             0.751865   \n",
       "297             0.584330           0.016986             0.751865   \n",
       "298             0.584330           0.016986             0.751865   \n",
       "7490            0.584324           0.017817             0.746647   \n",
       "4470            0.584322           0.015064             0.746027   \n",
       "3750            0.584322           0.015064             0.746271   \n",
       "2665            0.583965           0.019428             0.741435   \n",
       "4105            0.583963           0.020564             0.740533   \n",
       "7705            0.583954           0.017748             0.740245   \n",
       "7427            0.583948           0.016948             0.751669   \n",
       "5267            0.583948           0.016948             0.751669   \n",
       "4547            0.583948           0.016948             0.751669   \n",
       "1667            0.583948           0.016948             0.751493   \n",
       "3827            0.583948           0.016948             0.751493   \n",
       "5987            0.583948           0.016948             0.751669   \n",
       "\n",
       "      std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "2882            0.029374          0.464504         0.023223      0.570885   \n",
       "3890            0.031340          0.495725         0.018110      0.592755   \n",
       "3170            0.031903          0.494180         0.019874      0.591541   \n",
       "4322            0.029025          0.464612         0.024454      0.571007   \n",
       "5762            0.028987          0.464959         0.024152      0.571204   \n",
       "7202            0.028987          0.464959         0.024152      0.571204   \n",
       "6482            0.028987          0.464959         0.024152      0.571204   \n",
       "1738            0.029883          0.487072         0.029014      0.588238   \n",
       "1737            0.029883          0.487072         0.029014      0.588238   \n",
       "1736            0.029883          0.487072         0.029014      0.588238   \n",
       "3896            0.030567          0.487707         0.028854      0.588679   \n",
       "6058            0.030567          0.487707         0.028854      0.588679   \n",
       "7498            0.030567          0.487707         0.028854      0.588679   \n",
       "3176            0.030606          0.487392         0.029035      0.588403   \n",
       "3897            0.030567          0.487707         0.028854      0.588679   \n",
       "3898            0.030567          0.487707         0.028854      0.588679   \n",
       "3177            0.030606          0.487392         0.029035      0.588403   \n",
       "6057            0.030567          0.487707         0.028854      0.588679   \n",
       "6056            0.030567          0.487707         0.028854      0.588679   \n",
       "3178            0.030606          0.487392         0.029035      0.588403   \n",
       "7496            0.030567          0.487707         0.028854      0.588679   \n",
       "4618            0.030567          0.487707         0.028854      0.588679   \n",
       "6778            0.030567          0.487707         0.028854      0.588679   \n",
       "6776            0.030567          0.487707         0.028854      0.588679   \n",
       "4616            0.030567          0.487707         0.028854      0.588679   \n",
       "4617            0.030567          0.487707         0.028854      0.588679   \n",
       "5338            0.030567          0.487707         0.028854      0.588679   \n",
       "5337            0.030567          0.487707         0.028854      0.588679   \n",
       "2456            0.030520          0.487319         0.029109      0.588318   \n",
       "2457            0.030520          0.487319         0.029109      0.588318   \n",
       "2458            0.030520          0.487319         0.029109      0.588318   \n",
       "6777            0.030567          0.487707         0.028854      0.588679   \n",
       "7497            0.030567          0.487707         0.028854      0.588679   \n",
       "5336            0.030567          0.487707         0.028854      0.588679   \n",
       "2387            0.029407          0.484077         0.023042      0.585698   \n",
       "296             0.029465          0.486803         0.029970      0.588057   \n",
       "297             0.029465          0.486803         0.029970      0.588057   \n",
       "298             0.029465          0.486803         0.029970      0.588057   \n",
       "7490            0.030665          0.496622         0.018824      0.593197   \n",
       "4470            0.032031          0.485101         0.022442      0.584184   \n",
       "3750            0.031973          0.485101         0.022442      0.584278   \n",
       "2665            0.026449          0.496219         0.030907      0.591649   \n",
       "4105            0.027493          0.496945         0.030977      0.591907   \n",
       "7705            0.026537          0.499615         0.029935      0.593731   \n",
       "7427            0.029270          0.483715         0.022766      0.585395   \n",
       "5267            0.029270          0.483715         0.022766      0.585395   \n",
       "4547            0.029270          0.483715         0.022766      0.585395   \n",
       "1667            0.029125          0.483368         0.022547      0.585067   \n",
       "3827            0.029125          0.483368         0.022547      0.585067   \n",
       "5987            0.029270          0.483715         0.022766      0.585395   \n",
       "\n",
       "      std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "2882     0.020691       0.898350      0.008396                   36   \n",
       "3890     0.018734       0.897178      0.009404                   39   \n",
       "3170     0.020461       0.896751      0.009301                   36   \n",
       "4322     0.021164       0.898475      0.008701                   42   \n",
       "5762     0.021034       0.898647      0.008655                   48   \n",
       "7202     0.021034       0.898554      0.008724                   54   \n",
       "6482     0.021034       0.898543      0.008693                   51   \n",
       "1738     0.027114       0.898395      0.009032                   30   \n",
       "1737     0.027114       0.898395      0.009032                   30   \n",
       "1736     0.027114       0.898395      0.009032                   30   \n",
       "3896     0.027103       0.898902      0.008716                   39   \n",
       "6058     0.027103       0.898960      0.008751                   48   \n",
       "7498     0.027103       0.898973      0.008759                   54   \n",
       "3176     0.027314       0.898762      0.008959                   36   \n",
       "3897     0.027103       0.898902      0.008716                   39   \n",
       "3898     0.027103       0.898902      0.008716                   39   \n",
       "3177     0.027314       0.898762      0.008959                   36   \n",
       "6057     0.027103       0.898960      0.008751                   48   \n",
       "6056     0.027103       0.898960      0.008751                   48   \n",
       "3178     0.027314       0.898762      0.008959                   36   \n",
       "7496     0.027103       0.898973      0.008759                   54   \n",
       "4618     0.027103       0.898871      0.008788                   42   \n",
       "6778     0.027103       0.898961      0.008760                   51   \n",
       "6776     0.027103       0.898961      0.008760                   51   \n",
       "4616     0.027103       0.898871      0.008788                   42   \n",
       "4617     0.027103       0.898871      0.008788                   42   \n",
       "5338     0.027103       0.898903      0.008782                   45   \n",
       "5337     0.027103       0.898903      0.008782                   45   \n",
       "2456     0.027224       0.898537      0.009096                   33   \n",
       "2457     0.027224       0.898537      0.009096                   33   \n",
       "2458     0.027224       0.898537      0.009096                   33   \n",
       "6777     0.027103       0.898961      0.008760                   51   \n",
       "7497     0.027103       0.898973      0.008759                   54   \n",
       "5336     0.027103       0.898903      0.008782                   45   \n",
       "2387     0.022748       0.898478      0.009130                   33   \n",
       "296      0.027711       0.897047      0.009438                   24   \n",
       "297      0.027711       0.897047      0.009438                   24   \n",
       "298      0.027711       0.897047      0.009438                   24   \n",
       "7490     0.019037       0.897621      0.008977                   54   \n",
       "4470     0.021417       0.897425      0.009433                   42   \n",
       "3750     0.021566       0.897001      0.009516                   39   \n",
       "2665     0.027006       0.896136      0.009667                   33   \n",
       "4105     0.027587       0.896707      0.009695                   39   \n",
       "7705     0.026577       0.897643      0.009532                   54   \n",
       "7427     0.022495       0.898485      0.009091                   54   \n",
       "5267     0.022495       0.898515      0.009083                   45   \n",
       "4547     0.022495       0.898441      0.009075                   42   \n",
       "1667     0.022259       0.898124      0.009474                   30   \n",
       "3827     0.022259       0.898516      0.009159                   39   \n",
       "5987     0.022495       0.898482      0.009096                   48   \n",
       "\n",
       "      param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "2882                     0.1                           1   \n",
       "3890                     0.5                           1   \n",
       "3170                     0.5                           1   \n",
       "4322                     0.1                           1   \n",
       "5762                     0.1                           1   \n",
       "7202                     0.1                           1   \n",
       "6482                     0.1                           1   \n",
       "1738                     0.5                           3   \n",
       "1737                     0.5                           3   \n",
       "1736                     0.5                           3   \n",
       "3896                     0.5                           3   \n",
       "6058                     0.5                           3   \n",
       "7498                     0.5                           3   \n",
       "3176                     0.5                           3   \n",
       "3897                     0.5                           3   \n",
       "3898                     0.5                           3   \n",
       "3177                     0.5                           3   \n",
       "6057                     0.5                           3   \n",
       "6056                     0.5                           3   \n",
       "3178                     0.5                           3   \n",
       "7496                     0.5                           3   \n",
       "4618                     0.5                           3   \n",
       "6778                     0.5                           3   \n",
       "6776                     0.5                           3   \n",
       "4616                     0.5                           3   \n",
       "4617                     0.5                           3   \n",
       "5338                     0.5                           3   \n",
       "5337                     0.5                           3   \n",
       "2456                     0.5                           3   \n",
       "2457                     0.5                           3   \n",
       "2458                     0.5                           3   \n",
       "6777                     0.5                           3   \n",
       "7497                     0.5                           3   \n",
       "5336                     0.5                           3   \n",
       "2387                     0.4                           3   \n",
       "296                      0.5                           3   \n",
       "297                      0.5                           3   \n",
       "298                      0.5                           3   \n",
       "7490                     0.5                           1   \n",
       "4470                     0.3                           1   \n",
       "3750                     0.3                           1   \n",
       "2665                     0.8                           1   \n",
       "4105                     0.8                           1   \n",
       "7705                     0.8                           1   \n",
       "7427                     0.4                           3   \n",
       "5267                     0.4                           3   \n",
       "4547                     0.4                           3   \n",
       "1667                     0.4                           3   \n",
       "3827                     0.4                           3   \n",
       "5987                     0.4                           3   \n",
       "\n",
       "      param_rf__min_samples_split  param_rf__n_estimators  \n",
       "2882                            6                     200  \n",
       "3890                            6                     200  \n",
       "3170                            6                     200  \n",
       "4322                            6                     200  \n",
       "5762                            6                     200  \n",
       "7202                            6                     200  \n",
       "6482                            6                     200  \n",
       "1738                            6                     200  \n",
       "1737                            4                     200  \n",
       "1736                            2                     200  \n",
       "3896                            2                     200  \n",
       "6058                            6                     200  \n",
       "7498                            6                     200  \n",
       "3176                            2                     200  \n",
       "3897                            4                     200  \n",
       "3898                            6                     200  \n",
       "3177                            4                     200  \n",
       "6057                            4                     200  \n",
       "6056                            2                     200  \n",
       "3178                            6                     200  \n",
       "7496                            2                     200  \n",
       "4618                            6                     200  \n",
       "6778                            6                     200  \n",
       "6776                            2                     200  \n",
       "4616                            2                     200  \n",
       "4617                            4                     200  \n",
       "5338                            6                     200  \n",
       "5337                            4                     200  \n",
       "2456                            2                     200  \n",
       "2457                            4                     200  \n",
       "2458                            6                     200  \n",
       "6777                            4                     200  \n",
       "7497                            4                     200  \n",
       "5336                            2                     200  \n",
       "2387                            8                     200  \n",
       "296                             2                     200  \n",
       "297                             4                     200  \n",
       "298                             6                     200  \n",
       "7490                            6                     200  \n",
       "4470                           14                     200  \n",
       "3750                           14                     200  \n",
       "2665                            4                     200  \n",
       "4105                            4                     200  \n",
       "7705                            4                     200  \n",
       "7427                            8                     200  \n",
       "5267                            8                     200  \n",
       "4547                            8                     200  \n",
       "1667                            8                     200  \n",
       "3827                            8                     200  \n",
       "5987                            8                     200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 5 param settings mean_test_precision:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3666</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>39</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5105</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5106</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5107</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3664</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>39</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3665</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>39</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.885976</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>24</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.885976</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>24</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3667</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>39</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>39</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>39</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3670</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>39</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5109</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5110</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5111</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>42</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4385</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>42</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4387</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>42</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5831</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>48</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4388</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>42</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4389</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>42</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5104</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2950</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>42</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7267</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>54</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7265</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>54</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7268</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>54</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7269</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>54</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7264</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>54</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>54</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7271</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>54</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>33</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>33</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>33</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>33</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2228</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>33</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>33</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>33</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>33</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2944</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.885976</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>24</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4390</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>42</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>0.561658</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.76424</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.377754</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.886001</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>42</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "3666            0.561658           0.006501              0.76424   \n",
       "2951            0.561658           0.006501              0.76424   \n",
       "5105            0.561658           0.006501              0.76424   \n",
       "5106            0.561658           0.006501              0.76424   \n",
       "5107            0.561658           0.006501              0.76424   \n",
       "5108            0.561658           0.006501              0.76424   \n",
       "3664            0.561658           0.006501              0.76424   \n",
       "3665            0.561658           0.006501              0.76424   \n",
       "65              0.561658           0.006501              0.76424   \n",
       "66              0.561658           0.006501              0.76424   \n",
       "3667            0.561658           0.006501              0.76424   \n",
       "3668            0.561658           0.006501              0.76424   \n",
       "3669            0.561658           0.006501              0.76424   \n",
       "3670            0.561658           0.006501              0.76424   \n",
       "5109            0.561658           0.006501              0.76424   \n",
       "5110            0.561658           0.006501              0.76424   \n",
       "5111            0.561658           0.006501              0.76424   \n",
       "4384            0.561658           0.006501              0.76424   \n",
       "4385            0.561658           0.006501              0.76424   \n",
       "4387            0.561658           0.006501              0.76424   \n",
       "5831            0.561658           0.006501              0.76424   \n",
       "4388            0.561658           0.006501              0.76424   \n",
       "4389            0.561658           0.006501              0.76424   \n",
       "5104            0.561658           0.006501              0.76424   \n",
       "2950            0.561658           0.006501              0.76424   \n",
       "4391            0.561658           0.006501              0.76424   \n",
       "2949            0.561658           0.006501              0.76424   \n",
       "7267            0.561658           0.006501              0.76424   \n",
       "7265            0.561658           0.006501              0.76424   \n",
       "7268            0.561658           0.006501              0.76424   \n",
       "7269            0.561658           0.006501              0.76424   \n",
       "7264            0.561658           0.006501              0.76424   \n",
       "7270            0.561658           0.006501              0.76424   \n",
       "7271            0.561658           0.006501              0.76424   \n",
       "2224            0.561658           0.006501              0.76424   \n",
       "2225            0.561658           0.006501              0.76424   \n",
       "2226            0.561658           0.006501              0.76424   \n",
       "2227            0.561658           0.006501              0.76424   \n",
       "2228            0.561658           0.006501              0.76424   \n",
       "2229            0.561658           0.006501              0.76424   \n",
       "2230            0.561658           0.006501              0.76424   \n",
       "2231            0.561658           0.006501              0.76424   \n",
       "2944            0.561658           0.006501              0.76424   \n",
       "2945            0.561658           0.006501              0.76424   \n",
       "2946            0.561658           0.006501              0.76424   \n",
       "2947            0.561658           0.006501              0.76424   \n",
       "2948            0.561658           0.006501              0.76424   \n",
       "64              0.561658           0.006501              0.76424   \n",
       "4390            0.561658           0.006501              0.76424   \n",
       "4386            0.561658           0.006501              0.76424   \n",
       "\n",
       "      std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "3666            0.034217          0.377754         0.020738      0.495231   \n",
       "2951            0.034217          0.377754         0.020738      0.495231   \n",
       "5105            0.034217          0.377754         0.020738      0.495231   \n",
       "5106            0.034217          0.377754         0.020738      0.495231   \n",
       "5107            0.034217          0.377754         0.020738      0.495231   \n",
       "5108            0.034217          0.377754         0.020738      0.495231   \n",
       "3664            0.034217          0.377754         0.020738      0.495231   \n",
       "3665            0.034217          0.377754         0.020738      0.495231   \n",
       "65              0.034217          0.377754         0.020738      0.495231   \n",
       "66              0.034217          0.377754         0.020738      0.495231   \n",
       "3667            0.034217          0.377754         0.020738      0.495231   \n",
       "3668            0.034217          0.377754         0.020738      0.495231   \n",
       "3669            0.034217          0.377754         0.020738      0.495231   \n",
       "3670            0.034217          0.377754         0.020738      0.495231   \n",
       "5109            0.034217          0.377754         0.020738      0.495231   \n",
       "5110            0.034217          0.377754         0.020738      0.495231   \n",
       "5111            0.034217          0.377754         0.020738      0.495231   \n",
       "4384            0.034217          0.377754         0.020738      0.495231   \n",
       "4385            0.034217          0.377754         0.020738      0.495231   \n",
       "4387            0.034217          0.377754         0.020738      0.495231   \n",
       "5831            0.034217          0.377754         0.020738      0.495231   \n",
       "4388            0.034217          0.377754         0.020738      0.495231   \n",
       "4389            0.034217          0.377754         0.020738      0.495231   \n",
       "5104            0.034217          0.377754         0.020738      0.495231   \n",
       "2950            0.034217          0.377754         0.020738      0.495231   \n",
       "4391            0.034217          0.377754         0.020738      0.495231   \n",
       "2949            0.034217          0.377754         0.020738      0.495231   \n",
       "7267            0.034217          0.377754         0.020738      0.495231   \n",
       "7265            0.034217          0.377754         0.020738      0.495231   \n",
       "7268            0.034217          0.377754         0.020738      0.495231   \n",
       "7269            0.034217          0.377754         0.020738      0.495231   \n",
       "7264            0.034217          0.377754         0.020738      0.495231   \n",
       "7270            0.034217          0.377754         0.020738      0.495231   \n",
       "7271            0.034217          0.377754         0.020738      0.495231   \n",
       "2224            0.034217          0.377754         0.020738      0.495231   \n",
       "2225            0.034217          0.377754         0.020738      0.495231   \n",
       "2226            0.034217          0.377754         0.020738      0.495231   \n",
       "2227            0.034217          0.377754         0.020738      0.495231   \n",
       "2228            0.034217          0.377754         0.020738      0.495231   \n",
       "2229            0.034217          0.377754         0.020738      0.495231   \n",
       "2230            0.034217          0.377754         0.020738      0.495231   \n",
       "2231            0.034217          0.377754         0.020738      0.495231   \n",
       "2944            0.034217          0.377754         0.020738      0.495231   \n",
       "2945            0.034217          0.377754         0.020738      0.495231   \n",
       "2946            0.034217          0.377754         0.020738      0.495231   \n",
       "2947            0.034217          0.377754         0.020738      0.495231   \n",
       "2948            0.034217          0.377754         0.020738      0.495231   \n",
       "64              0.034217          0.377754         0.020738      0.495231   \n",
       "4390            0.034217          0.377754         0.020738      0.495231   \n",
       "4386            0.034217          0.377754         0.020738      0.495231   \n",
       "\n",
       "      std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "3666     0.017046       0.886001      0.011045                   39   \n",
       "2951     0.017046       0.886001      0.011045                   36   \n",
       "5105     0.017046       0.886001      0.011045                   45   \n",
       "5106     0.017046       0.886001      0.011045                   45   \n",
       "5107     0.017046       0.886001      0.011045                   45   \n",
       "5108     0.017046       0.886001      0.011045                   45   \n",
       "3664     0.017046       0.886001      0.011045                   39   \n",
       "3665     0.017046       0.886001      0.011045                   39   \n",
       "65       0.017046       0.885976      0.011062                   24   \n",
       "66       0.017046       0.885976      0.011062                   24   \n",
       "3667     0.017046       0.886001      0.011045                   39   \n",
       "3668     0.017046       0.886001      0.011045                   39   \n",
       "3669     0.017046       0.886001      0.011045                   39   \n",
       "3670     0.017046       0.886001      0.011045                   39   \n",
       "5109     0.017046       0.886001      0.011045                   45   \n",
       "5110     0.017046       0.886001      0.011045                   45   \n",
       "5111     0.017046       0.886001      0.011045                   45   \n",
       "4384     0.017046       0.886001      0.011045                   42   \n",
       "4385     0.017046       0.886001      0.011045                   42   \n",
       "4387     0.017046       0.886001      0.011045                   42   \n",
       "5831     0.017046       0.886001      0.011045                   48   \n",
       "4388     0.017046       0.886001      0.011045                   42   \n",
       "4389     0.017046       0.886001      0.011045                   42   \n",
       "5104     0.017046       0.886001      0.011045                   45   \n",
       "2950     0.017046       0.886001      0.011045                   36   \n",
       "4391     0.017046       0.886001      0.011045                   42   \n",
       "2949     0.017046       0.886001      0.011045                   36   \n",
       "7267     0.017046       0.886001      0.011045                   54   \n",
       "7265     0.017046       0.886001      0.011045                   54   \n",
       "7268     0.017046       0.886001      0.011045                   54   \n",
       "7269     0.017046       0.886001      0.011045                   54   \n",
       "7264     0.017046       0.886001      0.011045                   54   \n",
       "7270     0.017046       0.886001      0.011045                   54   \n",
       "7271     0.017046       0.886001      0.011045                   54   \n",
       "2224     0.017046       0.886001      0.011045                   33   \n",
       "2225     0.017046       0.886001      0.011045                   33   \n",
       "2226     0.017046       0.886001      0.011045                   33   \n",
       "2227     0.017046       0.886001      0.011045                   33   \n",
       "2228     0.017046       0.886001      0.011045                   33   \n",
       "2229     0.017046       0.886001      0.011045                   33   \n",
       "2230     0.017046       0.886001      0.011045                   33   \n",
       "2231     0.017046       0.886001      0.011045                   33   \n",
       "2944     0.017046       0.886001      0.011045                   36   \n",
       "2945     0.017046       0.886001      0.011045                   36   \n",
       "2946     0.017046       0.886001      0.011045                   36   \n",
       "2947     0.017046       0.886001      0.011045                   36   \n",
       "2948     0.017046       0.886001      0.011045                   36   \n",
       "64       0.017046       0.885976      0.011062                   24   \n",
       "4390     0.017046       0.886001      0.011045                   42   \n",
       "4386     0.017046       0.886001      0.011045                   42   \n",
       "\n",
       "      param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "3666                     0.1                          17   \n",
       "2951                     0.1                          17   \n",
       "5105                     0.1                          17   \n",
       "5106                     0.1                          17   \n",
       "5107                     0.1                          17   \n",
       "5108                     0.1                          17   \n",
       "3664                     0.1                          17   \n",
       "3665                     0.1                          17   \n",
       "65                       0.1                          17   \n",
       "66                       0.1                          17   \n",
       "3667                     0.1                          17   \n",
       "3668                     0.1                          17   \n",
       "3669                     0.1                          17   \n",
       "3670                     0.1                          17   \n",
       "5109                     0.1                          17   \n",
       "5110                     0.1                          17   \n",
       "5111                     0.1                          17   \n",
       "4384                     0.1                          17   \n",
       "4385                     0.1                          17   \n",
       "4387                     0.1                          17   \n",
       "5831                     0.1                          17   \n",
       "4388                     0.1                          17   \n",
       "4389                     0.1                          17   \n",
       "5104                     0.1                          17   \n",
       "2950                     0.1                          17   \n",
       "4391                     0.1                          17   \n",
       "2949                     0.1                          17   \n",
       "7267                     0.1                          17   \n",
       "7265                     0.1                          17   \n",
       "7268                     0.1                          17   \n",
       "7269                     0.1                          17   \n",
       "7264                     0.1                          17   \n",
       "7270                     0.1                          17   \n",
       "7271                     0.1                          17   \n",
       "2224                     0.1                          17   \n",
       "2225                     0.1                          17   \n",
       "2226                     0.1                          17   \n",
       "2227                     0.1                          17   \n",
       "2228                     0.1                          17   \n",
       "2229                     0.1                          17   \n",
       "2230                     0.1                          17   \n",
       "2231                     0.1                          17   \n",
       "2944                     0.1                          17   \n",
       "2945                     0.1                          17   \n",
       "2946                     0.1                          17   \n",
       "2947                     0.1                          17   \n",
       "2948                     0.1                          17   \n",
       "64                       0.1                          17   \n",
       "4390                     0.1                          17   \n",
       "4386                     0.1                          17   \n",
       "\n",
       "      param_rf__min_samples_split  param_rf__n_estimators  \n",
       "3666                            6                     200  \n",
       "2951                           16                     200  \n",
       "5105                            4                     200  \n",
       "5106                            6                     200  \n",
       "5107                            8                     200  \n",
       "5108                           10                     200  \n",
       "3664                            2                     200  \n",
       "3665                            4                     200  \n",
       "65                              4                     200  \n",
       "66                              6                     200  \n",
       "3667                            8                     200  \n",
       "3668                           10                     200  \n",
       "3669                           12                     200  \n",
       "3670                           14                     200  \n",
       "5109                           12                     200  \n",
       "5110                           14                     200  \n",
       "5111                           16                     200  \n",
       "4384                            2                     200  \n",
       "4385                            4                     200  \n",
       "4387                            8                     200  \n",
       "5831                           16                     200  \n",
       "4388                           10                     200  \n",
       "4389                           12                     200  \n",
       "5104                            2                     200  \n",
       "2950                           14                     200  \n",
       "4391                           16                     200  \n",
       "2949                           12                     200  \n",
       "7267                            8                     200  \n",
       "7265                            4                     200  \n",
       "7268                           10                     200  \n",
       "7269                           12                     200  \n",
       "7264                            2                     200  \n",
       "7270                           14                     200  \n",
       "7271                           16                     200  \n",
       "2224                            2                     200  \n",
       "2225                            4                     200  \n",
       "2226                            6                     200  \n",
       "2227                            8                     200  \n",
       "2228                           10                     200  \n",
       "2229                           12                     200  \n",
       "2230                           14                     200  \n",
       "2231                           16                     200  \n",
       "2944                            2                     200  \n",
       "2945                            4                     200  \n",
       "2946                            6                     200  \n",
       "2947                            8                     200  \n",
       "2948                           10                     200  \n",
       "64                              2                     200  \n",
       "4390                           14                     200  \n",
       "4386                            6                     200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 5 param settings mean_test_recall:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7706</th>\n",
       "      <td>0.578118</td>\n",
       "      <td>0.020128</td>\n",
       "      <td>0.742634</td>\n",
       "      <td>0.031288</td>\n",
       "      <td>0.499921</td>\n",
       "      <td>0.025514</td>\n",
       "      <td>0.594710</td>\n",
       "      <td>0.024425</td>\n",
       "      <td>0.897382</td>\n",
       "      <td>0.009126</td>\n",
       "      <td>54</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6409</th>\n",
       "      <td>0.580073</td>\n",
       "      <td>0.020694</td>\n",
       "      <td>0.737363</td>\n",
       "      <td>0.030502</td>\n",
       "      <td>0.499691</td>\n",
       "      <td>0.028018</td>\n",
       "      <td>0.593014</td>\n",
       "      <td>0.026019</td>\n",
       "      <td>0.896247</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7129</th>\n",
       "      <td>0.580073</td>\n",
       "      <td>0.020694</td>\n",
       "      <td>0.737363</td>\n",
       "      <td>0.030502</td>\n",
       "      <td>0.499691</td>\n",
       "      <td>0.028018</td>\n",
       "      <td>0.593014</td>\n",
       "      <td>0.026019</td>\n",
       "      <td>0.896345</td>\n",
       "      <td>0.009731</td>\n",
       "      <td>51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7705</th>\n",
       "      <td>0.583954</td>\n",
       "      <td>0.017748</td>\n",
       "      <td>0.740245</td>\n",
       "      <td>0.026537</td>\n",
       "      <td>0.499615</td>\n",
       "      <td>0.029935</td>\n",
       "      <td>0.593731</td>\n",
       "      <td>0.026577</td>\n",
       "      <td>0.897643</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>54</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6986</th>\n",
       "      <td>0.578118</td>\n",
       "      <td>0.020128</td>\n",
       "      <td>0.742045</td>\n",
       "      <td>0.031220</td>\n",
       "      <td>0.499458</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.594232</td>\n",
       "      <td>0.024085</td>\n",
       "      <td>0.897239</td>\n",
       "      <td>0.009230</td>\n",
       "      <td>51</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7849</th>\n",
       "      <td>0.580458</td>\n",
       "      <td>0.020881</td>\n",
       "      <td>0.737615</td>\n",
       "      <td>0.030847</td>\n",
       "      <td>0.499365</td>\n",
       "      <td>0.028382</td>\n",
       "      <td>0.592871</td>\n",
       "      <td>0.026267</td>\n",
       "      <td>0.896482</td>\n",
       "      <td>0.009607</td>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6985</th>\n",
       "      <td>0.583570</td>\n",
       "      <td>0.017937</td>\n",
       "      <td>0.740072</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.030113</td>\n",
       "      <td>0.593422</td>\n",
       "      <td>0.026779</td>\n",
       "      <td>0.897749</td>\n",
       "      <td>0.009440</td>\n",
       "      <td>51</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6266</th>\n",
       "      <td>0.578503</td>\n",
       "      <td>0.020788</td>\n",
       "      <td>0.742241</td>\n",
       "      <td>0.031513</td>\n",
       "      <td>0.499131</td>\n",
       "      <td>0.025023</td>\n",
       "      <td>0.594069</td>\n",
       "      <td>0.024418</td>\n",
       "      <td>0.897315</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>48</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4826</th>\n",
       "      <td>0.578503</td>\n",
       "      <td>0.020717</td>\n",
       "      <td>0.742639</td>\n",
       "      <td>0.031789</td>\n",
       "      <td>0.499038</td>\n",
       "      <td>0.024928</td>\n",
       "      <td>0.594055</td>\n",
       "      <td>0.024452</td>\n",
       "      <td>0.897027</td>\n",
       "      <td>0.009221</td>\n",
       "      <td>42</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>0.583570</td>\n",
       "      <td>0.019212</td>\n",
       "      <td>0.740310</td>\n",
       "      <td>0.027073</td>\n",
       "      <td>0.498905</td>\n",
       "      <td>0.030628</td>\n",
       "      <td>0.593277</td>\n",
       "      <td>0.027353</td>\n",
       "      <td>0.897610</td>\n",
       "      <td>0.009513</td>\n",
       "      <td>48</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>0.580082</td>\n",
       "      <td>0.020644</td>\n",
       "      <td>0.736954</td>\n",
       "      <td>0.030044</td>\n",
       "      <td>0.498866</td>\n",
       "      <td>0.027348</td>\n",
       "      <td>0.592327</td>\n",
       "      <td>0.025649</td>\n",
       "      <td>0.896073</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>0.578118</td>\n",
       "      <td>0.020779</td>\n",
       "      <td>0.742062</td>\n",
       "      <td>0.031691</td>\n",
       "      <td>0.498769</td>\n",
       "      <td>0.025148</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.024628</td>\n",
       "      <td>0.897270</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>45</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7707</th>\n",
       "      <td>0.579679</td>\n",
       "      <td>0.021492</td>\n",
       "      <td>0.745389</td>\n",
       "      <td>0.030959</td>\n",
       "      <td>0.498725</td>\n",
       "      <td>0.021689</td>\n",
       "      <td>0.594559</td>\n",
       "      <td>0.021123</td>\n",
       "      <td>0.897241</td>\n",
       "      <td>0.008566</td>\n",
       "      <td>54</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6987</th>\n",
       "      <td>0.579679</td>\n",
       "      <td>0.021492</td>\n",
       "      <td>0.745389</td>\n",
       "      <td>0.030959</td>\n",
       "      <td>0.498725</td>\n",
       "      <td>0.021689</td>\n",
       "      <td>0.594559</td>\n",
       "      <td>0.021123</td>\n",
       "      <td>0.897247</td>\n",
       "      <td>0.008641</td>\n",
       "      <td>51</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>0.579280</td>\n",
       "      <td>0.015199</td>\n",
       "      <td>0.742388</td>\n",
       "      <td>0.032779</td>\n",
       "      <td>0.498694</td>\n",
       "      <td>0.023052</td>\n",
       "      <td>0.592978</td>\n",
       "      <td>0.023348</td>\n",
       "      <td>0.897275</td>\n",
       "      <td>0.009504</td>\n",
       "      <td>48</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5329</th>\n",
       "      <td>0.579280</td>\n",
       "      <td>0.015199</td>\n",
       "      <td>0.742388</td>\n",
       "      <td>0.032779</td>\n",
       "      <td>0.498694</td>\n",
       "      <td>0.023052</td>\n",
       "      <td>0.592978</td>\n",
       "      <td>0.023348</td>\n",
       "      <td>0.897286</td>\n",
       "      <td>0.009228</td>\n",
       "      <td>45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489</th>\n",
       "      <td>0.579663</td>\n",
       "      <td>0.015224</td>\n",
       "      <td>0.742582</td>\n",
       "      <td>0.033479</td>\n",
       "      <td>0.498694</td>\n",
       "      <td>0.023052</td>\n",
       "      <td>0.593032</td>\n",
       "      <td>0.023449</td>\n",
       "      <td>0.897207</td>\n",
       "      <td>0.009296</td>\n",
       "      <td>54</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6769</th>\n",
       "      <td>0.579663</td>\n",
       "      <td>0.015224</td>\n",
       "      <td>0.743023</td>\n",
       "      <td>0.033417</td>\n",
       "      <td>0.498694</td>\n",
       "      <td>0.023052</td>\n",
       "      <td>0.593145</td>\n",
       "      <td>0.023368</td>\n",
       "      <td>0.897288</td>\n",
       "      <td>0.009273</td>\n",
       "      <td>51</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>0.583185</td>\n",
       "      <td>0.019980</td>\n",
       "      <td>0.740191</td>\n",
       "      <td>0.027224</td>\n",
       "      <td>0.498558</td>\n",
       "      <td>0.031169</td>\n",
       "      <td>0.592996</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.897463</td>\n",
       "      <td>0.009312</td>\n",
       "      <td>45</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5689</th>\n",
       "      <td>0.580082</td>\n",
       "      <td>0.020644</td>\n",
       "      <td>0.736820</td>\n",
       "      <td>0.030019</td>\n",
       "      <td>0.498539</td>\n",
       "      <td>0.027711</td>\n",
       "      <td>0.592047</td>\n",
       "      <td>0.025873</td>\n",
       "      <td>0.896201</td>\n",
       "      <td>0.009368</td>\n",
       "      <td>45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7635</th>\n",
       "      <td>0.582790</td>\n",
       "      <td>0.015301</td>\n",
       "      <td>0.745467</td>\n",
       "      <td>0.031977</td>\n",
       "      <td>0.498492</td>\n",
       "      <td>0.024527</td>\n",
       "      <td>0.594424</td>\n",
       "      <td>0.023968</td>\n",
       "      <td>0.896836</td>\n",
       "      <td>0.009276</td>\n",
       "      <td>54</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6195</th>\n",
       "      <td>0.582396</td>\n",
       "      <td>0.015550</td>\n",
       "      <td>0.745255</td>\n",
       "      <td>0.032230</td>\n",
       "      <td>0.498492</td>\n",
       "      <td>0.024527</td>\n",
       "      <td>0.594335</td>\n",
       "      <td>0.024070</td>\n",
       "      <td>0.896790</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>48</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>0.582396</td>\n",
       "      <td>0.015550</td>\n",
       "      <td>0.745255</td>\n",
       "      <td>0.032230</td>\n",
       "      <td>0.498492</td>\n",
       "      <td>0.024527</td>\n",
       "      <td>0.594335</td>\n",
       "      <td>0.024070</td>\n",
       "      <td>0.896775</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>51</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>0.580082</td>\n",
       "      <td>0.020644</td>\n",
       "      <td>0.737437</td>\n",
       "      <td>0.030480</td>\n",
       "      <td>0.498465</td>\n",
       "      <td>0.027755</td>\n",
       "      <td>0.592191</td>\n",
       "      <td>0.025948</td>\n",
       "      <td>0.895694</td>\n",
       "      <td>0.009568</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7777</th>\n",
       "      <td>0.576995</td>\n",
       "      <td>0.015286</td>\n",
       "      <td>0.735814</td>\n",
       "      <td>0.028122</td>\n",
       "      <td>0.498347</td>\n",
       "      <td>0.027790</td>\n",
       "      <td>0.591363</td>\n",
       "      <td>0.024967</td>\n",
       "      <td>0.896923</td>\n",
       "      <td>0.008550</td>\n",
       "      <td>54</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4106</th>\n",
       "      <td>0.578129</td>\n",
       "      <td>0.020186</td>\n",
       "      <td>0.742621</td>\n",
       "      <td>0.031387</td>\n",
       "      <td>0.498343</td>\n",
       "      <td>0.024976</td>\n",
       "      <td>0.593554</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>0.896639</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>39</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>0.579673</td>\n",
       "      <td>0.014804</td>\n",
       "      <td>0.742460</td>\n",
       "      <td>0.032721</td>\n",
       "      <td>0.498215</td>\n",
       "      <td>0.022723</td>\n",
       "      <td>0.592689</td>\n",
       "      <td>0.023253</td>\n",
       "      <td>0.897240</td>\n",
       "      <td>0.009313</td>\n",
       "      <td>42</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4825</th>\n",
       "      <td>0.583570</td>\n",
       "      <td>0.020188</td>\n",
       "      <td>0.740481</td>\n",
       "      <td>0.027571</td>\n",
       "      <td>0.498095</td>\n",
       "      <td>0.030339</td>\n",
       "      <td>0.592774</td>\n",
       "      <td>0.027249</td>\n",
       "      <td>0.897264</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>42</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>0.583559</td>\n",
       "      <td>0.015277</td>\n",
       "      <td>0.745916</td>\n",
       "      <td>0.031866</td>\n",
       "      <td>0.498042</td>\n",
       "      <td>0.023483</td>\n",
       "      <td>0.594190</td>\n",
       "      <td>0.023124</td>\n",
       "      <td>0.896533</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>42</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5475</th>\n",
       "      <td>0.583174</td>\n",
       "      <td>0.015957</td>\n",
       "      <td>0.745699</td>\n",
       "      <td>0.032401</td>\n",
       "      <td>0.498029</td>\n",
       "      <td>0.023595</td>\n",
       "      <td>0.594169</td>\n",
       "      <td>0.023546</td>\n",
       "      <td>0.896543</td>\n",
       "      <td>0.009233</td>\n",
       "      <td>45</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7057</th>\n",
       "      <td>0.576610</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.735710</td>\n",
       "      <td>0.028174</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>0.028218</td>\n",
       "      <td>0.591087</td>\n",
       "      <td>0.025276</td>\n",
       "      <td>0.896893</td>\n",
       "      <td>0.008531</td>\n",
       "      <td>51</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6337</th>\n",
       "      <td>0.576610</td>\n",
       "      <td>0.016176</td>\n",
       "      <td>0.735639</td>\n",
       "      <td>0.028450</td>\n",
       "      <td>0.497978</td>\n",
       "      <td>0.028644</td>\n",
       "      <td>0.591044</td>\n",
       "      <td>0.025741</td>\n",
       "      <td>0.896926</td>\n",
       "      <td>0.008508</td>\n",
       "      <td>48</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>0.578514</td>\n",
       "      <td>0.020342</td>\n",
       "      <td>0.742964</td>\n",
       "      <td>0.032097</td>\n",
       "      <td>0.497945</td>\n",
       "      <td>0.025444</td>\n",
       "      <td>0.593347</td>\n",
       "      <td>0.024798</td>\n",
       "      <td>0.896396</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>36</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6267</th>\n",
       "      <td>0.579295</td>\n",
       "      <td>0.021366</td>\n",
       "      <td>0.745106</td>\n",
       "      <td>0.031190</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.020918</td>\n",
       "      <td>0.593873</td>\n",
       "      <td>0.020880</td>\n",
       "      <td>0.897309</td>\n",
       "      <td>0.008552</td>\n",
       "      <td>48</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>0.579689</td>\n",
       "      <td>0.021023</td>\n",
       "      <td>0.745337</td>\n",
       "      <td>0.030894</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.020918</td>\n",
       "      <td>0.593964</td>\n",
       "      <td>0.020768</td>\n",
       "      <td>0.897165</td>\n",
       "      <td>0.008431</td>\n",
       "      <td>45</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>0.578514</td>\n",
       "      <td>0.019221</td>\n",
       "      <td>0.742941</td>\n",
       "      <td>0.030992</td>\n",
       "      <td>0.497848</td>\n",
       "      <td>0.025810</td>\n",
       "      <td>0.593342</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>0.895987</td>\n",
       "      <td>0.009350</td>\n",
       "      <td>33</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6121</th>\n",
       "      <td>0.580091</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.745761</td>\n",
       "      <td>0.031487</td>\n",
       "      <td>0.497726</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.594121</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.899002</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>48</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>0.580091</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.745761</td>\n",
       "      <td>0.031487</td>\n",
       "      <td>0.497726</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.594121</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.898749</td>\n",
       "      <td>0.008585</td>\n",
       "      <td>45</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>0.580091</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.745761</td>\n",
       "      <td>0.031487</td>\n",
       "      <td>0.497726</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.594121</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.898936</td>\n",
       "      <td>0.008423</td>\n",
       "      <td>54</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841</th>\n",
       "      <td>0.580091</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.745761</td>\n",
       "      <td>0.031487</td>\n",
       "      <td>0.497726</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.594121</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.898971</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>51</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5617</th>\n",
       "      <td>0.576225</td>\n",
       "      <td>0.016027</td>\n",
       "      <td>0.735267</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.497638</td>\n",
       "      <td>0.028534</td>\n",
       "      <td>0.590655</td>\n",
       "      <td>0.025564</td>\n",
       "      <td>0.896856</td>\n",
       "      <td>0.008465</td>\n",
       "      <td>45</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4035</th>\n",
       "      <td>0.583942</td>\n",
       "      <td>0.015290</td>\n",
       "      <td>0.746410</td>\n",
       "      <td>0.032566</td>\n",
       "      <td>0.497505</td>\n",
       "      <td>0.024020</td>\n",
       "      <td>0.594044</td>\n",
       "      <td>0.023824</td>\n",
       "      <td>0.896198</td>\n",
       "      <td>0.009699</td>\n",
       "      <td>39</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3889</th>\n",
       "      <td>0.580056</td>\n",
       "      <td>0.014414</td>\n",
       "      <td>0.743223</td>\n",
       "      <td>0.033010</td>\n",
       "      <td>0.497481</td>\n",
       "      <td>0.022406</td>\n",
       "      <td>0.592277</td>\n",
       "      <td>0.022969</td>\n",
       "      <td>0.896828</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>39</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>0.579698</td>\n",
       "      <td>0.020947</td>\n",
       "      <td>0.738365</td>\n",
       "      <td>0.031230</td>\n",
       "      <td>0.497462</td>\n",
       "      <td>0.028172</td>\n",
       "      <td>0.591697</td>\n",
       "      <td>0.026017</td>\n",
       "      <td>0.895197</td>\n",
       "      <td>0.009508</td>\n",
       "      <td>36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>0.580476</td>\n",
       "      <td>0.018138</td>\n",
       "      <td>0.746015</td>\n",
       "      <td>0.032024</td>\n",
       "      <td>0.497263</td>\n",
       "      <td>0.022219</td>\n",
       "      <td>0.593895</td>\n",
       "      <td>0.022633</td>\n",
       "      <td>0.898703</td>\n",
       "      <td>0.008640</td>\n",
       "      <td>42</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6410</th>\n",
       "      <td>0.580833</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.739633</td>\n",
       "      <td>0.028545</td>\n",
       "      <td>0.497248</td>\n",
       "      <td>0.023843</td>\n",
       "      <td>0.591724</td>\n",
       "      <td>0.022243</td>\n",
       "      <td>0.896171</td>\n",
       "      <td>0.009220</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7850</th>\n",
       "      <td>0.580449</td>\n",
       "      <td>0.016436</td>\n",
       "      <td>0.739360</td>\n",
       "      <td>0.028574</td>\n",
       "      <td>0.497248</td>\n",
       "      <td>0.023843</td>\n",
       "      <td>0.591628</td>\n",
       "      <td>0.022206</td>\n",
       "      <td>0.896307</td>\n",
       "      <td>0.009209</td>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7130</th>\n",
       "      <td>0.580449</td>\n",
       "      <td>0.016436</td>\n",
       "      <td>0.739360</td>\n",
       "      <td>0.028574</td>\n",
       "      <td>0.497248</td>\n",
       "      <td>0.023843</td>\n",
       "      <td>0.591628</td>\n",
       "      <td>0.022206</td>\n",
       "      <td>0.896192</td>\n",
       "      <td>0.009165</td>\n",
       "      <td>51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4827</th>\n",
       "      <td>0.579304</td>\n",
       "      <td>0.020246</td>\n",
       "      <td>0.745041</td>\n",
       "      <td>0.030706</td>\n",
       "      <td>0.497226</td>\n",
       "      <td>0.020640</td>\n",
       "      <td>0.593355</td>\n",
       "      <td>0.020390</td>\n",
       "      <td>0.896945</td>\n",
       "      <td>0.008623</td>\n",
       "      <td>42</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7779</th>\n",
       "      <td>0.577363</td>\n",
       "      <td>0.019137</td>\n",
       "      <td>0.734633</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.497173</td>\n",
       "      <td>0.027463</td>\n",
       "      <td>0.589730</td>\n",
       "      <td>0.026245</td>\n",
       "      <td>0.896942</td>\n",
       "      <td>0.009180</td>\n",
       "      <td>54</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "7706            0.578118           0.020128             0.742634   \n",
       "6409            0.580073           0.020694             0.737363   \n",
       "7129            0.580073           0.020694             0.737363   \n",
       "7705            0.583954           0.017748             0.740245   \n",
       "6986            0.578118           0.020128             0.742045   \n",
       "7849            0.580458           0.020881             0.737615   \n",
       "6985            0.583570           0.017937             0.740072   \n",
       "6266            0.578503           0.020788             0.742241   \n",
       "4826            0.578503           0.020717             0.742639   \n",
       "6265            0.583570           0.019212             0.740310   \n",
       "4969            0.580082           0.020644             0.736954   \n",
       "5546            0.578118           0.020779             0.742062   \n",
       "7707            0.579679           0.021492             0.745389   \n",
       "6987            0.579679           0.021492             0.745389   \n",
       "6049            0.579280           0.015199             0.742388   \n",
       "5329            0.579280           0.015199             0.742388   \n",
       "7489            0.579663           0.015224             0.742582   \n",
       "6769            0.579663           0.015224             0.743023   \n",
       "5545            0.583185           0.019980             0.740191   \n",
       "5689            0.580082           0.020644             0.736820   \n",
       "7635            0.582790           0.015301             0.745467   \n",
       "6195            0.582396           0.015550             0.745255   \n",
       "6915            0.582396           0.015550             0.745255   \n",
       "4249            0.580082           0.020644             0.737437   \n",
       "7777            0.576995           0.015286             0.735814   \n",
       "4106            0.578129           0.020186             0.742621   \n",
       "4609            0.579673           0.014804             0.742460   \n",
       "4825            0.583570           0.020188             0.740481   \n",
       "4755            0.583559           0.015277             0.745916   \n",
       "5475            0.583174           0.015957             0.745699   \n",
       "7057            0.576610           0.015713             0.735710   \n",
       "6337            0.576610           0.016176             0.735639   \n",
       "3386            0.578514           0.020342             0.742964   \n",
       "6267            0.579295           0.021366             0.745106   \n",
       "5547            0.579689           0.021023             0.745337   \n",
       "2666            0.578514           0.019221             0.742941   \n",
       "6121            0.580091           0.017589             0.745761   \n",
       "5401            0.580091           0.017589             0.745761   \n",
       "7561            0.580091           0.017589             0.745761   \n",
       "6841            0.580091           0.017589             0.745761   \n",
       "5617            0.576225           0.016027             0.735267   \n",
       "4035            0.583942           0.015290             0.746410   \n",
       "3889            0.580056           0.014414             0.743223   \n",
       "3529            0.579698           0.020947             0.738365   \n",
       "4681            0.580476           0.018138             0.746015   \n",
       "6410            0.580833           0.016574             0.739633   \n",
       "7850            0.580449           0.016436             0.739360   \n",
       "7130            0.580449           0.016436             0.739360   \n",
       "4827            0.579304           0.020246             0.745041   \n",
       "7779            0.577363           0.019137             0.734633   \n",
       "\n",
       "      std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "7706            0.031288          0.499921         0.025514      0.594710   \n",
       "6409            0.030502          0.499691         0.028018      0.593014   \n",
       "7129            0.030502          0.499691         0.028018      0.593014   \n",
       "7705            0.026537          0.499615         0.029935      0.593731   \n",
       "6986            0.031220          0.499458         0.024752      0.594232   \n",
       "7849            0.030847          0.499365         0.028382      0.592871   \n",
       "6985            0.026708          0.499252         0.030113      0.593422   \n",
       "6266            0.031513          0.499131         0.025023      0.594069   \n",
       "4826            0.031789          0.499038         0.024928      0.594055   \n",
       "6265            0.027073          0.498905         0.030628      0.593277   \n",
       "4969            0.030044          0.498866         0.027348      0.592327   \n",
       "5546            0.031691          0.498769         0.025148      0.593750   \n",
       "7707            0.030959          0.498725         0.021689      0.594559   \n",
       "6987            0.030959          0.498725         0.021689      0.594559   \n",
       "6049            0.032779          0.498694         0.023052      0.592978   \n",
       "5329            0.032779          0.498694         0.023052      0.592978   \n",
       "7489            0.033479          0.498694         0.023052      0.593032   \n",
       "6769            0.033417          0.498694         0.023052      0.593145   \n",
       "5545            0.027224          0.498558         0.031169      0.592996   \n",
       "5689            0.030019          0.498539         0.027711      0.592047   \n",
       "7635            0.031977          0.498492         0.024527      0.594424   \n",
       "6195            0.032230          0.498492         0.024527      0.594335   \n",
       "6915            0.032230          0.498492         0.024527      0.594335   \n",
       "4249            0.030480          0.498465         0.027755      0.592191   \n",
       "7777            0.028122          0.498347         0.027790      0.591363   \n",
       "4106            0.031387          0.498343         0.024976      0.593554   \n",
       "4609            0.032721          0.498215         0.022723      0.592689   \n",
       "4825            0.027571          0.498095         0.030339      0.592774   \n",
       "4755            0.031866          0.498042         0.023483      0.594190   \n",
       "5475            0.032401          0.498029         0.023595      0.594169   \n",
       "7057            0.028174          0.498000         0.028218      0.591087   \n",
       "6337            0.028450          0.497978         0.028644      0.591044   \n",
       "3386            0.032097          0.497945         0.025444      0.593347   \n",
       "6267            0.031190          0.497900         0.020918      0.593873   \n",
       "5547            0.030894          0.497900         0.020918      0.593964   \n",
       "2666            0.030992          0.497848         0.025810      0.593342   \n",
       "6121            0.031487          0.497726         0.023264      0.594121   \n",
       "5401            0.031487          0.497726         0.023264      0.594121   \n",
       "7561            0.031487          0.497726         0.023264      0.594121   \n",
       "6841            0.031487          0.497726         0.023264      0.594121   \n",
       "5617            0.028333          0.497638         0.028534      0.590655   \n",
       "4035            0.032566          0.497505         0.024020      0.594044   \n",
       "3889            0.033010          0.497481         0.022406      0.592277   \n",
       "3529            0.031230          0.497462         0.028172      0.591697   \n",
       "4681            0.032024          0.497263         0.022219      0.593895   \n",
       "6410            0.028545          0.497248         0.023843      0.591724   \n",
       "7850            0.028574          0.497248         0.023843      0.591628   \n",
       "7130            0.028574          0.497248         0.023843      0.591628   \n",
       "4827            0.030706          0.497226         0.020640      0.593355   \n",
       "7779            0.031400          0.497173         0.027463      0.589730   \n",
       "\n",
       "      std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "7706     0.024425       0.897382      0.009126                   54   \n",
       "6409     0.026019       0.896247      0.009614                   48   \n",
       "7129     0.026019       0.896345      0.009731                   51   \n",
       "7705     0.026577       0.897643      0.009532                   54   \n",
       "6986     0.024085       0.897239      0.009230                   51   \n",
       "7849     0.026267       0.896482      0.009607                   54   \n",
       "6985     0.026779       0.897749      0.009440                   51   \n",
       "6266     0.024418       0.897315      0.009161                   48   \n",
       "4826     0.024452       0.897027      0.009221                   42   \n",
       "6265     0.027353       0.897610      0.009513                   48   \n",
       "4969     0.025649       0.896073      0.009600                   42   \n",
       "5546     0.024628       0.897270      0.009171                   45   \n",
       "7707     0.021123       0.897241      0.008566                   54   \n",
       "6987     0.021123       0.897247      0.008641                   51   \n",
       "6049     0.023348       0.897275      0.009504                   48   \n",
       "5329     0.023348       0.897286      0.009228                   45   \n",
       "7489     0.023449       0.897207      0.009296                   54   \n",
       "6769     0.023368       0.897288      0.009273                   51   \n",
       "5545     0.027809       0.897463      0.009312                   45   \n",
       "5689     0.025873       0.896201      0.009368                   45   \n",
       "7635     0.023968       0.896836      0.009276                   54   \n",
       "6195     0.024070       0.896790      0.009231                   48   \n",
       "6915     0.024070       0.896775      0.009367                   51   \n",
       "4249     0.025948       0.895694      0.009568                   39   \n",
       "7777     0.024967       0.896923      0.008550                   54   \n",
       "4106     0.024239       0.896639      0.009200                   39   \n",
       "4609     0.023253       0.897240      0.009313                   42   \n",
       "4825     0.027249       0.897264      0.009435                   42   \n",
       "4755     0.023124       0.896533      0.009636                   42   \n",
       "5475     0.023546       0.896543      0.009233                   45   \n",
       "7057     0.025276       0.896893      0.008531                   51   \n",
       "6337     0.025741       0.896926      0.008508                   48   \n",
       "3386     0.024798       0.896396      0.009268                   36   \n",
       "6267     0.020880       0.897309      0.008552                   48   \n",
       "5547     0.020768       0.897165      0.008431                   45   \n",
       "2666     0.024632       0.895987      0.009350                   33   \n",
       "6121     0.023039       0.899002      0.008455                   48   \n",
       "5401     0.023039       0.898749      0.008585                   45   \n",
       "7561     0.023039       0.898936      0.008423                   54   \n",
       "6841     0.023039       0.898971      0.008419                   51   \n",
       "5617     0.025564       0.896856      0.008465                   45   \n",
       "4035     0.023824       0.896198      0.009699                   39   \n",
       "3889     0.022969       0.896828      0.009268                   39   \n",
       "3529     0.026017       0.895197      0.009508                   36   \n",
       "4681     0.022633       0.898703      0.008640                   42   \n",
       "6410     0.022243       0.896171      0.009220                   48   \n",
       "7850     0.022206       0.896307      0.009209                   54   \n",
       "7130     0.022206       0.896192      0.009165                   51   \n",
       "4827     0.020390       0.896945      0.008623                   42   \n",
       "7779     0.026245       0.896942      0.009180                   54   \n",
       "\n",
       "      param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "7706                     0.8                           1   \n",
       "6409                     1.0                           1   \n",
       "7129                     1.0                           1   \n",
       "7705                     0.8                           1   \n",
       "6986                     0.8                           1   \n",
       "7849                     1.0                           1   \n",
       "6985                     0.8                           1   \n",
       "6266                     0.8                           1   \n",
       "4826                     0.8                           1   \n",
       "6265                     0.8                           1   \n",
       "4969                     1.0                           1   \n",
       "5546                     0.8                           1   \n",
       "7707                     0.8                           1   \n",
       "6987                     0.8                           1   \n",
       "6049                     0.5                           1   \n",
       "5329                     0.5                           1   \n",
       "7489                     0.5                           1   \n",
       "6769                     0.5                           1   \n",
       "5545                     0.8                           1   \n",
       "5689                     1.0                           1   \n",
       "7635                     0.7                           1   \n",
       "6195                     0.7                           1   \n",
       "6915                     0.7                           1   \n",
       "4249                     1.0                           1   \n",
       "7777                     0.9                           1   \n",
       "4106                     0.8                           1   \n",
       "4609                     0.5                           1   \n",
       "4825                     0.8                           1   \n",
       "4755                     0.7                           1   \n",
       "5475                     0.7                           1   \n",
       "7057                     0.9                           1   \n",
       "6337                     0.9                           1   \n",
       "3386                     0.8                           1   \n",
       "6267                     0.8                           1   \n",
       "5547                     0.8                           1   \n",
       "2666                     0.8                           1   \n",
       "6121                     0.6                           1   \n",
       "5401                     0.6                           1   \n",
       "7561                     0.6                           1   \n",
       "6841                     0.6                           1   \n",
       "5617                     0.9                           1   \n",
       "4035                     0.7                           1   \n",
       "3889                     0.5                           1   \n",
       "3529                     1.0                           1   \n",
       "4681                     0.6                           1   \n",
       "6410                     1.0                           1   \n",
       "7850                     1.0                           1   \n",
       "7130                     1.0                           1   \n",
       "4827                     0.8                           1   \n",
       "7779                     0.9                           1   \n",
       "\n",
       "      param_rf__min_samples_split  param_rf__n_estimators  \n",
       "7706                            6                     200  \n",
       "6409                            4                     200  \n",
       "7129                            4                     200  \n",
       "7705                            4                     200  \n",
       "6986                            6                     200  \n",
       "7849                            4                     200  \n",
       "6985                            4                     200  \n",
       "6266                            6                     200  \n",
       "4826                            6                     200  \n",
       "6265                            4                     200  \n",
       "4969                            4                     200  \n",
       "5546                            6                     200  \n",
       "7707                            8                     200  \n",
       "6987                            8                     200  \n",
       "6049                            4                     200  \n",
       "5329                            4                     200  \n",
       "7489                            4                     200  \n",
       "6769                            4                     200  \n",
       "5545                            4                     200  \n",
       "5689                            4                     200  \n",
       "7635                            8                     200  \n",
       "6195                            8                     200  \n",
       "6915                            8                     200  \n",
       "4249                            4                     200  \n",
       "7777                            4                     200  \n",
       "4106                            6                     200  \n",
       "4609                            4                     200  \n",
       "4825                            4                     200  \n",
       "4755                            8                     200  \n",
       "5475                            8                     200  \n",
       "7057                            4                     200  \n",
       "6337                            4                     200  \n",
       "3386                            6                     200  \n",
       "6267                            8                     200  \n",
       "5547                            8                     200  \n",
       "2666                            6                     200  \n",
       "6121                            4                     200  \n",
       "5401                            4                     200  \n",
       "7561                            4                     200  \n",
       "6841                            4                     200  \n",
       "5617                            4                     200  \n",
       "4035                            8                     200  \n",
       "3889                            4                     200  \n",
       "3529                            4                     200  \n",
       "4681                            4                     200  \n",
       "6410                            6                     200  \n",
       "7850                            6                     200  \n",
       "7130                            6                     200  \n",
       "4827                            8                     200  \n",
       "7779                            8                     200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 5 param settings mean_test_f1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7706</th>\n",
       "      <td>0.578118</td>\n",
       "      <td>0.020128</td>\n",
       "      <td>0.742634</td>\n",
       "      <td>0.031288</td>\n",
       "      <td>0.499921</td>\n",
       "      <td>0.025514</td>\n",
       "      <td>0.594710</td>\n",
       "      <td>0.024425</td>\n",
       "      <td>0.897382</td>\n",
       "      <td>0.009126</td>\n",
       "      <td>54</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7707</th>\n",
       "      <td>0.579679</td>\n",
       "      <td>0.021492</td>\n",
       "      <td>0.745389</td>\n",
       "      <td>0.030959</td>\n",
       "      <td>0.498725</td>\n",
       "      <td>0.021689</td>\n",
       "      <td>0.594559</td>\n",
       "      <td>0.021123</td>\n",
       "      <td>0.897241</td>\n",
       "      <td>0.008566</td>\n",
       "      <td>54</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6987</th>\n",
       "      <td>0.579679</td>\n",
       "      <td>0.021492</td>\n",
       "      <td>0.745389</td>\n",
       "      <td>0.030959</td>\n",
       "      <td>0.498725</td>\n",
       "      <td>0.021689</td>\n",
       "      <td>0.594559</td>\n",
       "      <td>0.021123</td>\n",
       "      <td>0.897247</td>\n",
       "      <td>0.008641</td>\n",
       "      <td>51</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7635</th>\n",
       "      <td>0.582790</td>\n",
       "      <td>0.015301</td>\n",
       "      <td>0.745467</td>\n",
       "      <td>0.031977</td>\n",
       "      <td>0.498492</td>\n",
       "      <td>0.024527</td>\n",
       "      <td>0.594424</td>\n",
       "      <td>0.023968</td>\n",
       "      <td>0.896836</td>\n",
       "      <td>0.009276</td>\n",
       "      <td>54</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6195</th>\n",
       "      <td>0.582396</td>\n",
       "      <td>0.015550</td>\n",
       "      <td>0.745255</td>\n",
       "      <td>0.032230</td>\n",
       "      <td>0.498492</td>\n",
       "      <td>0.024527</td>\n",
       "      <td>0.594335</td>\n",
       "      <td>0.024070</td>\n",
       "      <td>0.896790</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>48</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>0.582396</td>\n",
       "      <td>0.015550</td>\n",
       "      <td>0.745255</td>\n",
       "      <td>0.032230</td>\n",
       "      <td>0.498492</td>\n",
       "      <td>0.024527</td>\n",
       "      <td>0.594335</td>\n",
       "      <td>0.024070</td>\n",
       "      <td>0.896775</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>51</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6986</th>\n",
       "      <td>0.578118</td>\n",
       "      <td>0.020128</td>\n",
       "      <td>0.742045</td>\n",
       "      <td>0.031220</td>\n",
       "      <td>0.499458</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.594232</td>\n",
       "      <td>0.024085</td>\n",
       "      <td>0.897239</td>\n",
       "      <td>0.009230</td>\n",
       "      <td>51</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>0.583559</td>\n",
       "      <td>0.015277</td>\n",
       "      <td>0.745916</td>\n",
       "      <td>0.031866</td>\n",
       "      <td>0.498042</td>\n",
       "      <td>0.023483</td>\n",
       "      <td>0.594190</td>\n",
       "      <td>0.023124</td>\n",
       "      <td>0.896533</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>42</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5475</th>\n",
       "      <td>0.583174</td>\n",
       "      <td>0.015957</td>\n",
       "      <td>0.745699</td>\n",
       "      <td>0.032401</td>\n",
       "      <td>0.498029</td>\n",
       "      <td>0.023595</td>\n",
       "      <td>0.594169</td>\n",
       "      <td>0.023546</td>\n",
       "      <td>0.896543</td>\n",
       "      <td>0.009233</td>\n",
       "      <td>45</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841</th>\n",
       "      <td>0.580091</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.745761</td>\n",
       "      <td>0.031487</td>\n",
       "      <td>0.497726</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.594121</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.898971</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>51</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>0.580091</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.745761</td>\n",
       "      <td>0.031487</td>\n",
       "      <td>0.497726</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.594121</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.898936</td>\n",
       "      <td>0.008423</td>\n",
       "      <td>54</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>0.580091</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.745761</td>\n",
       "      <td>0.031487</td>\n",
       "      <td>0.497726</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.594121</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.898749</td>\n",
       "      <td>0.008585</td>\n",
       "      <td>45</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6121</th>\n",
       "      <td>0.580091</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.745761</td>\n",
       "      <td>0.031487</td>\n",
       "      <td>0.497726</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.594121</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.899002</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>48</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6266</th>\n",
       "      <td>0.578503</td>\n",
       "      <td>0.020788</td>\n",
       "      <td>0.742241</td>\n",
       "      <td>0.031513</td>\n",
       "      <td>0.499131</td>\n",
       "      <td>0.025023</td>\n",
       "      <td>0.594069</td>\n",
       "      <td>0.024418</td>\n",
       "      <td>0.897315</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>48</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4826</th>\n",
       "      <td>0.578503</td>\n",
       "      <td>0.020717</td>\n",
       "      <td>0.742639</td>\n",
       "      <td>0.031789</td>\n",
       "      <td>0.499038</td>\n",
       "      <td>0.024928</td>\n",
       "      <td>0.594055</td>\n",
       "      <td>0.024452</td>\n",
       "      <td>0.897027</td>\n",
       "      <td>0.009221</td>\n",
       "      <td>42</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4035</th>\n",
       "      <td>0.583942</td>\n",
       "      <td>0.015290</td>\n",
       "      <td>0.746410</td>\n",
       "      <td>0.032566</td>\n",
       "      <td>0.497505</td>\n",
       "      <td>0.024020</td>\n",
       "      <td>0.594044</td>\n",
       "      <td>0.023824</td>\n",
       "      <td>0.896198</td>\n",
       "      <td>0.009699</td>\n",
       "      <td>39</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>0.579689</td>\n",
       "      <td>0.021023</td>\n",
       "      <td>0.745337</td>\n",
       "      <td>0.030894</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.020918</td>\n",
       "      <td>0.593964</td>\n",
       "      <td>0.020768</td>\n",
       "      <td>0.897165</td>\n",
       "      <td>0.008431</td>\n",
       "      <td>45</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>0.580476</td>\n",
       "      <td>0.018138</td>\n",
       "      <td>0.746015</td>\n",
       "      <td>0.032024</td>\n",
       "      <td>0.497263</td>\n",
       "      <td>0.022219</td>\n",
       "      <td>0.593895</td>\n",
       "      <td>0.022633</td>\n",
       "      <td>0.898703</td>\n",
       "      <td>0.008640</td>\n",
       "      <td>42</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6267</th>\n",
       "      <td>0.579295</td>\n",
       "      <td>0.021366</td>\n",
       "      <td>0.745106</td>\n",
       "      <td>0.031190</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.020918</td>\n",
       "      <td>0.593873</td>\n",
       "      <td>0.020880</td>\n",
       "      <td>0.897309</td>\n",
       "      <td>0.008552</td>\n",
       "      <td>48</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>0.578118</td>\n",
       "      <td>0.020779</td>\n",
       "      <td>0.742062</td>\n",
       "      <td>0.031691</td>\n",
       "      <td>0.498769</td>\n",
       "      <td>0.025148</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.024628</td>\n",
       "      <td>0.897270</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>45</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7705</th>\n",
       "      <td>0.583954</td>\n",
       "      <td>0.017748</td>\n",
       "      <td>0.740245</td>\n",
       "      <td>0.026537</td>\n",
       "      <td>0.499615</td>\n",
       "      <td>0.029935</td>\n",
       "      <td>0.593731</td>\n",
       "      <td>0.026577</td>\n",
       "      <td>0.897643</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>54</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3961</th>\n",
       "      <td>0.580091</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.745836</td>\n",
       "      <td>0.032149</td>\n",
       "      <td>0.496893</td>\n",
       "      <td>0.022403</td>\n",
       "      <td>0.593573</td>\n",
       "      <td>0.022818</td>\n",
       "      <td>0.898390</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>39</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4106</th>\n",
       "      <td>0.578129</td>\n",
       "      <td>0.020186</td>\n",
       "      <td>0.742621</td>\n",
       "      <td>0.031387</td>\n",
       "      <td>0.498343</td>\n",
       "      <td>0.024976</td>\n",
       "      <td>0.593554</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>0.896639</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>39</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6985</th>\n",
       "      <td>0.583570</td>\n",
       "      <td>0.017937</td>\n",
       "      <td>0.740072</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.030113</td>\n",
       "      <td>0.593422</td>\n",
       "      <td>0.026779</td>\n",
       "      <td>0.897749</td>\n",
       "      <td>0.009440</td>\n",
       "      <td>51</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4827</th>\n",
       "      <td>0.579304</td>\n",
       "      <td>0.020246</td>\n",
       "      <td>0.745041</td>\n",
       "      <td>0.030706</td>\n",
       "      <td>0.497226</td>\n",
       "      <td>0.020640</td>\n",
       "      <td>0.593355</td>\n",
       "      <td>0.020390</td>\n",
       "      <td>0.896945</td>\n",
       "      <td>0.008623</td>\n",
       "      <td>42</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>0.578514</td>\n",
       "      <td>0.020342</td>\n",
       "      <td>0.742964</td>\n",
       "      <td>0.032097</td>\n",
       "      <td>0.497945</td>\n",
       "      <td>0.025444</td>\n",
       "      <td>0.593347</td>\n",
       "      <td>0.024798</td>\n",
       "      <td>0.896396</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>36</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>0.578514</td>\n",
       "      <td>0.019221</td>\n",
       "      <td>0.742941</td>\n",
       "      <td>0.030992</td>\n",
       "      <td>0.497848</td>\n",
       "      <td>0.025810</td>\n",
       "      <td>0.593342</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>0.895987</td>\n",
       "      <td>0.009350</td>\n",
       "      <td>33</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>0.583570</td>\n",
       "      <td>0.019212</td>\n",
       "      <td>0.740310</td>\n",
       "      <td>0.027073</td>\n",
       "      <td>0.498905</td>\n",
       "      <td>0.030628</td>\n",
       "      <td>0.593277</td>\n",
       "      <td>0.027353</td>\n",
       "      <td>0.897610</td>\n",
       "      <td>0.009513</td>\n",
       "      <td>48</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>0.582395</td>\n",
       "      <td>0.016421</td>\n",
       "      <td>0.746233</td>\n",
       "      <td>0.033028</td>\n",
       "      <td>0.496541</td>\n",
       "      <td>0.024673</td>\n",
       "      <td>0.593270</td>\n",
       "      <td>0.024398</td>\n",
       "      <td>0.895905</td>\n",
       "      <td>0.009844</td>\n",
       "      <td>36</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7490</th>\n",
       "      <td>0.584324</td>\n",
       "      <td>0.017817</td>\n",
       "      <td>0.746647</td>\n",
       "      <td>0.030665</td>\n",
       "      <td>0.496622</td>\n",
       "      <td>0.018824</td>\n",
       "      <td>0.593197</td>\n",
       "      <td>0.019037</td>\n",
       "      <td>0.897621</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>54</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>0.583173</td>\n",
       "      <td>0.016087</td>\n",
       "      <td>0.746876</td>\n",
       "      <td>0.033199</td>\n",
       "      <td>0.496139</td>\n",
       "      <td>0.025247</td>\n",
       "      <td>0.593192</td>\n",
       "      <td>0.024992</td>\n",
       "      <td>0.895687</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>33</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6769</th>\n",
       "      <td>0.579663</td>\n",
       "      <td>0.015224</td>\n",
       "      <td>0.743023</td>\n",
       "      <td>0.033417</td>\n",
       "      <td>0.498694</td>\n",
       "      <td>0.023052</td>\n",
       "      <td>0.593145</td>\n",
       "      <td>0.023368</td>\n",
       "      <td>0.897288</td>\n",
       "      <td>0.009273</td>\n",
       "      <td>51</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>0.580093</td>\n",
       "      <td>0.017475</td>\n",
       "      <td>0.746128</td>\n",
       "      <td>0.031577</td>\n",
       "      <td>0.496269</td>\n",
       "      <td>0.022552</td>\n",
       "      <td>0.593129</td>\n",
       "      <td>0.022516</td>\n",
       "      <td>0.897926</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>36</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489</th>\n",
       "      <td>0.579663</td>\n",
       "      <td>0.015224</td>\n",
       "      <td>0.742582</td>\n",
       "      <td>0.033479</td>\n",
       "      <td>0.498694</td>\n",
       "      <td>0.023052</td>\n",
       "      <td>0.593032</td>\n",
       "      <td>0.023449</td>\n",
       "      <td>0.897207</td>\n",
       "      <td>0.009296</td>\n",
       "      <td>54</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7129</th>\n",
       "      <td>0.580073</td>\n",
       "      <td>0.020694</td>\n",
       "      <td>0.737363</td>\n",
       "      <td>0.030502</td>\n",
       "      <td>0.499691</td>\n",
       "      <td>0.028018</td>\n",
       "      <td>0.593014</td>\n",
       "      <td>0.026019</td>\n",
       "      <td>0.896345</td>\n",
       "      <td>0.009731</td>\n",
       "      <td>51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6409</th>\n",
       "      <td>0.580073</td>\n",
       "      <td>0.020694</td>\n",
       "      <td>0.737363</td>\n",
       "      <td>0.030502</td>\n",
       "      <td>0.499691</td>\n",
       "      <td>0.028018</td>\n",
       "      <td>0.593014</td>\n",
       "      <td>0.026019</td>\n",
       "      <td>0.896247</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>0.583939</td>\n",
       "      <td>0.018499</td>\n",
       "      <td>0.746934</td>\n",
       "      <td>0.030724</td>\n",
       "      <td>0.496282</td>\n",
       "      <td>0.018952</td>\n",
       "      <td>0.593004</td>\n",
       "      <td>0.019096</td>\n",
       "      <td>0.897580</td>\n",
       "      <td>0.009011</td>\n",
       "      <td>48</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>0.583185</td>\n",
       "      <td>0.019980</td>\n",
       "      <td>0.740191</td>\n",
       "      <td>0.027224</td>\n",
       "      <td>0.498558</td>\n",
       "      <td>0.031169</td>\n",
       "      <td>0.592996</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.897463</td>\n",
       "      <td>0.009312</td>\n",
       "      <td>45</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3387</th>\n",
       "      <td>0.579689</td>\n",
       "      <td>0.021023</td>\n",
       "      <td>0.745163</td>\n",
       "      <td>0.031196</td>\n",
       "      <td>0.496599</td>\n",
       "      <td>0.021758</td>\n",
       "      <td>0.592990</td>\n",
       "      <td>0.021294</td>\n",
       "      <td>0.896222</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>36</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5329</th>\n",
       "      <td>0.579280</td>\n",
       "      <td>0.015199</td>\n",
       "      <td>0.742388</td>\n",
       "      <td>0.032779</td>\n",
       "      <td>0.498694</td>\n",
       "      <td>0.023052</td>\n",
       "      <td>0.592978</td>\n",
       "      <td>0.023348</td>\n",
       "      <td>0.897286</td>\n",
       "      <td>0.009228</td>\n",
       "      <td>45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>0.579280</td>\n",
       "      <td>0.015199</td>\n",
       "      <td>0.742388</td>\n",
       "      <td>0.032779</td>\n",
       "      <td>0.498694</td>\n",
       "      <td>0.023052</td>\n",
       "      <td>0.592978</td>\n",
       "      <td>0.023348</td>\n",
       "      <td>0.897275</td>\n",
       "      <td>0.009504</td>\n",
       "      <td>48</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>0.579689</td>\n",
       "      <td>0.020234</td>\n",
       "      <td>0.745157</td>\n",
       "      <td>0.031113</td>\n",
       "      <td>0.496630</td>\n",
       "      <td>0.021188</td>\n",
       "      <td>0.592968</td>\n",
       "      <td>0.020813</td>\n",
       "      <td>0.896747</td>\n",
       "      <td>0.008374</td>\n",
       "      <td>39</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>0.579708</td>\n",
       "      <td>0.017666</td>\n",
       "      <td>0.745995</td>\n",
       "      <td>0.031480</td>\n",
       "      <td>0.495950</td>\n",
       "      <td>0.022413</td>\n",
       "      <td>0.592897</td>\n",
       "      <td>0.022308</td>\n",
       "      <td>0.897482</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>33</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6770</th>\n",
       "      <td>0.583939</td>\n",
       "      <td>0.018499</td>\n",
       "      <td>0.746462</td>\n",
       "      <td>0.030823</td>\n",
       "      <td>0.496282</td>\n",
       "      <td>0.018952</td>\n",
       "      <td>0.592888</td>\n",
       "      <td>0.019220</td>\n",
       "      <td>0.897606</td>\n",
       "      <td>0.009141</td>\n",
       "      <td>51</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7849</th>\n",
       "      <td>0.580458</td>\n",
       "      <td>0.020881</td>\n",
       "      <td>0.737615</td>\n",
       "      <td>0.030847</td>\n",
       "      <td>0.499365</td>\n",
       "      <td>0.028382</td>\n",
       "      <td>0.592871</td>\n",
       "      <td>0.026267</td>\n",
       "      <td>0.896482</td>\n",
       "      <td>0.009607</td>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4825</th>\n",
       "      <td>0.583570</td>\n",
       "      <td>0.020188</td>\n",
       "      <td>0.740481</td>\n",
       "      <td>0.027571</td>\n",
       "      <td>0.498095</td>\n",
       "      <td>0.030339</td>\n",
       "      <td>0.592774</td>\n",
       "      <td>0.027249</td>\n",
       "      <td>0.897264</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>42</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>0.584708</td>\n",
       "      <td>0.018985</td>\n",
       "      <td>0.747470</td>\n",
       "      <td>0.031340</td>\n",
       "      <td>0.495725</td>\n",
       "      <td>0.018110</td>\n",
       "      <td>0.592755</td>\n",
       "      <td>0.018734</td>\n",
       "      <td>0.897178</td>\n",
       "      <td>0.009404</td>\n",
       "      <td>39</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>0.579673</td>\n",
       "      <td>0.014804</td>\n",
       "      <td>0.742460</td>\n",
       "      <td>0.032721</td>\n",
       "      <td>0.498215</td>\n",
       "      <td>0.022723</td>\n",
       "      <td>0.592689</td>\n",
       "      <td>0.023253</td>\n",
       "      <td>0.897240</td>\n",
       "      <td>0.009313</td>\n",
       "      <td>42</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6348</th>\n",
       "      <td>0.580886</td>\n",
       "      <td>0.017410</td>\n",
       "      <td>0.748313</td>\n",
       "      <td>0.030923</td>\n",
       "      <td>0.495099</td>\n",
       "      <td>0.023057</td>\n",
       "      <td>0.592669</td>\n",
       "      <td>0.022015</td>\n",
       "      <td>0.896953</td>\n",
       "      <td>0.008862</td>\n",
       "      <td>48</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7788</th>\n",
       "      <td>0.580886</td>\n",
       "      <td>0.017410</td>\n",
       "      <td>0.748313</td>\n",
       "      <td>0.030923</td>\n",
       "      <td>0.495099</td>\n",
       "      <td>0.023057</td>\n",
       "      <td>0.592669</td>\n",
       "      <td>0.022015</td>\n",
       "      <td>0.896962</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>54</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "7706            0.578118           0.020128             0.742634   \n",
       "7707            0.579679           0.021492             0.745389   \n",
       "6987            0.579679           0.021492             0.745389   \n",
       "7635            0.582790           0.015301             0.745467   \n",
       "6195            0.582396           0.015550             0.745255   \n",
       "6915            0.582396           0.015550             0.745255   \n",
       "6986            0.578118           0.020128             0.742045   \n",
       "4755            0.583559           0.015277             0.745916   \n",
       "5475            0.583174           0.015957             0.745699   \n",
       "6841            0.580091           0.017589             0.745761   \n",
       "7561            0.580091           0.017589             0.745761   \n",
       "5401            0.580091           0.017589             0.745761   \n",
       "6121            0.580091           0.017589             0.745761   \n",
       "6266            0.578503           0.020788             0.742241   \n",
       "4826            0.578503           0.020717             0.742639   \n",
       "4035            0.583942           0.015290             0.746410   \n",
       "5547            0.579689           0.021023             0.745337   \n",
       "4681            0.580476           0.018138             0.746015   \n",
       "6267            0.579295           0.021366             0.745106   \n",
       "5546            0.578118           0.020779             0.742062   \n",
       "7705            0.583954           0.017748             0.740245   \n",
       "3961            0.580091           0.018730             0.745836   \n",
       "4106            0.578129           0.020186             0.742621   \n",
       "6985            0.583570           0.017937             0.740072   \n",
       "4827            0.579304           0.020246             0.745041   \n",
       "3386            0.578514           0.020342             0.742964   \n",
       "2666            0.578514           0.019221             0.742941   \n",
       "6265            0.583570           0.019212             0.740310   \n",
       "3315            0.582395           0.016421             0.746233   \n",
       "7490            0.584324           0.017817             0.746647   \n",
       "2595            0.583173           0.016087             0.746876   \n",
       "6769            0.579663           0.015224             0.743023   \n",
       "3241            0.580093           0.017475             0.746128   \n",
       "7489            0.579663           0.015224             0.742582   \n",
       "7129            0.580073           0.020694             0.737363   \n",
       "6409            0.580073           0.020694             0.737363   \n",
       "6050            0.583939           0.018499             0.746934   \n",
       "5545            0.583185           0.019980             0.740191   \n",
       "3387            0.579689           0.021023             0.745163   \n",
       "5329            0.579280           0.015199             0.742388   \n",
       "6049            0.579280           0.015199             0.742388   \n",
       "4107            0.579689           0.020234             0.745157   \n",
       "2521            0.579708           0.017666             0.745995   \n",
       "6770            0.583939           0.018499             0.746462   \n",
       "7849            0.580458           0.020881             0.737615   \n",
       "4825            0.583570           0.020188             0.740481   \n",
       "3890            0.584708           0.018985             0.747470   \n",
       "4609            0.579673           0.014804             0.742460   \n",
       "6348            0.580886           0.017410             0.748313   \n",
       "7788            0.580886           0.017410             0.748313   \n",
       "\n",
       "      std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "7706            0.031288          0.499921         0.025514      0.594710   \n",
       "7707            0.030959          0.498725         0.021689      0.594559   \n",
       "6987            0.030959          0.498725         0.021689      0.594559   \n",
       "7635            0.031977          0.498492         0.024527      0.594424   \n",
       "6195            0.032230          0.498492         0.024527      0.594335   \n",
       "6915            0.032230          0.498492         0.024527      0.594335   \n",
       "6986            0.031220          0.499458         0.024752      0.594232   \n",
       "4755            0.031866          0.498042         0.023483      0.594190   \n",
       "5475            0.032401          0.498029         0.023595      0.594169   \n",
       "6841            0.031487          0.497726         0.023264      0.594121   \n",
       "7561            0.031487          0.497726         0.023264      0.594121   \n",
       "5401            0.031487          0.497726         0.023264      0.594121   \n",
       "6121            0.031487          0.497726         0.023264      0.594121   \n",
       "6266            0.031513          0.499131         0.025023      0.594069   \n",
       "4826            0.031789          0.499038         0.024928      0.594055   \n",
       "4035            0.032566          0.497505         0.024020      0.594044   \n",
       "5547            0.030894          0.497900         0.020918      0.593964   \n",
       "4681            0.032024          0.497263         0.022219      0.593895   \n",
       "6267            0.031190          0.497900         0.020918      0.593873   \n",
       "5546            0.031691          0.498769         0.025148      0.593750   \n",
       "7705            0.026537          0.499615         0.029935      0.593731   \n",
       "3961            0.032149          0.496893         0.022403      0.593573   \n",
       "4106            0.031387          0.498343         0.024976      0.593554   \n",
       "6985            0.026708          0.499252         0.030113      0.593422   \n",
       "4827            0.030706          0.497226         0.020640      0.593355   \n",
       "3386            0.032097          0.497945         0.025444      0.593347   \n",
       "2666            0.030992          0.497848         0.025810      0.593342   \n",
       "6265            0.027073          0.498905         0.030628      0.593277   \n",
       "3315            0.033028          0.496541         0.024673      0.593270   \n",
       "7490            0.030665          0.496622         0.018824      0.593197   \n",
       "2595            0.033199          0.496139         0.025247      0.593192   \n",
       "6769            0.033417          0.498694         0.023052      0.593145   \n",
       "3241            0.031577          0.496269         0.022552      0.593129   \n",
       "7489            0.033479          0.498694         0.023052      0.593032   \n",
       "7129            0.030502          0.499691         0.028018      0.593014   \n",
       "6409            0.030502          0.499691         0.028018      0.593014   \n",
       "6050            0.030724          0.496282         0.018952      0.593004   \n",
       "5545            0.027224          0.498558         0.031169      0.592996   \n",
       "3387            0.031196          0.496599         0.021758      0.592990   \n",
       "5329            0.032779          0.498694         0.023052      0.592978   \n",
       "6049            0.032779          0.498694         0.023052      0.592978   \n",
       "4107            0.031113          0.496630         0.021188      0.592968   \n",
       "2521            0.031480          0.495950         0.022413      0.592897   \n",
       "6770            0.030823          0.496282         0.018952      0.592888   \n",
       "7849            0.030847          0.499365         0.028382      0.592871   \n",
       "4825            0.027571          0.498095         0.030339      0.592774   \n",
       "3890            0.031340          0.495725         0.018110      0.592755   \n",
       "4609            0.032721          0.498215         0.022723      0.592689   \n",
       "6348            0.030923          0.495099         0.023057      0.592669   \n",
       "7788            0.030923          0.495099         0.023057      0.592669   \n",
       "\n",
       "      std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "7706     0.024425       0.897382      0.009126                   54   \n",
       "7707     0.021123       0.897241      0.008566                   54   \n",
       "6987     0.021123       0.897247      0.008641                   51   \n",
       "7635     0.023968       0.896836      0.009276                   54   \n",
       "6195     0.024070       0.896790      0.009231                   48   \n",
       "6915     0.024070       0.896775      0.009367                   51   \n",
       "6986     0.024085       0.897239      0.009230                   51   \n",
       "4755     0.023124       0.896533      0.009636                   42   \n",
       "5475     0.023546       0.896543      0.009233                   45   \n",
       "6841     0.023039       0.898971      0.008419                   51   \n",
       "7561     0.023039       0.898936      0.008423                   54   \n",
       "5401     0.023039       0.898749      0.008585                   45   \n",
       "6121     0.023039       0.899002      0.008455                   48   \n",
       "6266     0.024418       0.897315      0.009161                   48   \n",
       "4826     0.024452       0.897027      0.009221                   42   \n",
       "4035     0.023824       0.896198      0.009699                   39   \n",
       "5547     0.020768       0.897165      0.008431                   45   \n",
       "4681     0.022633       0.898703      0.008640                   42   \n",
       "6267     0.020880       0.897309      0.008552                   48   \n",
       "5546     0.024628       0.897270      0.009171                   45   \n",
       "7705     0.026577       0.897643      0.009532                   54   \n",
       "3961     0.022818       0.898390      0.008869                   39   \n",
       "4106     0.024239       0.896639      0.009200                   39   \n",
       "6985     0.026779       0.897749      0.009440                   51   \n",
       "4827     0.020390       0.896945      0.008623                   42   \n",
       "3386     0.024798       0.896396      0.009268                   36   \n",
       "2666     0.024632       0.895987      0.009350                   33   \n",
       "6265     0.027353       0.897610      0.009513                   48   \n",
       "3315     0.024398       0.895905      0.009844                   36   \n",
       "7490     0.019037       0.897621      0.008977                   54   \n",
       "2595     0.024992       0.895687      0.009830                   33   \n",
       "6769     0.023368       0.897288      0.009273                   51   \n",
       "3241     0.022516       0.897926      0.008653                   36   \n",
       "7489     0.023449       0.897207      0.009296                   54   \n",
       "7129     0.026019       0.896345      0.009731                   51   \n",
       "6409     0.026019       0.896247      0.009614                   48   \n",
       "6050     0.019096       0.897580      0.009011                   48   \n",
       "5545     0.027809       0.897463      0.009312                   45   \n",
       "3387     0.021294       0.896222      0.008621                   36   \n",
       "5329     0.023348       0.897286      0.009228                   45   \n",
       "6049     0.023348       0.897275      0.009504                   48   \n",
       "4107     0.020813       0.896747      0.008374                   39   \n",
       "2521     0.022308       0.897482      0.008753                   33   \n",
       "6770     0.019220       0.897606      0.009141                   51   \n",
       "7849     0.026267       0.896482      0.009607                   54   \n",
       "4825     0.027249       0.897264      0.009435                   42   \n",
       "3890     0.018734       0.897178      0.009404                   39   \n",
       "4609     0.023253       0.897240      0.009313                   42   \n",
       "6348     0.022015       0.896953      0.008862                   48   \n",
       "7788     0.022015       0.896962      0.008847                   54   \n",
       "\n",
       "      param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "7706                     0.8                           1   \n",
       "7707                     0.8                           1   \n",
       "6987                     0.8                           1   \n",
       "7635                     0.7                           1   \n",
       "6195                     0.7                           1   \n",
       "6915                     0.7                           1   \n",
       "6986                     0.8                           1   \n",
       "4755                     0.7                           1   \n",
       "5475                     0.7                           1   \n",
       "6841                     0.6                           1   \n",
       "7561                     0.6                           1   \n",
       "5401                     0.6                           1   \n",
       "6121                     0.6                           1   \n",
       "6266                     0.8                           1   \n",
       "4826                     0.8                           1   \n",
       "4035                     0.7                           1   \n",
       "5547                     0.8                           1   \n",
       "4681                     0.6                           1   \n",
       "6267                     0.8                           1   \n",
       "5546                     0.8                           1   \n",
       "7705                     0.8                           1   \n",
       "3961                     0.6                           1   \n",
       "4106                     0.8                           1   \n",
       "6985                     0.8                           1   \n",
       "4827                     0.8                           1   \n",
       "3386                     0.8                           1   \n",
       "2666                     0.8                           1   \n",
       "6265                     0.8                           1   \n",
       "3315                     0.7                           1   \n",
       "7490                     0.5                           1   \n",
       "2595                     0.7                           1   \n",
       "6769                     0.5                           1   \n",
       "3241                     0.6                           1   \n",
       "7489                     0.5                           1   \n",
       "7129                     1.0                           1   \n",
       "6409                     1.0                           1   \n",
       "6050                     0.5                           1   \n",
       "5545                     0.8                           1   \n",
       "3387                     0.8                           1   \n",
       "5329                     0.5                           1   \n",
       "6049                     0.5                           1   \n",
       "4107                     0.8                           1   \n",
       "2521                     0.6                           1   \n",
       "6770                     0.5                           1   \n",
       "7849                     1.0                           1   \n",
       "4825                     0.8                           1   \n",
       "3890                     0.5                           1   \n",
       "4609                     0.5                           1   \n",
       "6348                     0.9                           3   \n",
       "7788                     0.9                           3   \n",
       "\n",
       "      param_rf__min_samples_split  param_rf__n_estimators  \n",
       "7706                            6                     200  \n",
       "7707                            8                     200  \n",
       "6987                            8                     200  \n",
       "7635                            8                     200  \n",
       "6195                            8                     200  \n",
       "6915                            8                     200  \n",
       "6986                            6                     200  \n",
       "4755                            8                     200  \n",
       "5475                            8                     200  \n",
       "6841                            4                     200  \n",
       "7561                            4                     200  \n",
       "5401                            4                     200  \n",
       "6121                            4                     200  \n",
       "6266                            6                     200  \n",
       "4826                            6                     200  \n",
       "4035                            8                     200  \n",
       "5547                            8                     200  \n",
       "4681                            4                     200  \n",
       "6267                            8                     200  \n",
       "5546                            6                     200  \n",
       "7705                            4                     200  \n",
       "3961                            4                     200  \n",
       "4106                            6                     200  \n",
       "6985                            4                     200  \n",
       "4827                            8                     200  \n",
       "3386                            6                     200  \n",
       "2666                            6                     200  \n",
       "6265                            4                     200  \n",
       "3315                            8                     200  \n",
       "7490                            6                     200  \n",
       "2595                            8                     200  \n",
       "6769                            4                     200  \n",
       "3241                            4                     200  \n",
       "7489                            4                     200  \n",
       "7129                            4                     200  \n",
       "6409                            4                     200  \n",
       "6050                            6                     200  \n",
       "5545                            4                     200  \n",
       "3387                            8                     200  \n",
       "5329                            4                     200  \n",
       "6049                            4                     200  \n",
       "4107                            8                     200  \n",
       "2521                            4                     200  \n",
       "6770                            6                     200  \n",
       "7849                            4                     200  \n",
       "4825                            4                     200  \n",
       "3890                            6                     200  \n",
       "4609                            4                     200  \n",
       "6348                           10                     200  \n",
       "7788                           10                     200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 5 param settings mean_test_auc:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3682</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899871</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3680</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899871</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899871</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4402</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899852</td>\n",
       "      <td>0.009455</td>\n",
       "      <td>42</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4401</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899852</td>\n",
       "      <td>0.009455</td>\n",
       "      <td>42</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899852</td>\n",
       "      <td>0.009455</td>\n",
       "      <td>42</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6560</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899837</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6561</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899837</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6562</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899837</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7281</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899835</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>54</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7282</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899835</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>54</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7280</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899835</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>54</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5121</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899833</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>45</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5122</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899833</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>45</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5120</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899833</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>45</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5840</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899825</td>\n",
       "      <td>0.009432</td>\n",
       "      <td>48</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899825</td>\n",
       "      <td>0.009432</td>\n",
       "      <td>48</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5842</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899825</td>\n",
       "      <td>0.009432</td>\n",
       "      <td>48</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960</th>\n",
       "      <td>0.576236</td>\n",
       "      <td>0.019411</td>\n",
       "      <td>0.758024</td>\n",
       "      <td>0.028395</td>\n",
       "      <td>0.479277</td>\n",
       "      <td>0.028117</td>\n",
       "      <td>0.583518</td>\n",
       "      <td>0.024675</td>\n",
       "      <td>0.899760</td>\n",
       "      <td>0.009332</td>\n",
       "      <td>36</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>0.576236</td>\n",
       "      <td>0.019411</td>\n",
       "      <td>0.758024</td>\n",
       "      <td>0.028395</td>\n",
       "      <td>0.479277</td>\n",
       "      <td>0.028117</td>\n",
       "      <td>0.583518</td>\n",
       "      <td>0.024675</td>\n",
       "      <td>0.899760</td>\n",
       "      <td>0.009332</td>\n",
       "      <td>36</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>0.576236</td>\n",
       "      <td>0.019411</td>\n",
       "      <td>0.758024</td>\n",
       "      <td>0.028395</td>\n",
       "      <td>0.479277</td>\n",
       "      <td>0.028117</td>\n",
       "      <td>0.583518</td>\n",
       "      <td>0.024675</td>\n",
       "      <td>0.899760</td>\n",
       "      <td>0.009332</td>\n",
       "      <td>36</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>0.576236</td>\n",
       "      <td>0.019411</td>\n",
       "      <td>0.758024</td>\n",
       "      <td>0.028395</td>\n",
       "      <td>0.479277</td>\n",
       "      <td>0.028117</td>\n",
       "      <td>0.583518</td>\n",
       "      <td>0.024675</td>\n",
       "      <td>0.899614</td>\n",
       "      <td>0.009401</td>\n",
       "      <td>33</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>0.576236</td>\n",
       "      <td>0.019411</td>\n",
       "      <td>0.758024</td>\n",
       "      <td>0.028395</td>\n",
       "      <td>0.479277</td>\n",
       "      <td>0.028117</td>\n",
       "      <td>0.583518</td>\n",
       "      <td>0.024675</td>\n",
       "      <td>0.899614</td>\n",
       "      <td>0.009401</td>\n",
       "      <td>33</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>0.576236</td>\n",
       "      <td>0.019411</td>\n",
       "      <td>0.758024</td>\n",
       "      <td>0.028395</td>\n",
       "      <td>0.479277</td>\n",
       "      <td>0.028117</td>\n",
       "      <td>0.583518</td>\n",
       "      <td>0.024675</td>\n",
       "      <td>0.899614</td>\n",
       "      <td>0.009401</td>\n",
       "      <td>33</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5113</th>\n",
       "      <td>0.579324</td>\n",
       "      <td>0.015904</td>\n",
       "      <td>0.749724</td>\n",
       "      <td>0.023757</td>\n",
       "      <td>0.485824</td>\n",
       "      <td>0.020082</td>\n",
       "      <td>0.585999</td>\n",
       "      <td>0.017390</td>\n",
       "      <td>0.899491</td>\n",
       "      <td>0.008128</td>\n",
       "      <td>45</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7273</th>\n",
       "      <td>0.578545</td>\n",
       "      <td>0.016515</td>\n",
       "      <td>0.749204</td>\n",
       "      <td>0.024082</td>\n",
       "      <td>0.485403</td>\n",
       "      <td>0.020023</td>\n",
       "      <td>0.585523</td>\n",
       "      <td>0.017617</td>\n",
       "      <td>0.899490</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>54</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553</th>\n",
       "      <td>0.578545</td>\n",
       "      <td>0.016515</td>\n",
       "      <td>0.749204</td>\n",
       "      <td>0.024082</td>\n",
       "      <td>0.485403</td>\n",
       "      <td>0.020023</td>\n",
       "      <td>0.585523</td>\n",
       "      <td>0.017617</td>\n",
       "      <td>0.899450</td>\n",
       "      <td>0.008316</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>0.578939</td>\n",
       "      <td>0.016278</td>\n",
       "      <td>0.749638</td>\n",
       "      <td>0.023815</td>\n",
       "      <td>0.485953</td>\n",
       "      <td>0.019541</td>\n",
       "      <td>0.586146</td>\n",
       "      <td>0.016929</td>\n",
       "      <td>0.899386</td>\n",
       "      <td>0.008292</td>\n",
       "      <td>48</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>0.575467</td>\n",
       "      <td>0.019215</td>\n",
       "      <td>0.757824</td>\n",
       "      <td>0.028580</td>\n",
       "      <td>0.478559</td>\n",
       "      <td>0.028304</td>\n",
       "      <td>0.582918</td>\n",
       "      <td>0.024944</td>\n",
       "      <td>0.899376</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>0.575467</td>\n",
       "      <td>0.019215</td>\n",
       "      <td>0.757824</td>\n",
       "      <td>0.028580</td>\n",
       "      <td>0.478559</td>\n",
       "      <td>0.028304</td>\n",
       "      <td>0.582918</td>\n",
       "      <td>0.024944</td>\n",
       "      <td>0.899376</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>0.575467</td>\n",
       "      <td>0.019215</td>\n",
       "      <td>0.757824</td>\n",
       "      <td>0.028580</td>\n",
       "      <td>0.478559</td>\n",
       "      <td>0.028304</td>\n",
       "      <td>0.582918</td>\n",
       "      <td>0.024944</td>\n",
       "      <td>0.899376</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7274</th>\n",
       "      <td>0.577788</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>0.751597</td>\n",
       "      <td>0.033621</td>\n",
       "      <td>0.478736</td>\n",
       "      <td>0.025064</td>\n",
       "      <td>0.581258</td>\n",
       "      <td>0.022628</td>\n",
       "      <td>0.899252</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>54</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6554</th>\n",
       "      <td>0.577788</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>0.751597</td>\n",
       "      <td>0.033621</td>\n",
       "      <td>0.478736</td>\n",
       "      <td>0.025064</td>\n",
       "      <td>0.581258</td>\n",
       "      <td>0.022628</td>\n",
       "      <td>0.899237</td>\n",
       "      <td>0.008041</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766</th>\n",
       "      <td>0.577013</td>\n",
       "      <td>0.013721</td>\n",
       "      <td>0.757850</td>\n",
       "      <td>0.028354</td>\n",
       "      <td>0.449924</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.559522</td>\n",
       "      <td>0.015857</td>\n",
       "      <td>0.899232</td>\n",
       "      <td>0.008838</td>\n",
       "      <td>48</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552</th>\n",
       "      <td>0.579707</td>\n",
       "      <td>0.019877</td>\n",
       "      <td>0.749065</td>\n",
       "      <td>0.030328</td>\n",
       "      <td>0.476553</td>\n",
       "      <td>0.025225</td>\n",
       "      <td>0.579299</td>\n",
       "      <td>0.025065</td>\n",
       "      <td>0.899228</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6486</th>\n",
       "      <td>0.577013</td>\n",
       "      <td>0.013721</td>\n",
       "      <td>0.757850</td>\n",
       "      <td>0.028354</td>\n",
       "      <td>0.449924</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.559522</td>\n",
       "      <td>0.015857</td>\n",
       "      <td>0.899220</td>\n",
       "      <td>0.008916</td>\n",
       "      <td>51</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7206</th>\n",
       "      <td>0.577013</td>\n",
       "      <td>0.013721</td>\n",
       "      <td>0.757850</td>\n",
       "      <td>0.028354</td>\n",
       "      <td>0.449924</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.559522</td>\n",
       "      <td>0.015857</td>\n",
       "      <td>0.899209</td>\n",
       "      <td>0.008911</td>\n",
       "      <td>54</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5046</th>\n",
       "      <td>0.576628</td>\n",
       "      <td>0.013336</td>\n",
       "      <td>0.757389</td>\n",
       "      <td>0.028285</td>\n",
       "      <td>0.449924</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.559398</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>0.899203</td>\n",
       "      <td>0.008949</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7272</th>\n",
       "      <td>0.579707</td>\n",
       "      <td>0.019952</td>\n",
       "      <td>0.749207</td>\n",
       "      <td>0.029931</td>\n",
       "      <td>0.476553</td>\n",
       "      <td>0.025225</td>\n",
       "      <td>0.579342</td>\n",
       "      <td>0.024787</td>\n",
       "      <td>0.899180</td>\n",
       "      <td>0.007994</td>\n",
       "      <td>54</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>0.577403</td>\n",
       "      <td>0.013126</td>\n",
       "      <td>0.751086</td>\n",
       "      <td>0.033298</td>\n",
       "      <td>0.479410</td>\n",
       "      <td>0.024485</td>\n",
       "      <td>0.581571</td>\n",
       "      <td>0.022195</td>\n",
       "      <td>0.899170</td>\n",
       "      <td>0.008166</td>\n",
       "      <td>45</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5834</th>\n",
       "      <td>0.577403</td>\n",
       "      <td>0.013460</td>\n",
       "      <td>0.751186</td>\n",
       "      <td>0.033499</td>\n",
       "      <td>0.478736</td>\n",
       "      <td>0.025064</td>\n",
       "      <td>0.581118</td>\n",
       "      <td>0.022681</td>\n",
       "      <td>0.899167</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>48</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>0.575467</td>\n",
       "      <td>0.018826</td>\n",
       "      <td>0.757531</td>\n",
       "      <td>0.028737</td>\n",
       "      <td>0.479206</td>\n",
       "      <td>0.027853</td>\n",
       "      <td>0.583361</td>\n",
       "      <td>0.024672</td>\n",
       "      <td>0.899153</td>\n",
       "      <td>0.009539</td>\n",
       "      <td>27</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.575467</td>\n",
       "      <td>0.018826</td>\n",
       "      <td>0.757531</td>\n",
       "      <td>0.028737</td>\n",
       "      <td>0.479206</td>\n",
       "      <td>0.027853</td>\n",
       "      <td>0.583361</td>\n",
       "      <td>0.024672</td>\n",
       "      <td>0.899153</td>\n",
       "      <td>0.009539</td>\n",
       "      <td>27</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>0.575467</td>\n",
       "      <td>0.018826</td>\n",
       "      <td>0.757531</td>\n",
       "      <td>0.028737</td>\n",
       "      <td>0.479206</td>\n",
       "      <td>0.027853</td>\n",
       "      <td>0.583361</td>\n",
       "      <td>0.024672</td>\n",
       "      <td>0.899153</td>\n",
       "      <td>0.009539</td>\n",
       "      <td>27</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5112</th>\n",
       "      <td>0.580485</td>\n",
       "      <td>0.019633</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>0.029877</td>\n",
       "      <td>0.476553</td>\n",
       "      <td>0.025225</td>\n",
       "      <td>0.579572</td>\n",
       "      <td>0.024869</td>\n",
       "      <td>0.899138</td>\n",
       "      <td>0.008103</td>\n",
       "      <td>45</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7345</th>\n",
       "      <td>0.577031</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>0.740347</td>\n",
       "      <td>0.024753</td>\n",
       "      <td>0.487836</td>\n",
       "      <td>0.023758</td>\n",
       "      <td>0.585429</td>\n",
       "      <td>0.022450</td>\n",
       "      <td>0.899128</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>54</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3672</th>\n",
       "      <td>0.580870</td>\n",
       "      <td>0.019597</td>\n",
       "      <td>0.750686</td>\n",
       "      <td>0.029264</td>\n",
       "      <td>0.476553</td>\n",
       "      <td>0.025225</td>\n",
       "      <td>0.579797</td>\n",
       "      <td>0.024621</td>\n",
       "      <td>0.899115</td>\n",
       "      <td>0.008239</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3035</th>\n",
       "      <td>0.578568</td>\n",
       "      <td>0.013845</td>\n",
       "      <td>0.748286</td>\n",
       "      <td>0.029542</td>\n",
       "      <td>0.475251</td>\n",
       "      <td>0.026655</td>\n",
       "      <td>0.577953</td>\n",
       "      <td>0.024364</td>\n",
       "      <td>0.899101</td>\n",
       "      <td>0.009230</td>\n",
       "      <td>36</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>0.578545</td>\n",
       "      <td>0.016515</td>\n",
       "      <td>0.749201</td>\n",
       "      <td>0.024131</td>\n",
       "      <td>0.484346</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.584716</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>0.899094</td>\n",
       "      <td>0.008220</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4394</th>\n",
       "      <td>0.577403</td>\n",
       "      <td>0.013460</td>\n",
       "      <td>0.751233</td>\n",
       "      <td>0.033428</td>\n",
       "      <td>0.478599</td>\n",
       "      <td>0.023617</td>\n",
       "      <td>0.580995</td>\n",
       "      <td>0.021574</td>\n",
       "      <td>0.899090</td>\n",
       "      <td>0.008221</td>\n",
       "      <td>42</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "3682            0.575851           0.019279             0.757958   \n",
       "3680            0.575851           0.019279             0.757958   \n",
       "3681            0.575851           0.019279             0.757958   \n",
       "4402            0.575851           0.019279             0.757958   \n",
       "4401            0.575851           0.019279             0.757958   \n",
       "4400            0.575851           0.019279             0.757958   \n",
       "6560            0.575851           0.019279             0.757958   \n",
       "6561            0.575851           0.019279             0.757958   \n",
       "6562            0.575851           0.019279             0.757958   \n",
       "7281            0.575851           0.019279             0.757958   \n",
       "7282            0.575851           0.019279             0.757958   \n",
       "7280            0.575851           0.019279             0.757958   \n",
       "5121            0.575851           0.019279             0.757958   \n",
       "5122            0.575851           0.019279             0.757958   \n",
       "5120            0.575851           0.019279             0.757958   \n",
       "5840            0.575851           0.019279             0.757958   \n",
       "5841            0.575851           0.019279             0.757958   \n",
       "5842            0.575851           0.019279             0.757958   \n",
       "2960            0.576236           0.019411             0.758024   \n",
       "2961            0.576236           0.019411             0.758024   \n",
       "2962            0.576236           0.019411             0.758024   \n",
       "2242            0.576236           0.019411             0.758024   \n",
       "2241            0.576236           0.019411             0.758024   \n",
       "2240            0.576236           0.019411             0.758024   \n",
       "5113            0.579324           0.015904             0.749724   \n",
       "7273            0.578545           0.016515             0.749204   \n",
       "6553            0.578545           0.016515             0.749204   \n",
       "5833            0.578939           0.016278             0.749638   \n",
       "1520            0.575467           0.019215             0.757824   \n",
       "1522            0.575467           0.019215             0.757824   \n",
       "1521            0.575467           0.019215             0.757824   \n",
       "7274            0.577788           0.013496             0.751597   \n",
       "6554            0.577788           0.013496             0.751597   \n",
       "5766            0.577013           0.013721             0.757850   \n",
       "6552            0.579707           0.019877             0.749065   \n",
       "6486            0.577013           0.013721             0.757850   \n",
       "7206            0.577013           0.013721             0.757850   \n",
       "5046            0.576628           0.013336             0.757389   \n",
       "7272            0.579707           0.019952             0.749207   \n",
       "5114            0.577403           0.013126             0.751086   \n",
       "5834            0.577403           0.013460             0.751186   \n",
       "802             0.575467           0.018826             0.757531   \n",
       "800             0.575467           0.018826             0.757531   \n",
       "801             0.575467           0.018826             0.757531   \n",
       "5112            0.580485           0.019633             0.749861   \n",
       "7345            0.577031           0.013250             0.740347   \n",
       "3672            0.580870           0.019597             0.750686   \n",
       "3035            0.578568           0.013845             0.748286   \n",
       "3673            0.578545           0.016515             0.749201   \n",
       "4394            0.577403           0.013460             0.751233   \n",
       "\n",
       "      std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "3682            0.028434          0.478906         0.028255      0.583214   \n",
       "3680            0.028434          0.478906         0.028255      0.583214   \n",
       "3681            0.028434          0.478906         0.028255      0.583214   \n",
       "4402            0.028434          0.478906         0.028255      0.583214   \n",
       "4401            0.028434          0.478906         0.028255      0.583214   \n",
       "4400            0.028434          0.478906         0.028255      0.583214   \n",
       "6560            0.028434          0.478906         0.028255      0.583214   \n",
       "6561            0.028434          0.478906         0.028255      0.583214   \n",
       "6562            0.028434          0.478906         0.028255      0.583214   \n",
       "7281            0.028434          0.478906         0.028255      0.583214   \n",
       "7282            0.028434          0.478906         0.028255      0.583214   \n",
       "7280            0.028434          0.478906         0.028255      0.583214   \n",
       "5121            0.028434          0.478906         0.028255      0.583214   \n",
       "5122            0.028434          0.478906         0.028255      0.583214   \n",
       "5120            0.028434          0.478906         0.028255      0.583214   \n",
       "5840            0.028434          0.478906         0.028255      0.583214   \n",
       "5841            0.028434          0.478906         0.028255      0.583214   \n",
       "5842            0.028434          0.478906         0.028255      0.583214   \n",
       "2960            0.028395          0.479277         0.028117      0.583518   \n",
       "2961            0.028395          0.479277         0.028117      0.583518   \n",
       "2962            0.028395          0.479277         0.028117      0.583518   \n",
       "2242            0.028395          0.479277         0.028117      0.583518   \n",
       "2241            0.028395          0.479277         0.028117      0.583518   \n",
       "2240            0.028395          0.479277         0.028117      0.583518   \n",
       "5113            0.023757          0.485824         0.020082      0.585999   \n",
       "7273            0.024082          0.485403         0.020023      0.585523   \n",
       "6553            0.024082          0.485403         0.020023      0.585523   \n",
       "5833            0.023815          0.485953         0.019541      0.586146   \n",
       "1520            0.028580          0.478559         0.028304      0.582918   \n",
       "1522            0.028580          0.478559         0.028304      0.582918   \n",
       "1521            0.028580          0.478559         0.028304      0.582918   \n",
       "7274            0.033621          0.478736         0.025064      0.581258   \n",
       "6554            0.033621          0.478736         0.025064      0.581258   \n",
       "5766            0.028354          0.449924         0.017621      0.559522   \n",
       "6552            0.030328          0.476553         0.025225      0.579299   \n",
       "6486            0.028354          0.449924         0.017621      0.559522   \n",
       "7206            0.028354          0.449924         0.017621      0.559522   \n",
       "5046            0.028285          0.449924         0.017621      0.559398   \n",
       "7272            0.029931          0.476553         0.025225      0.579342   \n",
       "5114            0.033298          0.479410         0.024485      0.581571   \n",
       "5834            0.033499          0.478736         0.025064      0.581118   \n",
       "802             0.028737          0.479206         0.027853      0.583361   \n",
       "800             0.028737          0.479206         0.027853      0.583361   \n",
       "801             0.028737          0.479206         0.027853      0.583361   \n",
       "5112            0.029877          0.476553         0.025225      0.579572   \n",
       "7345            0.024753          0.487836         0.023758      0.585429   \n",
       "3672            0.029264          0.476553         0.025225      0.579797   \n",
       "3035            0.029542          0.475251         0.026655      0.577953   \n",
       "3673            0.024131          0.484346         0.019989      0.584716   \n",
       "4394            0.033428          0.478599         0.023617      0.580995   \n",
       "\n",
       "      std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "3682     0.024835       0.899871      0.009476                   39   \n",
       "3680     0.024835       0.899871      0.009476                   39   \n",
       "3681     0.024835       0.899871      0.009476                   39   \n",
       "4402     0.024835       0.899852      0.009455                   42   \n",
       "4401     0.024835       0.899852      0.009455                   42   \n",
       "4400     0.024835       0.899852      0.009455                   42   \n",
       "6560     0.024835       0.899837      0.009435                   51   \n",
       "6561     0.024835       0.899837      0.009435                   51   \n",
       "6562     0.024835       0.899837      0.009435                   51   \n",
       "7281     0.024835       0.899835      0.009435                   54   \n",
       "7282     0.024835       0.899835      0.009435                   54   \n",
       "7280     0.024835       0.899835      0.009435                   54   \n",
       "5121     0.024835       0.899833      0.009434                   45   \n",
       "5122     0.024835       0.899833      0.009434                   45   \n",
       "5120     0.024835       0.899833      0.009434                   45   \n",
       "5840     0.024835       0.899825      0.009432                   48   \n",
       "5841     0.024835       0.899825      0.009432                   48   \n",
       "5842     0.024835       0.899825      0.009432                   48   \n",
       "2960     0.024675       0.899760      0.009332                   36   \n",
       "2961     0.024675       0.899760      0.009332                   36   \n",
       "2962     0.024675       0.899760      0.009332                   36   \n",
       "2242     0.024675       0.899614      0.009401                   33   \n",
       "2241     0.024675       0.899614      0.009401                   33   \n",
       "2240     0.024675       0.899614      0.009401                   33   \n",
       "5113     0.017390       0.899491      0.008128                   45   \n",
       "7273     0.017617       0.899490      0.008251                   54   \n",
       "6553     0.017617       0.899450      0.008316                   51   \n",
       "5833     0.016929       0.899386      0.008292                   48   \n",
       "1520     0.024944       0.899376      0.009546                   30   \n",
       "1522     0.024944       0.899376      0.009546                   30   \n",
       "1521     0.024944       0.899376      0.009546                   30   \n",
       "7274     0.022628       0.899252      0.007951                   54   \n",
       "6554     0.022628       0.899237      0.008041                   51   \n",
       "5766     0.015857       0.899232      0.008838                   48   \n",
       "6552     0.025065       0.899228      0.008001                   51   \n",
       "6486     0.015857       0.899220      0.008916                   51   \n",
       "7206     0.015857       0.899209      0.008911                   54   \n",
       "5046     0.015883       0.899203      0.008949                   45   \n",
       "7272     0.024787       0.899180      0.007994                   54   \n",
       "5114     0.022195       0.899170      0.008166                   45   \n",
       "5834     0.022681       0.899167      0.008064                   48   \n",
       "802      0.024672       0.899153      0.009539                   27   \n",
       "800      0.024672       0.899153      0.009539                   27   \n",
       "801      0.024672       0.899153      0.009539                   27   \n",
       "5112     0.024869       0.899138      0.008103                   45   \n",
       "7345     0.022450       0.899128      0.009641                   54   \n",
       "3672     0.024621       0.899115      0.008239                   39   \n",
       "3035     0.024364       0.899101      0.009230                   36   \n",
       "3673     0.017715       0.899094      0.008220                   39   \n",
       "4394     0.021574       0.899090      0.008221                   42   \n",
       "\n",
       "      param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "3682                     0.2                           3   \n",
       "3680                     0.2                           3   \n",
       "3681                     0.2                           3   \n",
       "4402                     0.2                           3   \n",
       "4401                     0.2                           3   \n",
       "4400                     0.2                           3   \n",
       "6560                     0.2                           3   \n",
       "6561                     0.2                           3   \n",
       "6562                     0.2                           3   \n",
       "7281                     0.2                           3   \n",
       "7282                     0.2                           3   \n",
       "7280                     0.2                           3   \n",
       "5121                     0.2                           3   \n",
       "5122                     0.2                           3   \n",
       "5120                     0.2                           3   \n",
       "5840                     0.2                           3   \n",
       "5841                     0.2                           3   \n",
       "5842                     0.2                           3   \n",
       "2960                     0.2                           3   \n",
       "2961                     0.2                           3   \n",
       "2962                     0.2                           3   \n",
       "2242                     0.2                           3   \n",
       "2241                     0.2                           3   \n",
       "2240                     0.2                           3   \n",
       "5113                     0.2                           1   \n",
       "7273                     0.2                           1   \n",
       "6553                     0.2                           1   \n",
       "5833                     0.2                           1   \n",
       "1520                     0.2                           3   \n",
       "1522                     0.2                           3   \n",
       "1521                     0.2                           3   \n",
       "7274                     0.2                           1   \n",
       "6554                     0.2                           1   \n",
       "5766                     0.1                           1   \n",
       "6552                     0.2                           1   \n",
       "6486                     0.1                           1   \n",
       "7206                     0.1                           1   \n",
       "5046                     0.1                           1   \n",
       "7272                     0.2                           1   \n",
       "5114                     0.2                           1   \n",
       "5834                     0.2                           1   \n",
       "802                      0.2                           3   \n",
       "800                      0.2                           3   \n",
       "801                      0.2                           3   \n",
       "5112                     0.2                           1   \n",
       "7345                     0.3                           1   \n",
       "3672                     0.2                           1   \n",
       "3035                     0.3                           3   \n",
       "3673                     0.2                           1   \n",
       "4394                     0.2                           1   \n",
       "\n",
       "      param_rf__min_samples_split  param_rf__n_estimators  \n",
       "3682                            6                     200  \n",
       "3680                            2                     200  \n",
       "3681                            4                     200  \n",
       "4402                            6                     200  \n",
       "4401                            4                     200  \n",
       "4400                            2                     200  \n",
       "6560                            2                     200  \n",
       "6561                            4                     200  \n",
       "6562                            6                     200  \n",
       "7281                            4                     200  \n",
       "7282                            6                     200  \n",
       "7280                            2                     200  \n",
       "5121                            4                     200  \n",
       "5122                            6                     200  \n",
       "5120                            2                     200  \n",
       "5840                            2                     200  \n",
       "5841                            4                     200  \n",
       "5842                            6                     200  \n",
       "2960                            2                     200  \n",
       "2961                            4                     200  \n",
       "2962                            6                     200  \n",
       "2242                            6                     200  \n",
       "2241                            4                     200  \n",
       "2240                            2                     200  \n",
       "5113                            4                     200  \n",
       "7273                            4                     200  \n",
       "6553                            4                     200  \n",
       "5833                            4                     200  \n",
       "1520                            2                     200  \n",
       "1522                            6                     200  \n",
       "1521                            4                     200  \n",
       "7274                            6                     200  \n",
       "6554                            6                     200  \n",
       "5766                           14                     200  \n",
       "6552                            2                     200  \n",
       "6486                           14                     200  \n",
       "7206                           14                     200  \n",
       "5046                           14                     200  \n",
       "7272                            2                     200  \n",
       "5114                            6                     200  \n",
       "5834                            6                     200  \n",
       "802                             6                     200  \n",
       "800                             2                     200  \n",
       "801                             4                     200  \n",
       "5112                            2                     200  \n",
       "7345                            4                     200  \n",
       "3672                            2                     200  \n",
       "3035                            8                     200  \n",
       "3673                            4                     200  \n",
       "4394                            6                     200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_rf = ms.load_gs_results(\"/q/PET-MBF/output/17_segment/localization/rf/results/rf_17s_r2-job592.csv\")\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "ms.print_top_n_hyperparams(res_rf, metrics, n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this 2nd round of hyperparameter tuning, we see negligible improvement in top validation AUC score. The top validation AUC increases from 0.899348 +/- 0.007906 to 0.899871 +/- 0.009476. Because this change was so slight, we will stop hyperparameter tuning at this point. \n",
    "\n",
    "We will use those hyperparameters which give the highest AUC, and increase n_estimators to 1000, as this should not affect overfitting but could potentially increase classification performance.\n",
    "\n",
    "#### Random forest final hyperparameter settings:\n",
    "- max_depth: 39\n",
    "- max_features: 0.2\n",
    "- min_samples_leaf: 3\n",
    "- min_samples_split: 4\n",
    "- n_estimators: 1000\n",
    "\n",
    "#### Explore cross validation AUC scores for final hyperparameter settings with increase of n_estimators to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 640\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hp_spec = {'n_estimators': [200, 1000],\n",
    "           'max_depth':  [39],\n",
    "           'min_samples_split': [4], \n",
    "           'min_samples_leaf': [3],\n",
    "           'max_features': [0.2]}\n",
    "\n",
    "ms.ms_on_hpc(model='rf',\n",
    "             data_dir='/q/PET-MBF/data',\n",
    "             out_dir='/q/PET-MBF/output',\n",
    "             hp_spec=hp_spec,\n",
    "             dataset='17_segment',\n",
    "             problem='localization',\n",
    "             ms_round=3,\n",
    "             mem='8GB',\n",
    "             time='10:00',\n",
    "             cpus=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_17s_r1-job575.csv  rf_17s_r2-job592.csv  rf_17s_r3-job640.csv\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n",
      "               639     batch jupyter- dberman_  R       9:41      1 ohi-hpc2-keon01 \n"
     ]
    }
   ],
   "source": [
    "!ls /q/PET-MBF/output/17_segment/localization/rf/results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 param settings mean_test_accuracy:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.578177</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>0.753116</td>\n",
       "      <td>0.027614</td>\n",
       "      <td>0.469956</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>0.574642</td>\n",
       "      <td>0.019482</td>\n",
       "      <td>0.900264</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899871</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "1            0.578177           0.018947             0.753116   \n",
       "0            0.575851           0.019279             0.757958   \n",
       "\n",
       "   std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "1            0.027614          0.469956         0.021281      0.574642   \n",
       "0            0.028434          0.478906         0.028255      0.583214   \n",
       "\n",
       "   std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "1     0.019482       0.900264      0.008690                   39   \n",
       "0     0.024835       0.899871      0.009476                   39   \n",
       "\n",
       "   param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "1                     0.2                           3   \n",
       "0                     0.2                           3   \n",
       "\n",
       "   param_rf__min_samples_split  param_rf__n_estimators  \n",
       "1                            4                    1000  \n",
       "0                            4                     200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 5 param settings mean_test_precision:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899871</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.578177</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>0.753116</td>\n",
       "      <td>0.027614</td>\n",
       "      <td>0.469956</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>0.574642</td>\n",
       "      <td>0.019482</td>\n",
       "      <td>0.900264</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "0            0.575851           0.019279             0.757958   \n",
       "1            0.578177           0.018947             0.753116   \n",
       "\n",
       "   std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "0            0.028434          0.478906         0.028255      0.583214   \n",
       "1            0.027614          0.469956         0.021281      0.574642   \n",
       "\n",
       "   std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "0     0.024835       0.899871      0.009476                   39   \n",
       "1     0.019482       0.900264      0.008690                   39   \n",
       "\n",
       "   param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "0                     0.2                           3   \n",
       "1                     0.2                           3   \n",
       "\n",
       "   param_rf__min_samples_split  param_rf__n_estimators  \n",
       "0                            4                     200  \n",
       "1                            4                    1000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 5 param settings mean_test_recall:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899871</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.578177</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>0.753116</td>\n",
       "      <td>0.027614</td>\n",
       "      <td>0.469956</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>0.574642</td>\n",
       "      <td>0.019482</td>\n",
       "      <td>0.900264</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "0            0.575851           0.019279             0.757958   \n",
       "1            0.578177           0.018947             0.753116   \n",
       "\n",
       "   std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "0            0.028434          0.478906         0.028255      0.583214   \n",
       "1            0.027614          0.469956         0.021281      0.574642   \n",
       "\n",
       "   std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "0     0.024835       0.899871      0.009476                   39   \n",
       "1     0.019482       0.900264      0.008690                   39   \n",
       "\n",
       "   param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "0                     0.2                           3   \n",
       "1                     0.2                           3   \n",
       "\n",
       "   param_rf__min_samples_split  param_rf__n_estimators  \n",
       "0                            4                     200  \n",
       "1                            4                    1000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 5 param settings mean_test_f1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899871</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.578177</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>0.753116</td>\n",
       "      <td>0.027614</td>\n",
       "      <td>0.469956</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>0.574642</td>\n",
       "      <td>0.019482</td>\n",
       "      <td>0.900264</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "0            0.575851           0.019279             0.757958   \n",
       "1            0.578177           0.018947             0.753116   \n",
       "\n",
       "   std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "0            0.028434          0.478906         0.028255      0.583214   \n",
       "1            0.027614          0.469956         0.021281      0.574642   \n",
       "\n",
       "   std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "0     0.024835       0.899871      0.009476                   39   \n",
       "1     0.019482       0.900264      0.008690                   39   \n",
       "\n",
       "   param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "0                     0.2                           3   \n",
       "1                     0.2                           3   \n",
       "\n",
       "   param_rf__min_samples_split  param_rf__n_estimators  \n",
       "0                            4                     200  \n",
       "1                            4                    1000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 5 param settings mean_test_auc:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>mean_test_auc</th>\n",
       "      <th>std_test_auc</th>\n",
       "      <th>param_rf__max_depth</th>\n",
       "      <th>param_rf__max_features</th>\n",
       "      <th>param_rf__min_samples_leaf</th>\n",
       "      <th>param_rf__min_samples_split</th>\n",
       "      <th>param_rf__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.578177</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>0.753116</td>\n",
       "      <td>0.027614</td>\n",
       "      <td>0.469956</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>0.574642</td>\n",
       "      <td>0.019482</td>\n",
       "      <td>0.900264</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.757958</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.478906</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.583214</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>0.899871</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
       "1            0.578177           0.018947             0.753116   \n",
       "0            0.575851           0.019279             0.757958   \n",
       "\n",
       "   std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
       "1            0.027614          0.469956         0.021281      0.574642   \n",
       "0            0.028434          0.478906         0.028255      0.583214   \n",
       "\n",
       "   std_test_f1  mean_test_auc  std_test_auc  param_rf__max_depth  \\\n",
       "1     0.019482       0.900264      0.008690                   39   \n",
       "0     0.024835       0.899871      0.009476                   39   \n",
       "\n",
       "   param_rf__max_features  param_rf__min_samples_leaf  \\\n",
       "1                     0.2                           3   \n",
       "0                     0.2                           3   \n",
       "\n",
       "   param_rf__min_samples_split  param_rf__n_estimators  \n",
       "1                            4                    1000  \n",
       "0                            4                     200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_rf = ms.load_gs_results(\"/q/PET-MBF/output/17_segment/localization/rf/results/rf_17s_r3-job640.csv\")\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "ms.print_top_n_hyperparams(res_rf, metrics, n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a marginal increase in validation AUC when the model is trained again with n_estimators of 1000 with other hyperparameters set to those which yeilded the highest validation AUC in the previous grid search.\n",
    "\n",
    "#### Random forest final hyperparameter settings:\n",
    "- max_depth: 39\n",
    "- max_features: 0.2\n",
    "- min_samples_leaf: 3\n",
    "- min_samples_split: 4\n",
    "- n_estimators: 1000\n",
    "\n",
    "We build and save the RF with these final hyperparameters for comparison against other models in the script e-emagin-pet/codebase/train_final_models/train_models_17s_loc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Bayesian optimization for hyperparameter tuning of the multilayer perceptron. Bayesian optimization is a method of automated hyperparameter tuning which can identify the optimal set of hyperparameters much more efficiently than a gridsearch. We use the [Hyperopt](http://hyperopt.github.io/hyperopt/) package through the [Ray Tune](https://docs.ray.io/en/latest/tune/) interface as our implementation of Bayesian optimization.\n",
    "\n",
    "We use the [parameter expressions](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/) outlined in the HyperOpt documentation to define the hyperparameter space that we want to search over. \n",
    "\n",
    "The following hyperparameters can be searched over. These are defined in the keras MLP implemented in in ms_on_hpc.py and can be added to by editing the implementation in that file. They are defined in that file with names that must be consistant with those in the hyperparameter space definitions used in this file. \n",
    "\n",
    "Hyperparameter | Description | Helpful Links | Hyperparameter name for Ray Tune | Data type\n",
    ":-------------:|:----------- | :------------ | :------------------------------: | :----:\n",
    "Learning Rate | The learning rate for the neural network. This influences how much the loss function changes at each step of the optimization of the loss function during training. A learning rate that is too high will cause the optimization process to jump past local minimums, sometimes causing exploding or unstable loss. A learning rate that is too slow will lead to training that will take far too long, and could cause optimization to get stuck at local minima that are not optimal. A reasonable range for learning rates to try with the Adam optimizer is between 1e-4 and 1e-2. | [Stanford Neural Network Notes: Loss](https://cs231n.github.io/neural-networks-3/#loss) | 'lr' | float \n",
    "Drop Probability | Dropout is applied to all hidden layers of the MLP at the same rate. Dropout is a regularization technique that can help prevent overfitting of a neural network. A higher drop probability has the effect of increased regularization, while lower drop probabiliy has the effect of decreased regularization. Drop probability is the probability that a hidden node will randomly be set to 0 at any step of training. Randomly setting hidden nodes to 0 (at a specified rate / probability) during training increases regularization by forcing the network not to rely too heavily on any one single feature or single set of features. | [Stanford Neural Network Notes: Regularization](https://cs231n.github.io/neural-networks-2/#reg) | 'drop_prob' | float\n",
    "L2 Regularization Term | L2 regularization is applied with the same weight at every layer of the network. A larger L2 regularization term leads to increased regularization, while a smaller L2 regularization term will lead to less regularization. L2 regularization penalizes the squared magnitude of the weights from each hidden layer. This has the effect of generally encouraging smaller weight values and discouraging layers that have very large differences between individual weight values. | [Stanford Neural Network Notes: Regularization](https://cs231n.github.io/neural-networks-2/#reg) | 'reg' | float\n",
    "Number of Layers | The number of layers in the network. More layers will generally increase the ability of the network to detect and represent more complex relationships. However, there are diminishing returns on adding additional layers. For a problem of this size especially (input space of 68 features) 3 layers is likely to be sufficient. Although increasing the number of layers will allow the network to represent more complex relationships, and thus, potentially increase the opportunity for overfitting, it is not recommended to limit the number of layers as a means of regularization--instead it is preferable to use dropout / regularization  | [Stanford Neural Network Notes: Representational Power](https://cs231n.github.io/neural-networks-1/#power) | 'num_layers'| int\n",
    "size of hidden layer n | The number of nodes in a hidden layer. Larger layer sizes increase representational power of the neural network, although like with number of layers, there is diminishing returns on increasing layer size. | [Stanford Neural Network Notes: Representational Power](https://cs231n.github.io/neural-networks-1/#power) | 'ln' where n is the layer number (i.e. for 'l1' for layer 1, 'l2' for layer 2...) | int\n",
    "\n",
    "\n",
    "To accomodate searching over networks with varying numbers of layers, we use hp.choice with the label 'layers' and a list of hyperparameter spaces, one corresponding to each number of layers we would like to include in our search space. In each element of the list, we set num_layers to be the appropriate number of layers, and include hyperparameter space definitions for ALL layers, l1 - ln, where we set the size of any layer greater than the current number of layers to 0. These layers will not actually be constructed, but their hidden size will be reported to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 599\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'lr': hp.choice('lr', [1e-4, 1e-3, 1e-2]),\n",
    "    'drop_prob': hp.choice('drop_prob', [0, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]),\n",
    "    'reg': hp.choice('reg', [0, 0.01, 0.1, 1, 10, 100]),\n",
    "    \n",
    "    'layers': hp.choice('layers', [{\n",
    "        'num_layers': 1,\n",
    "        'l1': hp.choice('l1_1', [8, 16, 32, 64, 128, 256]),\n",
    "        'l2': hp.choice('l2_1', [0]),\n",
    "        'l3': hp.choice('l3_1', [0])\n",
    "    }, {\n",
    "         'num_layers': 2,\n",
    "         'l1': hp.choice('l1_2', [8, 16, 32, 64, 128, 256]),\n",
    "         'l2': hp.choice('l2_2', [8, 16, 32, 64, 128, 256]),\n",
    "         'l3': hp.choice('l3_2', [0])\n",
    "    }, {\n",
    "         'num_layers': 3,\n",
    "         'l1': hp.choice('l1_3', [8, 16, 32, 64, 128, 256]),\n",
    "         'l2': hp.choice('l2_3', [8, 16, 32, 64, 128, 256]),\n",
    "         'l3': hp.choice('l3_3', [8, 16, 32, 64, 128, 256])\n",
    "    }])\n",
    "}\n",
    "\n",
    "ms.launch_nn_ray_tune_hpc(model='mlp',\n",
    "                          data_dir='/q/PET-MBF/data',\n",
    "                          out_dir='/q/PET-MBF/output',\n",
    "                          hp_spec=space,\n",
    "                          dataset='17_segment',\n",
    "                          problem='localization',\n",
    "                          ms_round=1,\n",
    "                          nn_search_alg='ho',\n",
    "                          num_samples=2000,\n",
    "                          epochs=200,\n",
    "                          job_name_suffix=None,\n",
    "                          mem='20GB',\n",
    "                          time='72:00:00',\n",
    "                          cpus=5,\n",
    "                          gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dberman_mitre-mlp_17s_r1-job599.out\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n",
      "               595     batch jupyter- dberman_  R    2:26:01      1 ohi-hpc2-keon01 \n",
      "               599     batch mlp_17s_ dberman_  R       1:04      1 ohi-hpc2-keon01 \n"
     ]
    }
   ],
   "source": [
    "!ls /q/PET-MBF/output/17_segment/localization/mlp/logs\n",
    "!squeue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage example: change_mofed_version.sh 4.5-1.0.1\r\n",
      "2020-12-21 15:54:58,153\tINFO services.py:1166 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://localhost:8265\u001b[39m\u001b[22m\r\n",
      "Usage example: change_mofed_version.sh 4.5-1.0.1\r\n",
      "2020-12-21 15:55:05.939442: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "2020-12-21 15:55:10,103\tINFO worker.py:634 -- Connecting to existing Ray cluster at address: 10.16.65.11:46463\r\n",
      "2020-12-21 15:55:10,169\tWARNING experiment.py:256 -- No name detected on trainable. Using DEFAULT.\r\n",
      "2020-12-21 15:55:10,174\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\r\n",
      "2020-12-21 15:56:23,445\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.6237599849700928 seconds to complete, which may be a performance bottleneck.\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:23.827416: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.574735: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.600630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.600691: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.633254: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.650082: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.659628: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.686906: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.695303: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.696744: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.699939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.721084: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2794680000 Hz\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.722245: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f625a808100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.722277: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.817247: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f625a8ddd40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.817302: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.820726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.820794: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.820827: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.820839: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.820851: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.820861: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.820872: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.820883: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.823236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:26.824703: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:29.528641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:29.528713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:29.528722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:29.535575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21071 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:25:00.0, compute capability: 7.5)\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:30.631323: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m 2020-12-21 15:56:32.165109: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2020-12-21 15:56:34,606\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.5982069969177246 seconds to complete, which may be a performance bottleneck.\r\n",
      "2020-12-21 15:56:44,658\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.5953879356384277 seconds to complete, which may be a performance bottleneck.\r\n",
      "gpu:  1\r\n",
      "Nodes in the Ray cluster:\r\n",
      "[{'NodeID': '7fddd0ba959b1616f124d38c5d9fbc7cbd87b87b', 'Alive': True, 'NodeManagerAddress': '10.16.65.11', 'NodeManagerHostname': 'ohi-hpc2-keon01', 'NodeManagerPort': 51558, 'ObjectManagerPort': 41933, 'ObjectStoreSocketName': '/tmp/ray/session_2020-12-21_15-54-57_732570_65872/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2020-12-21_15-54-57_732570_65872/sockets/raylet', 'MetricsExportPort': 52921, 'alive': True, 'Resources': {'object_store_memory': 1048.0, 'node:10.16.65.11': 1.0, 'memory': 3356.0, 'GPU': 1.0, 'accelerator_type:RTX': 1.0, 'CPU': 96.0}}]\r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 9.1/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1999 PENDING, 1 RUNNING)\r\n",
      "+------------------+----------+-------+-------------+-------------+-------------+-------------+---------------------+--------+-------+\r\n",
      "| Trial name       | status   | loc   |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |\r\n",
      "|------------------+----------+-------+-------------+-------------+-------------+-------------+---------------------+--------+-------|\r\n",
      "| DEFAULT_00040ba2 | PENDING  |       |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_000a181c | PENDING  |       |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_001057b8 | PENDING  |       |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_0016b428 | PENDING  |       |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_001cf9c8 | PENDING  |       |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_0022ffbc | PENDING  |       |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_00296172 | PENDING  |       |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_002fcd46 | PENDING  |       |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_0036468a | PENDING  |       |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_003c6e66 | PENDING  |       |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_00429c64 | PENDING  |       |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_0048e2a4 | PENDING  |       |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_004f4856 | PENDING  |       |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_0055a98a | PENDING  |       |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_005c0e92 | PENDING  |       |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_00629e1a | PENDING  |       |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_0068f8aa | PENDING  |       |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_006f1726 | PENDING  |       |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_007527e2 | PENDING  |       |        0.05 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |\r\n",
      "| DEFAULT_e77538f4 | RUNNING  |       |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |\r\n",
      "+------------------+----------+-------+-------------+-------------+-------------+-------------+---------------------+--------+-------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m  input shape:  68 \r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66322)\u001b[0m \r\n",
      "Result for DEFAULT_e77538f4:\r\n",
      "  date: 2020-12-21_15-56-34\r\n",
      "  done: false\r\n",
      "  experiment_id: bbcef2b3ccfa4294b0ec3e7c12eb7f2d\r\n",
      "  experiment_tag: 1_drop_prob=0,l1=8,l2=256,l3=0,num_layers=2,lr=0.0001,reg=1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 1\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66322\r\n",
      "  time_since_restore: 10.922654390335083\r\n",
      "  time_this_iter_s: 10.922654390335083\r\n",
      "  time_total_s: 10.922654390335083\r\n",
      "  timestamp: 1608566194\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.055457744747400284\r\n",
      "  train_auc: 0.4991476535797119\r\n",
      "  train_loss: 515.5027465820312\r\n",
      "  training_iteration: 1\r\n",
      "  trial_id: e77538f4\r\n",
      "  val_accuracy: 0.08024691045284271\r\n",
      "  val_auc: 0.46752870082855225\r\n",
      "  val_loss: 513.8192749023438\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 11.0/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1999 PENDING, 1 RUNNING)\r\n",
      "+------------------+----------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status   | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+----------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING  |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING  |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING  |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0068f8aa | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_006f1726 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_007527e2 | PENDING  |                   |        0.05 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e77538f4 | RUNNING  | 10.16.65.11:66322 |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |      1 |          10.9227 |      515.503 |    513.819 |        0.0554577 |\r\n",
      "+------------------+----------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:49.492419: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.033850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.072186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.072293: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.074697: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.075668: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.075873: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.078401: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.079077: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.079193: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.081853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.089911: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2794680000 Hz\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.090346: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe09a9e8960 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.090363: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.157690: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe09a807520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.157735: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.159006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.159047: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.159076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.159090: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.159102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.159115: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.159127: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.159140: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.161425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.161462: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.492283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.492362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.492373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:51.496883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21071 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:25:00.0, compute capability: 7.5)\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:52.642671: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m 2020-12-21 15:56:53.029384: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2020-12-21 15:56:54,703\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.5846512317657471 seconds to complete, which may be a performance bottleneck.\r\n",
      "\r\n",
      "Result for DEFAULT_e77538f4:\r\n",
      "  date: 2020-12-21_15-56-39\r\n",
      "  done: false\r\n",
      "  experiment_id: bbcef2b3ccfa4294b0ec3e7c12eb7f2d\r\n",
      "  experiment_tag: 1_drop_prob=0,l1=8,l2=256,l3=0,num_layers=2,lr=0.0001,reg=1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 65\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66322\r\n",
      "  time_since_restore: 15.927937746047974\r\n",
      "  time_this_iter_s: 0.06722140312194824\r\n",
      "  time_total_s: 15.927937746047974\r\n",
      "  timestamp: 1608566199\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.20334507524967194\r\n",
      "  train_auc: 0.770289421081543\r\n",
      "  train_loss: 351.0860290527344\r\n",
      "  training_iteration: 65\r\n",
      "  trial_id: e77538f4\r\n",
      "  val_accuracy: 0.15740740299224854\r\n",
      "  val_auc: 0.7663161158561707\r\n",
      "  val_loss: 349.98919677734375\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 11.0/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1999 PENDING, 1 RUNNING)\r\n",
      "+------------------+----------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status   | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+----------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING  |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING  |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING  |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0068f8aa | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_006f1726 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_007527e2 | PENDING  |                   |        0.05 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e77538f4 | RUNNING  | 10.16.65.11:66322 |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |     74 |          16.5481 |      332.927 |     331.89 |         0.204225 |\r\n",
      "+------------------+----------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "Result for DEFAULT_e77538f4:\r\n",
      "  date: 2020-12-21_15-56-44\r\n",
      "  done: false\r\n",
      "  experiment_id: bbcef2b3ccfa4294b0ec3e7c12eb7f2d\r\n",
      "  experiment_tag: 1_drop_prob=0,l1=8,l2=256,l3=0,num_layers=2,lr=0.0001,reg=1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 138\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66322\r\n",
      "  time_since_restore: 20.979442834854126\r\n",
      "  time_this_iter_s: 0.06895899772644043\r\n",
      "  time_total_s: 20.979442834854126\r\n",
      "  timestamp: 1608566204\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.2354753464460373\r\n",
      "  train_auc: 0.8497437834739685\r\n",
      "  train_loss: 227.69017028808594\r\n",
      "  training_iteration: 138\r\n",
      "  trial_id: e77538f4\r\n",
      "  val_accuracy: 0.20370370149612427\r\n",
      "  val_auc: 0.8306562900543213\r\n",
      "  val_loss: 226.9787139892578\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 11.0/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1999 PENDING, 1 RUNNING)\r\n",
      "+------------------+----------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status   | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+----------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING  |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING  |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING  |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING  |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0068f8aa | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_006f1726 | PENDING  |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_007527e2 | PENDING  |                   |        0.05 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e77538f4 | RUNNING  | 10.16.65.11:66322 |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    138 |          20.9794 |       227.69 |    226.979 |         0.235475 |\r\n",
      "+------------------+----------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "2020-12-21 15:57:04,778\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.6316719055175781 seconds to complete, which may be a performance bottleneck.\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m  input shape:  68 \r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66255)\u001b[0m \r\n",
      "Result for DEFAULT_e777a68e:\r\n",
      "  date: 2020-12-21_15-56-53\r\n",
      "  done: false\r\n",
      "  experiment_id: 2fc8d6298fa6420cb346d1667b9b5f0a\r\n",
      "  experiment_tag: 2_drop_prob=0.05,l1=8,l2=32,l3=64,num_layers=3,lr=0.001,reg=0.1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 1\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66255\r\n",
      "  time_since_restore: 4.412330150604248\r\n",
      "  time_this_iter_s: 4.412330150604248\r\n",
      "  time_total_s: 4.412330150604248\r\n",
      "  timestamp: 1608566213\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.18617957830429077\r\n",
      "  train_auc: 0.4902760088443756\r\n",
      "  train_loss: 20.8732852935791\r\n",
      "  training_iteration: 1\r\n",
      "  trial_id: e777a68e\r\n",
      "  val_accuracy: 0.046296294778585434\r\n",
      "  val_auc: 0.5692344307899475\r\n",
      "  val_loss: 19.763351440429688\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 11.0/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1998 PENDING, 1 RUNNING, 1 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0068f8aa | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_006f1726 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e777a68e | RUNNING    | 10.16.65.11:66255 |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |      1 |          4.41233 |      20.8733 |    19.7634 |         0.18618  |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |         25.8359  |     155.706  |   155.21   |         0.238556 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "Result for DEFAULT_e777a68e:\r\n",
      "  date: 2020-12-21_15-56-58\r\n",
      "  done: false\r\n",
      "  experiment_id: 2fc8d6298fa6420cb346d1667b9b5f0a\r\n",
      "  experiment_tag: 2_drop_prob=0.05,l1=8,l2=32,l3=64,num_layers=3,lr=0.001,reg=0.1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 60\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66255\r\n",
      "  time_since_restore: 9.427993774414062\r\n",
      "  time_this_iter_s: 0.07415771484375\r\n",
      "  time_total_s: 9.427993774414062\r\n",
      "  timestamp: 1608566218\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.27508804202079773\r\n",
      "  train_auc: 0.8903757929801941\r\n",
      "  train_loss: 0.5209622383117676\r\n",
      "  training_iteration: 60\r\n",
      "  trial_id: e777a68e\r\n",
      "  val_accuracy: 0.1975308656692505\r\n",
      "  val_auc: 0.8718388676643372\r\n",
      "  val_loss: 0.5834538340568542\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 11.0/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1998 PENDING, 1 RUNNING, 1 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0068f8aa | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_006f1726 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e777a68e | RUNNING    | 10.16.65.11:66255 |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |     60 |          9.42799 |     0.520962 |   0.583454 |         0.275088 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |         25.8359  |   155.706    | 155.21     |         0.238556 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:10.392525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:11.881941: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:11.917436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:11.917500: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:11.919429: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:11.920414: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:11.920609: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:11.922949: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:11.923527: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:11.923660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:11.926275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:11.934817: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2794680000 Hz\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:11.935240: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f906a9e6120 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:11.935258: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.006400: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f906a806920 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.006459: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.008896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.008943: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.008973: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.008986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.008997: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.009008: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.009019: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.009033: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.013663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.013707: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.361184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.361260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.361270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:12.366440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21071 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:25:00.0, compute capability: 7.5)\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:13.506085: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m 2020-12-21 15:57:13.897178: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2020-12-21 15:57:15,242\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.5952320098876953 seconds to complete, which may be a performance bottleneck.\r\n",
      "\r\n",
      "Result for DEFAULT_e777a68e:\r\n",
      "  date: 2020-12-21_15-57-03\r\n",
      "  done: false\r\n",
      "  experiment_id: 2fc8d6298fa6420cb346d1667b9b5f0a\r\n",
      "  experiment_tag: 2_drop_prob=0.05,l1=8,l2=32,l3=64,num_layers=3,lr=0.001,reg=0.1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 128\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66255\r\n",
      "  time_since_restore: 14.511497497558594\r\n",
      "  time_this_iter_s: 0.08610343933105469\r\n",
      "  time_total_s: 14.511497497558594\r\n",
      "  timestamp: 1608566223\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.27068662643432617\r\n",
      "  train_auc: 0.9118342399597168\r\n",
      "  train_loss: 0.2939503490924835\r\n",
      "  training_iteration: 128\r\n",
      "  trial_id: e777a68e\r\n",
      "  val_accuracy: 0.20061728358268738\r\n",
      "  val_auc: 0.886155366897583\r\n",
      "  val_loss: 0.3327696621417999\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 11.0/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1998 PENDING, 1 RUNNING, 1 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0068f8aa | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_006f1726 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e777a68e | RUNNING    | 10.16.65.11:66255 |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    128 |          14.5115 |      0.29395 |    0.33277 |         0.270687 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |          25.8359 |    155.706   |  155.21    |         0.238556 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "Result for DEFAULT_e777a68e:\r\n",
      "  date: 2020-12-21_15-57-08\r\n",
      "  done: false\r\n",
      "  experiment_id: 2fc8d6298fa6420cb346d1667b9b5f0a\r\n",
      "  experiment_tag: 2_drop_prob=0.05,l1=8,l2=32,l3=64,num_layers=3,lr=0.001,reg=0.1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 187\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66255\r\n",
      "  time_since_restore: 19.53594446182251\r\n",
      "  time_this_iter_s: 0.07191300392150879\r\n",
      "  time_total_s: 19.53594446182251\r\n",
      "  timestamp: 1608566228\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.26760563254356384\r\n",
      "  train_auc: 0.9084963798522949\r\n",
      "  train_loss: 0.29549530148506165\r\n",
      "  training_iteration: 187\r\n",
      "  trial_id: e777a68e\r\n",
      "  val_accuracy: 0.23148147761821747\r\n",
      "  val_auc: 0.8955860733985901\r\n",
      "  val_loss: 0.3131440281867981\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 11.0/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1998 PENDING, 1 RUNNING, 1 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0068f8aa | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_006f1726 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e777a68e | RUNNING    | 10.16.65.11:66255 |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    187 |          19.5359 |     0.295495 |   0.313144 |         0.267606 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |          25.8359 |   155.706    | 155.21     |         0.238556 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "2020-12-21 15:57:25,325\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.6118402481079102 seconds to complete, which may be a performance bottleneck.\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m  input shape:  68 \r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66276)\u001b[0m \r\n",
      "Result for DEFAULT_e7798896:\r\n",
      "  date: 2020-12-21_15-57-14\r\n",
      "  done: false\r\n",
      "  experiment_id: 271a7781a15945bf9fce8eed97e84f36\r\n",
      "  experiment_tag: 3_drop_prob=0.25,l1=128,l2=64,l3=8,num_layers=3,lr=0.0001,reg=0.1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 1\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66276\r\n",
      "  time_since_restore: 4.401835203170776\r\n",
      "  time_this_iter_s: 4.401835203170776\r\n",
      "  time_total_s: 4.401835203170776\r\n",
      "  timestamp: 1608566234\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.12367957830429077\r\n",
      "  train_auc: 0.4946793019771576\r\n",
      "  train_loss: 40.21224594116211\r\n",
      "  training_iteration: 1\r\n",
      "  trial_id: e7798896\r\n",
      "  val_accuracy: 0.07407407462596893\r\n",
      "  val_auc: 0.4590599238872528\r\n",
      "  val_loss: 39.70863342285156\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.9/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1997 PENDING, 1 RUNNING, 2 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0068f8aa | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e7798896 | RUNNING    | 10.16.65.11:66276 |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |      1 |          4.40184 |    40.2122   |  39.7086   |         0.12368  |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |         25.8359  |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |         20.5357  |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "Result for DEFAULT_e7798896:\r\n",
      "  date: 2020-12-21_15-57-19\r\n",
      "  done: false\r\n",
      "  experiment_id: 271a7781a15945bf9fce8eed97e84f36\r\n",
      "  experiment_tag: 3_drop_prob=0.25,l1=128,l2=64,l3=8,num_layers=3,lr=0.0001,reg=0.1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 60\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66276\r\n",
      "  time_since_restore: 9.411218881607056\r\n",
      "  time_this_iter_s: 0.09000825881958008\r\n",
      "  time_total_s: 9.411218881607056\r\n",
      "  timestamp: 1608566239\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.14480634033679962\r\n",
      "  train_auc: 0.6340733170509338\r\n",
      "  train_loss: 14.178119659423828\r\n",
      "  training_iteration: 60\r\n",
      "  trial_id: e7798896\r\n",
      "  val_accuracy: 0.11728394776582718\r\n",
      "  val_auc: 0.7574016451835632\r\n",
      "  val_loss: 14.028741836547852\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.9/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1997 PENDING, 1 RUNNING, 2 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0068f8aa | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e7798896 | RUNNING    | 10.16.65.11:66276 |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |     68 |          10.0069 |    12.3402   |  12.2085   |         0.136884 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |          25.8359 |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |          20.5357 |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:31.246754: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.724149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.760814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.760867: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.762791: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.763725: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.763913: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.766186: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.766728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.766846: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.771164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.777494: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2794680000 Hz\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.777950: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f066a9e6790 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.777969: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.900787: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f066a80e0b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.900841: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.903140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.903187: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.903220: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.903235: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.903248: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.903260: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.903272: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.903286: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.911654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:32.911746: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:33.247291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:33.247365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:33.247375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:33.251496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21071 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:25:00.0, compute capability: 7.5)\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:34.270960: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m 2020-12-21 15:57:34.653672: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2020-12-21 15:57:35,967\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.617081880569458 seconds to complete, which may be a performance bottleneck.\r\n",
      "\r\n",
      "Result for DEFAULT_e7798896:\r\n",
      "  date: 2020-12-21_15-57-24\r\n",
      "  done: false\r\n",
      "  experiment_id: 271a7781a15945bf9fce8eed97e84f36\r\n",
      "  experiment_tag: 3_drop_prob=0.25,l1=128,l2=64,l3=8,num_layers=3,lr=0.0001,reg=0.1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 128\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66276\r\n",
      "  time_since_restore: 14.467337608337402\r\n",
      "  time_this_iter_s: 0.07706904411315918\r\n",
      "  time_total_s: 14.467337608337402\r\n",
      "  timestamp: 1608566244\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.1395246535539627\r\n",
      "  train_auc: 0.7795704007148743\r\n",
      "  train_loss: 4.350083351135254\r\n",
      "  training_iteration: 128\r\n",
      "  trial_id: e7798896\r\n",
      "  val_accuracy: 0.10493826866149902\r\n",
      "  val_auc: 0.8486125469207764\r\n",
      "  val_loss: 4.2797088623046875\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.9/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1997 PENDING, 1 RUNNING, 2 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0068f8aa | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e7798896 | RUNNING    | 10.16.65.11:66276 |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    128 |          14.4673 |     4.35008  |   4.27971  |         0.139525 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |          25.8359 |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |          20.5357 |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "Result for DEFAULT_e7798896:\r\n",
      "  date: 2020-12-21_15-57-29\r\n",
      "  done: false\r\n",
      "  experiment_id: 271a7781a15945bf9fce8eed97e84f36\r\n",
      "  experiment_tag: 3_drop_prob=0.25,l1=128,l2=64,l3=8,num_layers=3,lr=0.0001,reg=0.1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 187\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66276\r\n",
      "  time_since_restore: 19.484020471572876\r\n",
      "  time_this_iter_s: 0.07518124580383301\r\n",
      "  time_total_s: 19.484020471572876\r\n",
      "  timestamp: 1608566249\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.1672535240650177\r\n",
      "  train_auc: 0.83502197265625\r\n",
      "  train_loss: 1.5888395309448242\r\n",
      "  training_iteration: 187\r\n",
      "  trial_id: e7798896\r\n",
      "  val_accuracy: 0.15432098507881165\r\n",
      "  val_auc: 0.8743293881416321\r\n",
      "  val_loss: 1.5461870431900024\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.9/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1997 PENDING, 1 RUNNING, 2 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0068f8aa | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e7798896 | RUNNING    | 10.16.65.11:66276 |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    195 |          20.0993 |     1.39285  |   1.35926  |         0.165053 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |          25.8359 |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |          20.5357 |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "2020-12-21 15:57:46,013\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.6346161365509033 seconds to complete, which may be a performance bottleneck.\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m  input shape:  68 \r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66259)\u001b[0m \r\n",
      "Result for DEFAULT_e77b6968:\r\n",
      "  date: 2020-12-21_15-57-35\r\n",
      "  done: false\r\n",
      "  experiment_id: d8de7af302fc4acdb23cefb9788e9aaa\r\n",
      "  experiment_tag: 4_drop_prob=0.1,l1=16,l2=16,l3=0,num_layers=2,lr=0.01,reg=1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 1\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66259\r\n",
      "  time_since_restore: 4.257453441619873\r\n",
      "  time_this_iter_s: 4.257453441619873\r\n",
      "  time_total_s: 4.257453441619873\r\n",
      "  timestamp: 1608566255\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.2777288854122162\r\n",
      "  train_auc: 0.5660200119018555\r\n",
      "  train_loss: 37.75578689575195\r\n",
      "  training_iteration: 1\r\n",
      "  trial_id: e77b6968\r\n",
      "  val_accuracy: 0.1358024626970291\r\n",
      "  val_auc: 0.7125266194343567\r\n",
      "  val_loss: 18.202463150024414\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.9/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1996 PENDING, 1 RUNNING, 3 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e77b6968 | RUNNING    | 10.16.65.11:66259 |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |   1   |      1 |          4.25745 |    37.7558   |  18.2025   |         0.277729 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |         25.8359  |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |         20.5357  |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    200 |         20.4825  |     1.28971  |   1.25512  |         0.168134 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "Result for DEFAULT_e77b6968:\r\n",
      "  date: 2020-12-21_15-57-40\r\n",
      "  done: false\r\n",
      "  experiment_id: d8de7af302fc4acdb23cefb9788e9aaa\r\n",
      "  experiment_tag: 4_drop_prob=0.1,l1=16,l2=16,l3=0,num_layers=2,lr=0.01,reg=1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 66\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66259\r\n",
      "  time_since_restore: 9.261367321014404\r\n",
      "  time_this_iter_s: 0.06673812866210938\r\n",
      "  time_total_s: 9.261367321014404\r\n",
      "  timestamp: 1608566260\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.1417253464460373\r\n",
      "  train_auc: 0.7986266016960144\r\n",
      "  train_loss: 0.47133776545524597\r\n",
      "  training_iteration: 66\r\n",
      "  trial_id: e77b6968\r\n",
      "  val_accuracy: 0.1388888955116272\r\n",
      "  val_auc: 0.7520678639411926\r\n",
      "  val_loss: 0.5093125700950623\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.9/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1996 PENDING, 1 RUNNING, 3 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e77b6968 | RUNNING    | 10.16.65.11:66259 |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |   1   |     75 |          9.88352 |     0.466068 |   0.45437  |         0.134243 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |         25.8359  |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |         20.5357  |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    200 |         20.4825  |     1.28971  |   1.25512  |         0.168134 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:50.614808: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.134474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.184921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.184981: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.187166: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.188172: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.188374: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.190669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.191191: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.191298: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.195411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.201867: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2794680000 Hz\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.202189: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0b2680e520 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.202210: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.306325: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0b268de130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.306377: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.308943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.309038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.309090: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.309116: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.309135: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.309158: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.309177: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.309193: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.313500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.313542: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.663967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.664044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.664054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:52.666811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21071 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:25:00.0, compute capability: 7.5)\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:53.533668: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m 2020-12-21 15:57:53.929040: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2020-12-21 15:57:55,993\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.5843884944915771 seconds to complete, which may be a performance bottleneck.\r\n",
      "\r\n",
      "Result for DEFAULT_e77b6968:\r\n",
      "  date: 2020-12-21_15-57-45\r\n",
      "  done: false\r\n",
      "  experiment_id: d8de7af302fc4acdb23cefb9788e9aaa\r\n",
      "  experiment_tag: 4_drop_prob=0.1,l1=16,l2=16,l3=0,num_layers=2,lr=0.01,reg=1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 140\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66259\r\n",
      "  time_since_restore: 14.285926103591919\r\n",
      "  time_this_iter_s: 0.06562232971191406\r\n",
      "  time_total_s: 14.285926103591919\r\n",
      "  timestamp: 1608566265\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.14964789152145386\r\n",
      "  train_auc: 0.7862130999565125\r\n",
      "  train_loss: 0.4848842918872833\r\n",
      "  training_iteration: 140\r\n",
      "  trial_id: e77b6968\r\n",
      "  val_accuracy: 0.07407407462596893\r\n",
      "  val_auc: 0.7828438878059387\r\n",
      "  val_loss: 0.5640329718589783\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.9/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1996 PENDING, 1 RUNNING, 3 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00629e1a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e77b6968 | RUNNING    | 10.16.65.11:66259 |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |   1   |    140 |          14.2859 |     0.484884 |   0.564033 |         0.149648 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |          25.8359 |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |          20.5357 |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    200 |          20.4825 |     1.28971  |   1.25512  |         0.168134 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m  input shape:  68 \r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66252)\u001b[0m \r\n",
      "Result for DEFAULT_e77d58f4:\r\n",
      "  date: 2020-12-21_15-57-54\r\n",
      "  done: false\r\n",
      "  experiment_id: 35e564f175f94ca291be32e1983c3904\r\n",
      "  experiment_tag: 5_drop_prob=0.01,l1=32,l2=0,l3=0,num_layers=1,lr=0.01,reg=0.1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 1\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66252\r\n",
      "  time_since_restore: 4.139896631240845\r\n",
      "  time_this_iter_s: 4.139896631240845\r\n",
      "  time_total_s: 4.139896631240845\r\n",
      "  timestamp: 1608566274\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.14436618983745575\r\n",
      "  train_auc: 0.5703222155570984\r\n",
      "  train_loss: 3.7068593502044678\r\n",
      "  training_iteration: 1\r\n",
      "  trial_id: e77d58f4\r\n",
      "  val_accuracy: 0.16049382090568542\r\n",
      "  val_auc: 0.6941144466400146\r\n",
      "  val_loss: 1.4839699268341064\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.9/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1995 PENDING, 1 RUNNING, 4 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e77d58f4 | RUNNING    | 10.16.65.11:66252 |        0.01 |          32 |           0 |           0 |                   1 | 0.01   |   0.1 |      1 |           4.1399 |     3.70686  |   1.48397  |         0.144366 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |          25.8359 |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |          20.5357 |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    200 |          20.4825 |     1.28971  |   1.25512  |         0.168134 |\r\n",
      "| DEFAULT_e77b6968 | TERMINATED |                   |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |   1   |    200 |          18.9876 |     0.461314 |   0.459231 |         0.144366 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "2020-12-21 15:58:06,039\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.6127908229827881 seconds to complete, which may be a performance bottleneck.\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:08.807793: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.269343: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.304534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.304606: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.306729: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.307691: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.307877: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.310111: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.310647: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.310753: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.313340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.320080: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2794680000 Hz\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.320402: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f4b7a9cba50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.320422: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.390350: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f4b7a80f5e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.390394: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.391701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.391743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.391772: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.391785: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.391798: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.391809: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.391821: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.391833: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.394149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.394183: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.738642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.738713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.738723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:10.741323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21071 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:25:00.0, compute capability: 7.5)\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:11.608415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m 2020-12-21 15:58:11.991082: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2020-12-21 15:58:16,014\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.5811996459960938 seconds to complete, which may be a performance bottleneck.\r\n",
      "\r\n",
      "Result for DEFAULT_e77d58f4:\r\n",
      "  date: 2020-12-21_15-57-59\r\n",
      "  done: false\r\n",
      "  experiment_id: 35e564f175f94ca291be32e1983c3904\r\n",
      "  experiment_tag: 5_drop_prob=0.01,l1=32,l2=0,l3=0,num_layers=1,lr=0.01,reg=0.1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 72\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66252\r\n",
      "  time_since_restore: 9.14254879951477\r\n",
      "  time_this_iter_s: 0.059253692626953125\r\n",
      "  time_total_s: 9.14254879951477\r\n",
      "  timestamp: 1608566279\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.24031689763069153\r\n",
      "  train_auc: 0.8732881546020508\r\n",
      "  train_loss: 0.3409225642681122\r\n",
      "  training_iteration: 72\r\n",
      "  trial_id: e77d58f4\r\n",
      "  val_accuracy: 0.2993827164173126\r\n",
      "  val_auc: 0.8773579597473145\r\n",
      "  val_loss: 0.3526703417301178\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.9/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1995 PENDING, 1 RUNNING, 4 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e77d58f4 | RUNNING    | 10.16.65.11:66252 |        0.01 |          32 |           0 |           0 |                   1 | 0.01   |   0.1 |     72 |          9.14255 |     0.340923 |   0.35267  |         0.240317 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |         25.8359  |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |         20.5357  |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    200 |         20.4825  |     1.28971  |   1.25512  |         0.168134 |\r\n",
      "| DEFAULT_e77b6968 | TERMINATED |                   |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |   1   |    200 |         18.9876  |     0.461314 |   0.459231 |         0.144366 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "Result for DEFAULT_e77d58f4:\r\n",
      "  date: 2020-12-21_15-58-04\r\n",
      "  done: false\r\n",
      "  experiment_id: 35e564f175f94ca291be32e1983c3904\r\n",
      "  experiment_tag: 5_drop_prob=0.01,l1=32,l2=0,l3=0,num_layers=1,lr=0.01,reg=0.1\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 152\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66252\r\n",
      "  time_since_restore: 14.162471771240234\r\n",
      "  time_this_iter_s: 0.06053447723388672\r\n",
      "  time_total_s: 14.162471771240234\r\n",
      "  timestamp: 1608566284\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.24295774102210999\r\n",
      "  train_auc: 0.8729714751243591\r\n",
      "  train_loss: 0.338628888130188\r\n",
      "  training_iteration: 152\r\n",
      "  trial_id: e77d58f4\r\n",
      "  val_accuracy: 0.26543208956718445\r\n",
      "  val_auc: 0.8789305686950684\r\n",
      "  val_loss: 0.3414188325405121\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.9/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1995 PENDING, 1 RUNNING, 4 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_005c0e92 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e77d58f4 | RUNNING    | 10.16.65.11:66252 |        0.01 |          32 |           0 |           0 |                   1 | 0.01   |   0.1 |    152 |          14.1625 |     0.338629 |   0.341419 |         0.242958 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |          25.8359 |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |          20.5357 |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    200 |          20.4825 |     1.28971  |   1.25512  |         0.168134 |\r\n",
      "| DEFAULT_e77b6968 | TERMINATED |                   |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |   1   |    200 |          18.9876 |     0.461314 |   0.459231 |         0.144366 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "2020-12-21 15:58:26,044\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.6014127731323242 seconds to complete, which may be a performance bottleneck.\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m  input shape:  68 \r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66260)\u001b[0m \r\n",
      "Result for DEFAULT_e77f402e:\r\n",
      "  date: 2020-12-21_15-58-12\r\n",
      "  done: false\r\n",
      "  experiment_id: b664a5ca322148b386b0cbf3fad493fe\r\n",
      "  experiment_tag: 6_drop_prob=0.2,l1=16,l2=0,l3=0,num_layers=1,lr=0.001,reg=0\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 1\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66260\r\n",
      "  time_since_restore: 4.006901025772095\r\n",
      "  time_this_iter_s: 4.006901025772095\r\n",
      "  time_total_s: 4.006901025772095\r\n",
      "  timestamp: 1608566292\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.1544894427061081\r\n",
      "  train_auc: 0.5035275816917419\r\n",
      "  train_loss: 0.7357775568962097\r\n",
      "  training_iteration: 1\r\n",
      "  trial_id: e77f402e\r\n",
      "  val_accuracy: 0.2222222238779068\r\n",
      "  val_auc: 0.547774076461792\r\n",
      "  val_loss: 0.6836395263671875\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.8/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1994 PENDING, 1 RUNNING, 5 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e77f402e | RUNNING    | 10.16.65.11:66260 |        0.2  |          16 |           0 |           0 |                   1 | 0.001  |   0   |      1 |           4.0069 |     0.735778 |   0.68364  |         0.154489 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |          25.8359 |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |          20.5357 |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    200 |          20.4825 |     1.28971  |   1.25512  |         0.168134 |\r\n",
      "| DEFAULT_e77b6968 | TERMINATED |                   |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |   1   |    200 |          18.9876 |     0.461314 |   0.459231 |         0.144366 |\r\n",
      "| DEFAULT_e77d58f4 | TERMINATED |                   |        0.01 |          32 |           0 |           0 |                   1 | 0.01   |   0.1 |    200 |          17.786  |     0.337853 |   0.369621 |         0.255282 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "Result for DEFAULT_e77f402e:\r\n",
      "  date: 2020-12-21_15-58-17\r\n",
      "  done: false\r\n",
      "  experiment_id: b664a5ca322148b386b0cbf3fad493fe\r\n",
      "  experiment_tag: 6_drop_prob=0.2,l1=16,l2=0,l3=0,num_layers=1,lr=0.001,reg=0\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 72\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66260\r\n",
      "  time_since_restore: 9.057812452316284\r\n",
      "  time_this_iter_s: 0.08474349975585938\r\n",
      "  time_total_s: 9.057812452316284\r\n",
      "  timestamp: 1608566297\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.28389084339141846\r\n",
      "  train_auc: 0.8809598088264465\r\n",
      "  train_loss: 0.30778345465660095\r\n",
      "  training_iteration: 72\r\n",
      "  trial_id: e77f402e\r\n",
      "  val_accuracy: 0.28703704476356506\r\n",
      "  val_auc: 0.8890393376350403\r\n",
      "  val_loss: 0.31016939878463745\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.8/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1994 PENDING, 1 RUNNING, 5 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e77f402e | RUNNING    | 10.16.65.11:66260 |        0.2  |          16 |           0 |           0 |                   1 | 0.001  |   0   |     72 |          9.05781 |     0.307783 |   0.310169 |         0.283891 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |         25.8359  |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |         20.5357  |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    200 |         20.4825  |     1.28971  |   1.25512  |         0.168134 |\r\n",
      "| DEFAULT_e77b6968 | TERMINATED |                   |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |   1   |    200 |         18.9876  |     0.461314 |   0.459231 |         0.144366 |\r\n",
      "| DEFAULT_e77d58f4 | TERMINATED |                   |        0.01 |          32 |           0 |           0 |                   1 | 0.01   |   0.1 |    200 |         17.786   |     0.337853 |   0.369621 |         0.255282 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:26.805007: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.302353: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.338747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.338807: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.340783: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.341733: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.341954: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.344318: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.344876: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.344994: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.347656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.354760: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2794680000 Hz\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.355258: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7efb269e6e40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.355276: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.425605: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7efb268088d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.425658: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.426951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.426989: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.427015: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.427027: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.427038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.427050: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.427068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.427096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.429565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.429604: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.777481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.777552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.777561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:28.780386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21071 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:25:00.0, compute capability: 7.5)\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:29.786707: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m 2020-12-21 15:58:30.169884: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2020-12-21 15:58:36,076\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.6177470684051514 seconds to complete, which may be a performance bottleneck.\r\n",
      "\r\n",
      "Result for DEFAULT_e77f402e:\r\n",
      "  date: 2020-12-21_15-58-22\r\n",
      "  done: false\r\n",
      "  experiment_id: b664a5ca322148b386b0cbf3fad493fe\r\n",
      "  experiment_tag: 6_drop_prob=0.2,l1=16,l2=0,l3=0,num_layers=1,lr=0.001,reg=0\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 153\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66260\r\n",
      "  time_since_restore: 14.104112386703491\r\n",
      "  time_this_iter_s: 0.06329894065856934\r\n",
      "  time_total_s: 14.104112386703491\r\n",
      "  timestamp: 1608566302\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.2737676203250885\r\n",
      "  train_auc: 0.896812379360199\r\n",
      "  train_loss: 0.28967174887657166\r\n",
      "  training_iteration: 153\r\n",
      "  trial_id: e77f402e\r\n",
      "  val_accuracy: 0.2716049253940582\r\n",
      "  val_auc: 0.897061824798584\r\n",
      "  val_loss: 0.29613471031188965\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.8/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1994 PENDING, 1 RUNNING, 5 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0055a98a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e77f402e | RUNNING    | 10.16.65.11:66260 |        0.2  |          16 |           0 |           0 |                   1 | 0.001  |   0   |    153 |          14.1041 |     0.289672 |   0.296135 |         0.273768 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |          25.8359 |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |          20.5357 |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    200 |          20.4825 |     1.28971  |   1.25512  |         0.168134 |\r\n",
      "| DEFAULT_e77b6968 | TERMINATED |                   |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |   1   |    200 |          18.9876 |     0.461314 |   0.459231 |         0.144366 |\r\n",
      "| DEFAULT_e77d58f4 | TERMINATED |                   |        0.01 |          32 |           0 |           0 |                   1 | 0.01   |   0.1 |    200 |          17.786  |     0.337853 |   0.369621 |         0.255282 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m  input shape:  68 \r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66247)\u001b[0m \r\n",
      "Result for DEFAULT_e781094a:\r\n",
      "  date: 2020-12-21_15-58-30\r\n",
      "  done: false\r\n",
      "  experiment_id: 9bbd441c6f2943d2bbff0cb650da42a4\r\n",
      "  experiment_tag: 7_drop_prob=0.01,l1=64,l2=64,l3=0,num_layers=2,lr=0.01,reg=0\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 1\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66247\r\n",
      "  time_since_restore: 4.209648370742798\r\n",
      "  time_this_iter_s: 4.209648370742798\r\n",
      "  time_total_s: 4.209648370742798\r\n",
      "  timestamp: 1608566310\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.18089789152145386\r\n",
      "  train_auc: 0.7829305529594421\r\n",
      "  train_loss: 0.40891239047050476\r\n",
      "  training_iteration: 1\r\n",
      "  trial_id: e781094a\r\n",
      "  val_accuracy: 0.2191358059644699\r\n",
      "  val_auc: 0.8148670792579651\r\n",
      "  val_loss: 0.4583956003189087\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.8/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1993 PENDING, 1 RUNNING, 6 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e781094a | RUNNING    | 10.16.65.11:66247 |        0.01 |          64 |          64 |           0 |                   2 | 0.01   |   0   |      1 |          4.20965 |     0.408912 |   0.458396 |         0.180898 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |         25.8359  |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |         20.5357  |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    200 |         20.4825  |     1.28971  |   1.25512  |         0.168134 |\r\n",
      "| DEFAULT_e77b6968 | TERMINATED |                   |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |   1   |    200 |         18.9876  |     0.461314 |   0.459231 |         0.144366 |\r\n",
      "| DEFAULT_e77d58f4 | TERMINATED |                   |        0.01 |          32 |           0 |           0 |                   1 | 0.01   |   0.1 |    200 |         17.786   |     0.337853 |   0.369621 |         0.255282 |\r\n",
      "| DEFAULT_e77f402e | TERMINATED |                   |        0.2  |          16 |           0 |           0 |                   1 | 0.001  |   0   |    200 |         17.6175  |     0.288643 |   0.293767 |         0.261884 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:45.332025: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.830791: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.871612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.871674: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.873819: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.874787: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.874971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.877326: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.877898: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.878015: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.880562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.887211: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2794680000 Hz\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.887546: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb1e29e73b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.887569: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.962869: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb1e2810660 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.962911: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.965211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.965252: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.965277: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.965289: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.965300: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.965312: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.965322: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.965334: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.970304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:46.970347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:47.299651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:47.299721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:47.299730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:47.304047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21071 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:25:00.0, compute capability: 7.5)\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:48.142826: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m 2020-12-21 15:58:48.520398: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2020-12-21 15:58:49,825\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.6332948207855225 seconds to complete, which may be a performance bottleneck.\r\n",
      "\r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.8/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1993 PENDING, 1 RUNNING, 6 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e781094a | RUNNING    | 10.16.65.11:66247 |        0.01 |          64 |          64 |           0 |                   2 | 0.01   |   0   |     70 |           8.8114 |     0.101104 |   0.504678 |         0.419454 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |          25.8359 |   155.706    | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |          20.5357 |     0.294507 |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    200 |          20.4825 |     1.28971  |   1.25512  |         0.168134 |\r\n",
      "| DEFAULT_e77b6968 | TERMINATED |                   |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |   1   |    200 |          18.9876 |     0.461314 |   0.459231 |         0.144366 |\r\n",
      "| DEFAULT_e77d58f4 | TERMINATED |                   |        0.01 |          32 |           0 |           0 |                   1 | 0.01   |   0.1 |    200 |          17.786  |     0.337853 |   0.369621 |         0.255282 |\r\n",
      "| DEFAULT_e77f402e | TERMINATED |                   |        0.2  |          16 |           0 |           0 |                   1 | 0.001  |   0   |    200 |          17.6175 |     0.288643 |   0.293767 |         0.261884 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "Result for DEFAULT_e781094a:\r\n",
      "  date: 2020-12-21_15-58-35\r\n",
      "  done: false\r\n",
      "  experiment_id: 9bbd441c6f2943d2bbff0cb650da42a4\r\n",
      "  experiment_tag: 7_drop_prob=0.01,l1=64,l2=64,l3=0,num_layers=2,lr=0.01,reg=0\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 71\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66247\r\n",
      "  time_since_restore: 8.886071681976318\r\n",
      "  time_this_iter_s: 0.07467436790466309\r\n",
      "  time_total_s: 8.886071681976318\r\n",
      "  timestamp: 1608566315\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.40757042169570923\r\n",
      "  train_auc: 0.9893367886543274\r\n",
      "  train_loss: 0.09981530159711838\r\n",
      "  training_iteration: 71\r\n",
      "  trial_id: e781094a\r\n",
      "  val_accuracy: 0.2839506268501282\r\n",
      "  val_auc: 0.857966423034668\r\n",
      "  val_loss: 0.5297183990478516\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.8/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1993 PENDING, 1 RUNNING, 6 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_004f4856 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1 |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e781094a | RUNNING    | 10.16.65.11:66247 |        0.01 |          64 |          64 |           0 |                   2 | 0.01   |   0   |    145 |          14.4566 |    0.0451216 |   0.816694 |         0.430458 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1   |    200 |          25.8359 |  155.706     | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1 |    200 |          20.5357 |    0.294507  |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1 |    200 |          20.4825 |    1.28971   |   1.25512  |         0.168134 |\r\n",
      "| DEFAULT_e77b6968 | TERMINATED |                   |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |   1   |    200 |          18.9876 |    0.461314  |   0.459231 |         0.144366 |\r\n",
      "| DEFAULT_e77d58f4 | TERMINATED |                   |        0.01 |          32 |           0 |           0 |                   1 | 0.01   |   0.1 |    200 |          17.786  |    0.337853  |   0.369621 |         0.255282 |\r\n",
      "| DEFAULT_e77f402e | TERMINATED |                   |        0.2  |          16 |           0 |           0 |                   1 | 0.001  |   0   |    200 |          17.6175 |    0.288643  |   0.293767 |         0.261884 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "2020-12-21 15:58:59,828\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.611274242401123 seconds to complete, which may be a performance bottleneck.\r\n",
      "\r\n",
      "Result for DEFAULT_e781094a:\r\n",
      "  date: 2020-12-21_15-58-41\r\n",
      "  done: false\r\n",
      "  experiment_id: 9bbd441c6f2943d2bbff0cb650da42a4\r\n",
      "  experiment_tag: 7_drop_prob=0.01,l1=64,l2=64,l3=0,num_layers=2,lr=0.01,reg=0\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 146\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66247\r\n",
      "  time_since_restore: 14.522924661636353\r\n",
      "  time_this_iter_s: 0.0663306713104248\r\n",
      "  time_total_s: 14.522924661636353\r\n",
      "  timestamp: 1608566321\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.43089789152145386\r\n",
      "  train_auc: 0.9970159530639648\r\n",
      "  train_loss: 0.04815298318862915\r\n",
      "  training_iteration: 146\r\n",
      "  trial_id: e781094a\r\n",
      "  val_accuracy: 0.31790122389793396\r\n",
      "  val_auc: 0.8219954371452332\r\n",
      "  val_loss: 0.8353847861289978\r\n",
      "  \r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m  input shape:  68 \r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66316)\u001b[0m \r\n",
      "Result for DEFAULT_e782e490:\r\n",
      "  date: 2020-12-21_15-58-49\r\n",
      "  done: false\r\n",
      "  experiment_id: 7328a616e4464775905d03c01a389b96\r\n",
      "  experiment_tag: 8_drop_prob=0.05,l1=128,l2=0,l3=0,num_layers=1,lr=0.0001,reg=0.01\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 1\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66316\r\n",
      "  time_since_restore: 4.005887985229492\r\n",
      "  time_this_iter_s: 4.005887985229492\r\n",
      "  time_total_s: 4.005887985229492\r\n",
      "  timestamp: 1608566329\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.2869718372821808\r\n",
      "  train_auc: 0.5236819386482239\r\n",
      "  train_loss: 3.2654459476470947\r\n",
      "  training_iteration: 1\r\n",
      "  trial_id: e782e490\r\n",
      "  val_accuracy: 0.12037037312984467\r\n",
      "  val_auc: 0.5677583813667297\r\n",
      "  val_loss: 3.2154653072357178\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.8/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1992 PENDING, 1 RUNNING, 7 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e782e490 | RUNNING    | 10.16.65.11:66316 |        0.05 |         128 |           0 |           0 |                   1 | 0.0001 |  0.01 |      1 |          4.00589 |    3.26545   |   3.21547  |         0.286972 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |  1    |    200 |         25.8359  |  155.706     | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |  0.1  |    200 |         20.5357  |    0.294507  |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |  0.1  |    200 |         20.4825  |    1.28971   |   1.25512  |         0.168134 |\r\n",
      "| DEFAULT_e77b6968 | TERMINATED |                   |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |  1    |    200 |         18.9876  |    0.461314  |   0.459231 |         0.144366 |\r\n",
      "| DEFAULT_e77d58f4 | TERMINATED |                   |        0.01 |          32 |           0 |           0 |                   1 | 0.01   |  0.1  |    200 |         17.786   |    0.337853  |   0.369621 |         0.255282 |\r\n",
      "| DEFAULT_e77f402e | TERMINATED |                   |        0.2  |          16 |           0 |           0 |                   1 | 0.001  |  0    |    200 |         17.6175  |    0.288643  |   0.293767 |         0.261884 |\r\n",
      "| DEFAULT_e781094a | TERMINATED |                   |        0.01 |          64 |          64 |           0 |                   2 | 0.01   |  0    |    200 |         18.1455  |    0.0385367 |   0.917933 |         0.444982 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "Result for DEFAULT_e782e490:\r\n",
      "  date: 2020-12-21_15-58-54\r\n",
      "  done: false\r\n",
      "  experiment_id: 7328a616e4464775905d03c01a389b96\r\n",
      "  experiment_tag: 8_drop_prob=0.05,l1=128,l2=0,l3=0,num_layers=1,lr=0.0001,reg=0.01\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 73\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66316\r\n",
      "  time_since_restore: 9.031712532043457\r\n",
      "  time_this_iter_s: 0.06425118446350098\r\n",
      "  time_total_s: 9.031712532043457\r\n",
      "  timestamp: 1608566334\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.34286972880363464\r\n",
      "  train_auc: 0.8443374633789062\r\n",
      "  train_loss: 1.1949137449264526\r\n",
      "  training_iteration: 73\r\n",
      "  trial_id: e782e490\r\n",
      "  val_accuracy: 0.34567901492118835\r\n",
      "  val_auc: 0.8329431414604187\r\n",
      "  val_loss: 1.2027395963668823\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.7/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1992 PENDING, 1 RUNNING, 7 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e782e490 | RUNNING    | 10.16.65.11:66316 |        0.05 |         128 |           0 |           0 |                   1 | 0.0001 |  0.01 |     83 |          9.69019 |    1.0665    |   1.075    |         0.332306 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |  1    |    200 |         25.8359  |  155.706     | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |  0.1  |    200 |         20.5357  |    0.294507  |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |  0.1  |    200 |         20.4825  |    1.28971   |   1.25512  |         0.168134 |\r\n",
      "| DEFAULT_e77b6968 | TERMINATED |                   |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |  1    |    200 |         18.9876  |    0.461314  |   0.459231 |         0.144366 |\r\n",
      "| DEFAULT_e77d58f4 | TERMINATED |                   |        0.01 |          32 |           0 |           0 |                   1 | 0.01   |  0.1  |    200 |         17.786   |    0.337853  |   0.369621 |         0.255282 |\r\n",
      "| DEFAULT_e77f402e | TERMINATED |                   |        0.2  |          16 |           0 |           0 |                   1 | 0.001  |  0    |    200 |         17.6175  |    0.288643  |   0.293767 |         0.261884 |\r\n",
      "| DEFAULT_e781094a | TERMINATED |                   |        0.01 |          64 |          64 |           0 |                   2 | 0.01   |  0    |    200 |         18.1455  |    0.0385367 |   0.917933 |         0.444982 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:03.247192: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.736476: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.771668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.771730: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.773889: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.774859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.775055: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.777431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.778015: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.778135: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.780756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.787068: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2794680000 Hz\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.787501: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f93669e8350 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.787526: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.857695: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f936680fcc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.857748: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.859148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m pciBusID: 0000:25:00.0 name: Quadro RTX 6000 computeCapability: 7.5\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 22.17GiB deviceMemoryBandwidth: 581.23GiB/s\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.859187: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.859217: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.859231: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.859242: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.859253: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.859264: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.859275: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.861663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:04.861700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:05.210208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:05.210277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:05.210287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:05.214614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21071 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:25:00.0, compute capability: 7.5)\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:06.355802: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m 2020-12-21 15:59:06.744289: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n",
      "2020-12-21 15:59:09,867\tWARNING util.py:139 -- The `experiment_checkpoint` operation took 0.6120197772979736 seconds to complete, which may be a performance bottleneck.\r\n",
      "\r\n",
      "Result for DEFAULT_e782e490:\r\n",
      "  date: 2020-12-21_15-58-59\r\n",
      "  done: false\r\n",
      "  experiment_id: 7328a616e4464775905d03c01a389b96\r\n",
      "  experiment_tag: 8_drop_prob=0.05,l1=128,l2=0,l3=0,num_layers=1,lr=0.0001,reg=0.01\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 154\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66316\r\n",
      "  time_since_restore: 14.101690769195557\r\n",
      "  time_this_iter_s: 0.06905508041381836\r\n",
      "  time_total_s: 14.101690769195557\r\n",
      "  timestamp: 1608566339\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.3551936745643616\r\n",
      "  train_auc: 0.8914516568183899\r\n",
      "  train_loss: 0.5498172044754028\r\n",
      "  training_iteration: 154\r\n",
      "  trial_id: e782e490\r\n",
      "  val_accuracy: 0.37962964177131653\r\n",
      "  val_auc: 0.8717640042304993\r\n",
      "  val_loss: 0.5752063989639282\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.7/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1992 PENDING, 1 RUNNING, 7 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |   reg |   iter |   total time (s) |   train_loss |   val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_0048e2a4 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |  0.1  |        |                  |              |            |                  |\r\n",
      "| DEFAULT_e782e490 | RUNNING    | 10.16.65.11:66316 |        0.05 |         128 |           0 |           0 |                   1 | 0.0001 |  0.01 |    155 |          14.7557 |    0.545539  |   0.57137  |         0.352993 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |  1    |    200 |          25.8359 |  155.706     | 155.21     |         0.238556 |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |  0.1  |    200 |          20.5357 |    0.294507  |   0.345022 |         0.264525 |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |  0.1  |    200 |          20.4825 |    1.28971   |   1.25512  |         0.168134 |\r\n",
      "| DEFAULT_e77b6968 | TERMINATED |                   |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |  1    |    200 |          18.9876 |    0.461314  |   0.459231 |         0.144366 |\r\n",
      "| DEFAULT_e77d58f4 | TERMINATED |                   |        0.01 |          32 |           0 |           0 |                   1 | 0.01   |  0.1  |    200 |          17.786  |    0.337853  |   0.369621 |         0.255282 |\r\n",
      "| DEFAULT_e77f402e | TERMINATED |                   |        0.2  |          16 |           0 |           0 |                   1 | 0.001  |  0    |    200 |          17.6175 |    0.288643  |   0.293767 |         0.261884 |\r\n",
      "| DEFAULT_e781094a | TERMINATED |                   |        0.01 |          64 |          64 |           0 |                   2 | 0.01   |  0    |    200 |          18.1455 |    0.0385367 |   0.917933 |         0.444982 |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+-------+--------+------------------+--------------+------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m  input shape:  68 \r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=66238)\u001b[0m \r\n",
      "Result for DEFAULT_e784cc42:\r\n",
      "  date: 2020-12-21_15-59-07\r\n",
      "  done: false\r\n",
      "  experiment_id: ef02783799b94ad199b5f5cf0e4cf8be\r\n",
      "  experiment_tag: 9_drop_prob=0.25,l1=64,l2=128,l3=32,num_layers=3,lr=0.0001,reg=100\r\n",
      "  hostname: ohi-hpc2-keon01\r\n",
      "  iterations_since_restore: 1\r\n",
      "  node_ip: 10.16.65.11\r\n",
      "  pid: 66238\r\n",
      "  time_since_restore: 4.373101472854614\r\n",
      "  time_this_iter_s: 4.373101472854614\r\n",
      "  time_total_s: 4.373101472854614\r\n",
      "  timestamp: 1608566347\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  train_accuracy: 0.05897887423634529\r\n",
      "  train_auc: 0.5296187996864319\r\n",
      "  train_loss: 43646.5625\r\n",
      "  training_iteration: 1\r\n",
      "  trial_id: e784cc42\r\n",
      "  val_accuracy: 0.06790123134851456\r\n",
      "  val_auc: 0.5987361073493958\r\n",
      "  val_loss: 43239.40625\r\n",
      "  \r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.7/251.4 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 1/96 CPUs, 1/1 GPUs, 0.0/163.87 GiB heap, 0.0/51.17 GiB objects (0/1.0 accelerator_type:RTX)\r\n",
      "Result logdir: /q/PET-MBF/output/17_segment/localization/mlp/results/mlp_17s_r1-job599\r\n",
      "Number of trials: 2000 (1991 PENDING, 1 RUNNING, 8 TERMINATED)\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+--------+--------+------------------+---------------+--------------+------------------+\r\n",
      "| Trial name       | status     | loc               |   drop_prob |   layers/l1 |   layers/l2 |   layers/l3 |   layers/num_layers |     lr |    reg |   iter |   total time (s) |    train_loss |     val_loss |   train_accuracy |\r\n",
      "|------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+--------+--------+------------------+---------------+--------------+------------------|\r\n",
      "| DEFAULT_00040ba2 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1  |        |                  |               |              |                  |\r\n",
      "| DEFAULT_000a181c | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1  |        |                  |               |              |                  |\r\n",
      "| DEFAULT_001057b8 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1  |        |                  |               |              |                  |\r\n",
      "| DEFAULT_0016b428 | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1  |        |                  |               |              |                  |\r\n",
      "| DEFAULT_001cf9c8 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1  |        |                  |               |              |                  |\r\n",
      "| DEFAULT_0022ffbc | PENDING    |                   |        0.15 |         256 |          16 |          64 |                   3 | 0.0001 |   0.1  |        |                  |               |              |                  |\r\n",
      "| DEFAULT_00296172 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1  |        |                  |               |              |                  |\r\n",
      "| DEFAULT_002fcd46 | PENDING    |                   |        0.15 |         256 |          16 |         128 |                   3 | 0.0001 |   0.1  |        |                  |               |              |                  |\r\n",
      "| DEFAULT_0036468a | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1  |        |                  |               |              |                  |\r\n",
      "| DEFAULT_003c6e66 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1  |        |                  |               |              |                  |\r\n",
      "| DEFAULT_00429c64 | PENDING    |                   |        0.15 |         256 |          32 |         128 |                   3 | 0.0001 |   0.1  |        |                  |               |              |                  |\r\n",
      "| DEFAULT_e784cc42 | RUNNING    | 10.16.65.11:66238 |        0.25 |          64 |         128 |          32 |                   3 | 0.0001 | 100    |      1 |           4.3731 | 43646.6       | 43239.4      |        0.0589789 |\r\n",
      "| DEFAULT_e77538f4 | TERMINATED |                   |        0    |           8 |         256 |           0 |                   2 | 0.0001 |   1    |    200 |          25.8359 |   155.706     |   155.21     |        0.238556  |\r\n",
      "| DEFAULT_e777a68e | TERMINATED |                   |        0.05 |           8 |          32 |          64 |                   3 | 0.001  |   0.1  |    200 |          20.5357 |     0.294507  |     0.345022 |        0.264525  |\r\n",
      "| DEFAULT_e7798896 | TERMINATED |                   |        0.25 |         128 |          64 |           8 |                   3 | 0.0001 |   0.1  |    200 |          20.4825 |     1.28971   |     1.25512  |        0.168134  |\r\n",
      "| DEFAULT_e77b6968 | TERMINATED |                   |        0.1  |          16 |          16 |           0 |                   2 | 0.01   |   1    |    200 |          18.9876 |     0.461314  |     0.459231 |        0.144366  |\r\n",
      "| DEFAULT_e77d58f4 | TERMINATED |                   |        0.01 |          32 |           0 |           0 |                   1 | 0.01   |   0.1  |    200 |          17.786  |     0.337853  |     0.369621 |        0.255282  |\r\n",
      "| DEFAULT_e77f402e | TERMINATED |                   |        0.2  |          16 |           0 |           0 |                   1 | 0.001  |   0    |    200 |          17.6175 |     0.288643  |     0.293767 |        0.261884  |\r\n",
      "| DEFAULT_e781094a | TERMINATED |                   |        0.01 |          64 |          64 |           0 |                   2 | 0.01   |   0    |    200 |          18.1455 |     0.0385367 |     0.917933 |        0.444982  |\r\n",
      "| DEFAULT_e782e490 | TERMINATED |                   |        0.05 |         128 |           0 |           0 |                   1 | 0.0001 |   0.01 |    200 |          17.5507 |     0.415098  |     0.453481 |        0.366637  |\r\n",
      "+------------------+------------+-------------------+-------------+-------------+-------------+-------------+---------------------+--------+--------+--------+------------------+---------------+--------------+------------------+\r\n",
      "... 1980 more trials not shown (1980 PENDING)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat /q/PET-MBF/output/17_segment/localization/mlp/logs/dberman_mitre-mlp_17s_r1-job599.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 599\r\n",
      "Cluster: deepops\r\n",
      "Use of uninitialized value $user in concatenation (.) or string at /usr/local/bin/seff line 154, <DATA> line 602.\r\n",
      "User/Group: /domain users\r\n",
      "State: COMPLETED (exit code 0)\r\n",
      "Nodes: 1\r\n",
      "Cores per node: 6\r\n",
      "CPU Utilized: 20:09:11\r\n",
      "CPU Efficiency: 31.27% of 2-16:26:42 core-walltime\r\n",
      "Job Wall-clock time: 10:44:27\r\n",
      "Memory Utilized: 5.56 GB\r\n",
      "Memory Efficiency: 27.78% of 20.00 GB\r\n"
     ]
    }
   ],
   "source": [
    "!seff 599"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reviewed the results of this hyperparameter search in Tensor board and looked at the highest performing models by AUC. We saw that the top performing model showed a val AUC score of 0.91.668, and had the following hyperparameter settings:\n",
    "\n",
    "- **drop_prob:** 0.10\n",
    "- **lr:** 0.0001\n",
    "- **reg:** 0\n",
    "- **l1:** 256\n",
    "- **l2:** 32\n",
    "- **l3:** 0\n",
    "\n",
    "The second highest performing hyperparameter settings get a AUC of 91.11, and has hyperparameter settings:\n",
    "\n",
    "- **drop_prob:** 0.15\n",
    "- **lr:** 0.0001\n",
    "- **reg:** 0\n",
    "- **l1:** 256\n",
    "- **l2:** 32\n",
    "- **l3:** 128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
